[{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Setting up an R environment","text":"guide, explore options setting R environment. discuss local, remote, virtual environments. advantages shortcomings. Outcomes Understand different options working R Learn advantages disadvantages option Choose option best fits needs","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"local-environments","dir":"Articles","previous_headings":"Environment setups","what":"Local environments","title":"Setting up an R environment","text":"Choosing work R locally means install R IDE local computer. approach offers following advantages: Fast responsive performance reliance internet connectivity Flexibility customize environment main disadvantages working locally : need install R IDE local computer, manage software environment, manage backups version control collaborative projects. can challenge new users, number resources available help get started troubleshoot issues may encounter. get started, install R CRAN. can download latest version R operating system . installed R, need install IDE. recommend RStudio, free open-source IDE R. RStudio provides number features make easier work R. projects require collaboration, recommend using version control system Git hosting service GitHub GitLab. tools help manage code collaborate others.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"remote-environments","dir":"Articles","previous_headings":"Environment setups","what":"Remote environments","title":"Setting up an R environment","text":"can also choose work R cloud, remote environment. number cloud-based options working R, including Posit Cloud Microsoft Azure. options provide pre-configured R environment can access computer internet connection. Posit Cloud provides environment can create, edit, run R projects anywhere internet access. offers several advantages: need install R RStudio locally Access projects device Collaborate others real-time Easily share work drawbacks working cloud include: Reliance stable internet connection Potential latency performance issues Limited customization options compared local setup get started Posit Cloud, need create account. can sign free account . created account, see list spaces. default personal workspace, can also join invited spaces. Visit Guide documentation learn features Posit Cloud.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"virtual-environments","dir":"Articles","previous_headings":"Environment setups","what":"Virtual environments","title":"Setting up an R environment","text":"new R, may want consider working cloud get started. plan continue work R future, likely want install R IDE local computer explore using virtual environment. Virtual environments, Docker, provide way use pre-configured computing environment create can share others. Virtual environments good option want ensure everyone research group working computing environment. Pre-configured virtual environments exist R Rocker project can used locally cloud. Docker platform allows create, deploy, run applications containers. Rocker collection Docker images specifically designed running R. Docker enables package application along dependencies container, ensuring run consistently across different environments. Rocker extends concept R, providing pre-built Docker images various R configurations. Using Docker Rocker offers several benefits: Reproducible environments Simplified dependency management Easy deployment scaling drawbacks using Docker Rocker include: Learning curve setting managing Docker containers Increased memory resource requirements Potential compatibility issues certain packages libraries start using Docker Rocker, follow steps: Install Docker local machine Pull desired Rocker image Docker Hub Run container using pulled image Access RStudio browser http://localhost:8787 log username rstudio password set","code":"docker pull rocker/rstudio docker run -d -p 8787:8787 -e PASSWORD=your_password --name rstudio_container rocker/rstudio"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Setting up an R environment","text":"guide, discussed strategies working R. three options offer unique advantages. ??, summarize characteristics, benefits, drawbacks option. Table 1:  Comparison different environments working R RStudio Give try see one works best needs! Remember, can always switch different environments needs change.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Setting up Git and GitHub","text":"guide, cover basics setting Git GitHub. also cover basics using Git GitHub manage project. guide intended beginners new Git GitHub. also intended new using Git GitHub R. Outcomes Recognize purpose Git GitHub Establish working Git GitHub environment Recognize basic Git GitHub workflow managing project","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"what-is-git-github-and-why-should-i-use-it","dir":"Articles","previous_headings":"","what":"What is Git, Github? And why should I use it?","title":"Setting up Git and GitHub","text":"Git version control system. allows track changes files folders time. also allows collaborate others projects. Think MS Word's \"Track Changes\" feature steroids. Git command line tool, also graphical user interfaces (GUIs) interact Git user-friendly way. Git great tool managing projects. especially useful managing projects involve multiple people. GitHub web-based hosting service Git repositories. allows store Git repositories cloud. also allows collaborate others projects sharing projects. GitHub great place store share code. also great place find code others shared. Combining Git GitHub allows store Git managed project repositories cloud. means can access repositories anywhere. also means can collaborate others projects. can also use GitHub share code others. especially useful making projects reproducible.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"install-and-setup-git","dir":"Articles","previous_headings":"How do I set up Git?","what":"Install and setup Git","title":"Setting up Git and GitHub","text":"process installation setup differ based operating system using. using Windows machine, likely need install Git. using Mac Linux machine, likely already Git installed. Windows users need install Git. can downloading installer https://git-scm.com/downloads. downloaded installer, need run . need follow instructions installer complete installation. Mac Linux users can verify Git installed opening terminal window typing git --version. Git installed, see version number. Git installed, see error message. Git installed, need set Git configuration. defaults fine, need set name email address, address used create GitHub account. can opening terminal window typing following commands: Alternatively, can use usethis package R set Git environment. Open RStudio run following code console:","code":"git config --global user.name \"Your Name\" git config --global user.email \"your.email@email.edu\" if (!require(\"usethis\")) {   install.packages(\"usethis\") } library(usethis)  use_git_config(user.name = \"Your Name\", user.email = \"your.email@school.edu\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"how-do-i-set-up-github","dir":"Articles","previous_headings":"","what":"How do I set up GitHub?","title":"Setting up Git and GitHub","text":"set GitHub, need create account. can <github.com>. service free extra features available students educators. created account, able create repositories. can also create organizations teams. can use collaborate others projects.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"how-do-i-manage-my-project-with-git-and-github","dir":"Articles","previous_headings":"","what":"How do I manage my project with Git and GitHub?","title":"Setting up Git and GitHub","text":"Git installation Github account set , can manage local remote repositories. can command line, recommedable use Git pane RStudio interface Git GitHub. can perform various key operations. introduce operations related common tasks textbook. . Copy remote lab repository local machine: Navigate repository GitHub Click 'Code' button copy clone URL (https) File > New Project > Version Control > Git Paste URL repository Choose directory location project Create Project B. Fork clone remote template repository local machine: Navigate repository GitHub Click 'Fork' button Select account fork repository forked repository, click 'Code' button copy clone URL (https) File > New Project > Version Control > Git Paste URL forked repository Choose directory location project Create Project C. Create/ Join remote repository clone local machine: Click 'New' button top right corner repositories listing page Name repository Select 'Public' 'Private' Select 'Initialize repository README' Click 'Create Repository' new repository, click 'Code' button copy clone URL (https) File > New Project > Version Control > Git Paste URL repository Choose directory location project Create Project Tip Bryan Hester (2020) excellent reference resource things Git GitHub R users.","code":""},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Creating reproducible examples","text":"guide, explore create reproducible examples using reprex package (Bryan et al. 2022) R. Reproducible examples essential effective communication collaboration among data scientists statisticians. make great R reproducible example Outcomes ...","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"what-is-a-reproducible-example","dir":"Articles","previous_headings":"Introduction","what":"What is a Reproducible Example?","title":"Creating reproducible examples","text":"Reproducible examples crucial effectively communicating problems, solutions, ideas world data science. post, discuss importance reproducible examples demonstrate create using reprex package R. reproducible example, often referred \"reprex,\" minimal, self-contained piece code demonstrates specific issue concept. include: brief description problem question necessary data reproduce issue R code used generate output actual output, including error messages warnings","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"why-use-the-reprex-package","dir":"Articles","previous_headings":"Introduction","what":"Why Use the reprex Package?","title":"Creating reproducible examples","text":"reprex package R streamlines process creating reproducible examples : Automatically capturing code, input data, output Formatting example easy sharing various platforms (e.g., GitHub, Stack Overflow) Encouraging best practices creating clear concise examples","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"installing-and-loading-the-reprex-package","dir":"Articles","previous_headings":"Introduction","what":"Installing and Loading the reprex Package","title":"Creating reproducible examples","text":"get started reprex package, first install CRAN load R session:","code":"install.packages(\"reprex\") library(reprex)"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"creating-a-reproducible-example-with-reprex","dir":"Articles","previous_headings":"Introduction","what":"Creating a Reproducible Example with reprex","title":"Creating reproducible examples","text":"section, demonstrate create reproducible example using reprex package.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"basic-usage","dir":"Articles","previous_headings":"Introduction > Creating a Reproducible Example with reprex","what":"Basic Usage","title":"Creating reproducible examples","text":"create simple reprex, write R code call reprex() function: generate formatted output includes code, input data, results.","code":"library(reprex)  code <- ' x <- 1:10 mean(x) '  reprex(input = code)"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"customizing-output-format","dir":"Articles","previous_headings":"Introduction > Creating a Reproducible Example with reprex","what":"Customizing Output Format","title":"Creating reproducible examples","text":"can customize output format reprex specifying venue argument. example, create reprex suitable GitHub, use:","code":"reprex(input = code, venue = \"gh\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"including-data","dir":"Articles","previous_headings":"Introduction > Creating a Reproducible Example with reprex","what":"Including Data","title":"Creating reproducible examples","text":"example requires specific data, can include using dput() function: incorporate data reprex, allowing others reproduce example easily.","code":"data <- data.frame(x = 1:10, y = 11:20) data_dput <- dput(data)  code_with_data <- ' data <- {{ data_dput }} plot(data$x, data$y) '  reprex(input = code_with_data)"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"sharing-your-reproducible-example","dir":"Articles","previous_headings":"Introduction","what":"Sharing Your Reproducible Example","title":"Creating reproducible examples","text":"created reprex, can share various platforms GitHub, Stack Overflow, via email. formatted output generated reprex package ensures example easy read understand.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"conclusion","dir":"Articles","previous_headings":"Introduction","what":"Conclusion","title":"Creating reproducible examples","text":"blog post, discussed importance reproducible examples demonstrated create using reprex package R. creating clear concise reprexes, can effectively communicate problems, solutions, ideas peers collaborators. Give reprex package try see can improve workflow!","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"references","dir":"Articles","previous_headings":"Introduction","what":"References","title":"Creating reproducible examples","text":"StackOverflow: R, Git, RStudio, GitHub Reddit: R, Git, RStudio, Github RStudio Community https://reprex.tidyverse.org/ https://github.com/MilesMcBain/datapasta Datapasta package allows copy paste data frames RStudio reprex. useful tool creating reproducible examples. example use datapasta create reprex.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"repositories","dir":"Articles","previous_headings":"Published","what":"Repositories","title":"Identifying data and data sources","text":"Language-dedicated repositories great source data language research. included listing commonly used repositories. Table 1: Data repositories","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"corpora-and-datasets","dir":"Articles","previous_headings":"Published","what":"Corpora and datasets","title":"Identifying data and data sources","text":"included listing corpora datasets available language research. list exhaustive, includes common corpora datasets used language research. Table 2: Corpora language datasets","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"data-sharing-platforms","dir":"Articles","previous_headings":"Published","what":"Data sharing platforms","title":"Identifying data and data sources","text":"https://dataverse.org/ https://osf.io/ https://www.zenodo.org/ https://figshare.com/ https://www.researchgate.net/ https://www.researchsquare.com/","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"aggregated-listings","dir":"Articles","previous_headings":"Published","what":"Aggregated listings","title":"Identifying data and data sources","text":"list data available language research constantly growing. document wide variety resources. Table 3 included attempts others provide summary corpus data language resources available. Table 3: Aggregated listings language corpora datasets","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"application-programming-interfaces-apis","dir":"Articles","previous_headings":"Custom-built","what":"Application programming interfaces (APIs)","title":"Identifying data and data sources","text":"many APIs available accessing language corpora datasets. included R packages provide access resources. Table 4: R Package APIs language corpora datasets.)","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"other-language-resources","dir":"Articles","previous_headings":"","what":"Other language resources","title":"Identifying data and data sources","text":"Data language research limited (primary) text sources. sources may include processed data previous research; word lists, linguistic features, etc.. Alone combination text sources data can rich viable source data research project. Table 5: language resources","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"html-language-of-the-web","dir":"Articles","previous_headings":"","what":"HTML: language of the web","title":"Web scraping with R","text":"HTML cousin XML (eXtensible Markup Language) organizes web documents hierarchical format read browser navigate web. Take example toy webpage created demonstration (fig-example-webpage?). Figure 1: Example web page. file accessed browser render webpage test.html plain-text format seen (exm-html-structure?). element file delineated opening closing HTML tag, <head><\/head>. Tags nested within tags create structural hierarchy. Tags can take class id labels distinguish tags often contain attributes dictate tag behave according Cascading Style Sheet (CSS) rules rendered visually browser. example, two <div> tags toy example: one label class = \"intro\" class = \"conc\". <div> tags often used separate sections webpage may require special visual formatting. <> tag, hand, creates web link. part tag's function, requires attribute href= web protocol --case link email address mailto:francojc@wfu.edu. often , however, href= contains URL (Uniform Resource Locator). working example might look like : <href=\"https://francojc.github.io/\">homepage<\/>. {{< fa medal >}} Dive deeper Cascading Style Sheets (CSS) used dictate HTML elements rendered visually browser. example, div tag class attribute intro targeted CSS rule render text larger font size bold. CSS rules often written separate file linked HTML file. web scraping purposes, however, interested visual rendering HTML file, rather structure HTML file. tag attributes can provide useful information parsing HTML files. aim web scrape download HTML file(s) contain data interested . include information may ultimately need, downloading raw source HTML effectively creating local archive, copy, webpage. Thus, webpage updated removed web, still access data accessed. Later curation process parse (.e. read extract) target information relevant research hand. However, often useful parse raw HTML process acquiring data interested harvesting data multiple pages like use HTML structure guide data extraction (.e. URLs pages) provide preliminary background working HTML, use toy example demonstrate read parse HTML using R. use rvest(Wickham 2022) package. First, install/load package, , read parse HTML character vector named web_file assigning result html. (exm-read-html-toy?) read_html() retrieves raw HTML makes accessible parsing R. subtype XML, read_html() converts raw HTML object class xml_document, can see calling class() html object (exm-class-html-toy-class?). object class xml_document represents HTML tag node. tag nodes elements can accessed using html_elements() function specifying tag/node/element isolate. Notice output (exm-parse-html-toy-1?) returned div tags respective children, tags contained within. isolate one tags class, add class name tag separating .. Great. Now say want drill isolate subordinate <p> nodes. can add p node filter, (exm-parse-html-toy-3?). extract text contained within node use html_text() function. result (exm-parse-html-toy-4?) character vector two elements corresponding text contained <p> tag. paying close attention might noticed second element vector includes extra whitespace period. trim leading trailing whitespace text can add trim = TRUE argument html_text(), (exm-parse-html-toy-5?). basic understanding read parse HTML, can now turn realistic example.","code":"<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\"> <html>   <head>     <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />     <title>My website<\/title>   <\/head>   <body>     <div class=\"intro\">       <p>Welcome!<\/p>       <p>This is my first website.<\/p>     <\/div>     <table>       <tr>         <td>Contact me:<\/td>         <td>           <a href=\"mailto:francojc@wfu.edu\">francojc@wfu.edu<\/a>         <\/td>       <\/tr>     <\/table>     <div class=\"conc\">       <p>Good-bye!<\/p>     <\/div>   <\/body> <\/html> ## {html_document} ## <html> ## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ... ## [2] <body>\\n    <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is  ... ## [1] \"xml_document\" \"xml_node\" ## {xml_nodeset (2)} ## [1] <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is my first web ... ## [2] <div class=\"conc\">\\n      <p>Good-bye!<\/p>\\n    <\/div> ## {xml_nodeset (1)} ## [1] <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is my first web ... ## {xml_nodeset (2)} ## [1] <p>Welcome!<\/p> ## [2] <p>This is my first website.<\/p> ## [1] \"Welcome!\"                  \"This is my first website.\" ## [1] \"Welcome!\"                  \"This is my first website.\""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"federalist-papers","dir":"Articles","previous_headings":"Acquire data from the web","what":"Federalist Papers","title":"Web scraping with R","text":"Say investigate authorship question Federalist Papers following footsteps Mosteller Wallace (1963). want scrape text Federalist Papers Library Congress website. main page Federalist Papers located https://guides.loc.gov/federalist-papers/full-text can seen (fig-web-scrape-screenshot?). Figure 2: Screenshot Library Congress website Federalist Papers main page contains links text 85 papers. goal scrape archive raw HTML page parse HTML extract links papers. scrape archive raw HTML 85 papers. first step web scrape investigate site page(s) want scrape ascertain licensing restrictions. Many, websites, include plain text file robots.txt root main URL. file declares webpages 'robot' (including web scraping scripts) can access. can use robotstxt package find URLs accessible 1. next step read parse raw HTML. can using read_html() function. point captured raw HTML assigning object named html. archive raw HTML file project directory. can using write_html() function xml2 package (R-xml2?). update project directory structure can seen (exm-fed-project-dir?). Now, also want scrape HTML contains pages corresponding 85 Federalist Papers. Perusing main page can see papers organized nine groups, e.g. \"Federalist Nos. 1-10\". aim scrape HTML nine pages. can using rvest package, need identify HTML elements contain URLs first main webpage html. helpful use browser inspect specific elements webpage, much toy example (exm-html-structure?). view raw displayed HTML, browser equipped command can enable hovering mouse element page want target using right click select \"Inspect\" (Chrome) \"Inspect Element\" (Safari, Brave). split browser window vertical horizontally showing displayed raw HTML underlying webpage. can see HTML elements contain URLs (fig-fed-inspect?). Figure 3: Screenshot HTML elements containing URLs Federalist Papers take closer look source HTML (fig-fed-inspect-source?) can inspect elements contain URLs divise strategy isolating extracted. Figure 4: Screenshot source HTML Federalist Papers (fig-fed-inspect-source?) can see HTML elements contain URLs nested within <ul> element. <ul> element set class attributes (.s-lg-subtab-ul, nav, nav-pills, nav-stacked). one unique <ul> element can use isolate element. search HTML <ul> elements page. output (exm-fed-papers-loc-url-ul?) shows 4 <ul> elements page. little hard see output, second <ul> element one targeting, contains class attributes identified (fig-fed-inspect-source?). can use html_attr() function extract class attribute <ul> elements see one unique <ul> element want isolate. Effectively case, second <ul> element output (exm-fed-papers-loc-url-ul-class?) unique class attribute, .s-lg-subtab-ul. can use isolate element using html_nodes() function. pipe another html_nodes() function isolate <li> elements nested within <ul> element. See (exm-fed-papers-loc-url-ul-class-s-lg-subtab-ul?). Great. Now, get URLs add another html_nodes() function (exm-fed-papers-loc-url-ul-class-s-lg-subtab-ul?) isolate <> elements nested within <li> elements function html_attr() extract value attribute. case, attribute <> elements want href. See (exm-fed-papers-loc-url-ul-li-?). can assign URLs variable, fed_urls. URLs hand, can now retrieve HTML nine pages. can, course, manually, (exm-fed-papers-rewrite-html-manual?). tedious error prone, furthermore, scale well. 1000 URLs retrieve HTML , write 1000 lines code. Instead, can write function us. See (exm-fed-papers-rewrite-function?). function (exm-fed-papers-rewrite-function?) takes URL argument. creates file name path URL. file name last part URL, extension .html. file path path federalist_papers directory data/original directory. function reads HTML URL writes file. (exm-fed-papers-rewrite-function?) might seem like step (exm-fed-papers-rewrite-html-manual?), . can now use walk() function purrr iterate URLs fed_urls apply read_write_html() function URL. See (exm-fed-papers-rewrite-map?). {{< fa regular hand-point->}} Tip processing multiple webpages, often important manage load server. R, can use Sys.sleep() introduce short delays requests. helps reduce server load iterating list webpages. example, can use Sys.sleep(1) function introduce 1 second delay requests. Another tip use message() function print status message console. can helpful processing large number webpages. result (exm-fed-papers-rewrite-map?) can seen project directory (exm-fed-papers-directory?). course, finish acquisition process, need ensure documented code created data origin file. Since created resource much information use document. Keep mind data origin file written way transparent researcher -collaborators general research community. section, built previously introduced R coding concepts employed various others process acquiring data web. also considered topics general nature concern interacting data found internet. likely appreciate, web scraping often requires knowledge familiarity R well web technologies. Rest assured, however, practice increase confidence abilities. encourage practice websites.","code":"## [1] TRUE ## {html_document} ## <html lang=\"en\"> ## [1] <head>\\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\">\\n<meta http ... ## [2] <body class=\"s-lg-guide-body\">\\r\\n<a id=\"s-lg-public-skiplink\" class=\"ale ... data/ |── analysis/ ├── derived/ └── original/     └── federalist_papers/         └── main.html ## {xml_nodeset (4)} ## [1] <ul class=\"nav nav-pills nav-stacked split-button-nav\" role=\"menu\">\\n<li  ... ## [2] <ul class=\"s-lg-subtab-ul nav nav-pills nav-stacked\">\\n<li class=\"\"><a ti ... ## [3] <ul id=\"s-lg-page-prevnext\" class=\"pager s-lib-hide\">\\n<li class=\"previou ... ## [4] <ul id=\"s-lg-guide-header-attributes\" class=\"\">\\n<li id=\"s-lg-guide-heade ... ## [1] \"nav nav-pills nav-stacked split-button-nav\" ## [2] \"s-lg-subtab-ul nav nav-pills nav-stacked\"   ## [3] \"pager s-lib-hide\"                           ## [4] \"\" ## {xml_nodeset (9)} ## [1] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [2] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [3] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [4] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [5] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [6] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [7] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [8] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [9] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [1] \"https://guides.loc.gov/federalist-papers/text-1-10\"  ## [2] \"https://guides.loc.gov/federalist-papers/text-11-20\" ## [3] \"https://guides.loc.gov/federalist-papers/text-21-30\" ## [4] \"https://guides.loc.gov/federalist-papers/text-31-40\" ## [5] \"https://guides.loc.gov/federalist-papers/text-41-50\" ## [6] \"https://guides.loc.gov/federalist-papers/text-51-60\" ## [7] \"https://guides.loc.gov/federalist-papers/text-61-70\" ## [8] \"https://guides.loc.gov/federalist-papers/text-71-80\" ## [9] \"https://guides.loc.gov/federalist-papers/text-81-85\" read_write_html <- function(url) {   Sys.sleep(1) # 1 second delay   # ... } read_write_html <- function(url) {   Sys.sleep(1) # 1 second delay   message(\"Processing \", url) # Prints: \"Processing https://www.example.com\"   # ... } data/ |-- analysis/ |-- derived/ └── original/     └── federalist_papers/         |-- main.html         |-- text-1-10.html         |-- text-11-20.html         |-- text-21-30.html         |-- text-31-40.html         |-- text-41-50.html         |-- text-51-60.html         |-- text-61-70.html         |-- text-71-80.html         └── text-81-85.html"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"orientation","dir":"Articles","previous_headings":"Curate data","what":"Orientation","title":"Web scraping with R","text":"provide example curation process using semi-structured data, work Federalist Papers data acquired web scrape Library Congress website (sec-acquire-data?). data stored series HTML files, seen (exm-cd-federalist-data-files?). data origin file, fed_papers_do.csv (tbl-cd-fed-data-origin?), gives us overview data. file structure data origin description, can surmise working HTML files contain 85 Federalist Papers. 85 papers grouped 9 files, meaning file multiple papers contained within. can also see HTML files named according range papers contained within file. example, text-1-10.html file contains first 10 papers. also good idea inspect data files . Since HTML files, can open web browser. (fig-cd-federalist-html?) shows text-1-10.html file opened web browser. Figure 5: text-1-10.html file opened web browser. point want think curated dataset look like terms rows columns. columns, helpful think variables can extract Federalist Papers. example, can extract paper number, paper title, paper's author(s), venue paper published, paper's text. variables, paper number, title, author(s) metadata paper, venue metadata publication paper, leave venue curated dataset. rows, can think unit analysis . want conduct text analysis Federalist Papers predict author paper based features text, unit analysis paper. Now, can envision case row paper, may case structure papers, namely paragraphs, use us. keep mind work curation process. information mind, idealized version curated dataset shown (tbl-cd-fed-data-idealized?).","code":"data/ ├── analysis/ ├── derived/ └── original/     │── fed_papers_do.csv     └── federalist_papers/         ├── main.html         ├── text-1-10.html         ├── text-11-20.html         ├── text-21-30.html         ├── text-31-40.html         ├── text-41-50.html         ├── text-51-60.html         ├── text-61-70.html         ├── text-71-80.html         └── text-81-85.html"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"tidy-the-data","dir":"Articles","previous_headings":"Curate data","what":"Tidy the data","title":"Web scraping with R","text":"idealized dataset structure guide work. extract data metadata files need take closer look structure HTML documents. start HTML file opened browser (fig-cd-federalist-html?) look HTML source code browser's inspect tool. (fig-cd-federalist-html-inspect?) shows HTML source code first paper text-1-10.html file. Figure 6: HTML source code first paper text-1-10.html file. structure HTML files suggests desired content within div tag, forms kind box around paper. multiple div tags file, need find way identify div tag contains desired content, content. can see div tag want two class attributes, s-lib-box s-lib-box-std. div tag contains h2 tag paper number appears. first p tag contains title paper. second p tag contains venue paper. third p tag contains author paper. remaining p tags contain text paper. closer view div tag shown (fig-cd-federalist-html-inspect-2?). Figure 7: closer view div tag containing first paper text-1-10.html file. read text-1-10.html file use testing ground extracting relevant information. Load rvest package read file read_html(), (exm-cd-federalist-html?). Given discovered HTML inspection, extract div tags s-lib-box s-lib-box-std class attributes. can use html_elements() function extract div tags append .s-lib-box.s-lib-box-std html_elements() function specify class attributes. (exm-cd-federalist-html-div?) shows result assigned object called fed_divs. Using html_attr(\"class\") function can see fed_divs object file contains 11 div tags, div tags want, one also includes s-lib-floating-box class. can exclude adding :(.s-lib-floating-box) CSS selector. worth checking one two HTML files see consistent pattern. inspection HTML files data, turns first HTML file .s-lib-floating-box class. CSS solution might way go. alternative, R-side solution use div.s-lib-box, , subset fed_divs vector exclude first element, extract div tag located. (exm-cd-federalist-html-div-subset?) shows result assigned object called fed_divs. Assuming moment solution (exm-cd-federalist-html-div-subset?) way go, can move forward extract paper number, title, author, text div tag fed_divs. can use html_elements() function extract h2 tag, contains paper number, later work p tags, contain title, author, text. already isolated relevant div tags, using fed_divs object can now continue use html_elements() function extract HTML elements within. paper number h2 tag, immediately div tag. can use html_element() function extract single h2 tag div fed_divs, (exm-cd-federalist-html-h2?). result vector contains h2 tag div tag fed_divs, complete HTML tags attributes. can use html_text() function extract text h2 tag, (exm-cd-federalist-html-h2-text?). included str_trim() function stringr package remove potential whitespace text code (exm-cd-federalist-html-h2-text?). result vector paper numbers, can later leverage create new column dataset. Next, can extract p tags fed_divs object. p tags contain title, author, text. noted , first p tag inside div tag contains paper title. Targeting p tag requires use CSS selector, :nth-child(). CSS selector allows us arbitrarily select p tag want order appears. case, want first p tag, can use :nth-child(1). (exm-cd-federalist-html-p-title?) shows result extracting text assigned object called titles. {{< fa medal >}} Dive deeper CSS selectors powerful tool extracting data HTML files. :nth-child() selector just one many. information CSS selectors, see W3Schools CSS Selector Reference. rvest package supports many, CSS selectors. Consult rvest::html_elements() documentation information. get vector length 13, 10. Scanning output can see likely offender 'PUBLIUS.' text. can exclude adding :(:contains(\"PUBLIUS.\")) CSS selector, (exm-cd-federalist-html-p-title-subset?). works, solution 'brittle', meaning potentially overspecific easily break. example, text title happens include 'PUBLIUS.' excluded. Furthermore, extra p tag contains text 'PUBLIUS.' included. can make solution robust looking general solution. One possibility anchor p tags div tag .clearfix class appears directly (>) p tags want, (exm-cd-federalist-html-p-title-subset-2?). Now 10 titles. Moving author, assume use approach changing nth-child argument 3. However, inspecting HTML reveals author always third p tag, sometimes second. consistent, however, author always preceded text 'Author:'. can use :contains() CSS selector select p tag contains text 'Author:'. result (exm-cd-federalist-html-p-author?), now able extract paper number, title author. dataset taking shape, can appreciate (tbl-cd-federalist-html-dataset-preview?). last step extract text paper. contents papers contained p tags follow p tag author. nice able target p tags follow author p tag. However, CSS selector allows us . alternative read p tags div.clearfix tag select p tags follow author p tag. approach requires additional step. need conduct process paper HTML file separate str_which() function identify index p tag contains author. can use str_subset() function select p tags follow author p tag. gets text single paper within single div tag. can wrap function apply div tag. Now can see text looks like first paper. Putting together first HTML file get following. can test first HTML file. try fifth HTML file, just make sure. Now can apply function HTML files. can data checks make sure right number rows columns.","code":"## {html_document} ## <html lang=\"en\"> ## [1] <head>\\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\">\\n<meta http ... ## [2] <body class=\"s-lg-guide-body\">\\r\\n<a id=\"s-lg-public-skiplink\" class=\"ale ... ##  [1] \"s-lib-box s-lib-box-std s-lib-floating-box\" ##  [2] \"s-lib-box s-lib-box-std\"                    ##  [3] \"s-lib-box s-lib-box-std\"                    ##  [4] \"s-lib-box s-lib-box-std\"                    ##  [5] \"s-lib-box s-lib-box-std\"                    ##  [6] \"s-lib-box s-lib-box-std\"                    ##  [7] \"s-lib-box s-lib-box-std\"                    ##  [8] \"s-lib-box s-lib-box-std\"                    ##  [9] \"s-lib-box s-lib-box-std\"                    ## [10] \"s-lib-box s-lib-box-std\"                    ## [11] \"s-lib-box s-lib-box-std\" ## {xml_nodeset (10)} ##  [1] <h2 class=\"s-lib-box-title\">Federalist No. 1\\n                           ... ##  [2] <h2 class=\"s-lib-box-title\">Federalist No. 2\\n                           ... ##  [3] <h2 class=\"s-lib-box-title\">Federalist No. 3\\n                           ... ##  [4] <h2 class=\"s-lib-box-title\">Federalist No. 4\\n                           ... ##  [5] <h2 class=\"s-lib-box-title\">Federalist No. 5\\n                           ... ##  [6] <h2 class=\"s-lib-box-title\">Federalist No. 6\\n                           ... ##  [7] <h2 class=\"s-lib-box-title\">Federalist No. 7\\n                           ... ##  [8] <h2 class=\"s-lib-box-title\">Federalist No. 8\\n                           ... ##  [9] <h2 class=\"s-lib-box-title\">Federalist No. 9\\n                           ... ## [10] <h2 class=\"s-lib-box-title\">Federalist No. 10\\n                          ... ##  [1] \"Federalist No. 1\"  \"Federalist No. 2\"  \"Federalist No. 3\"  ##  [4] \"Federalist No. 4\"  \"Federalist No. 5\"  \"Federalist No. 6\"  ##  [7] \"Federalist No. 7\"  \"Federalist No. 8\"  \"Federalist No. 9\"  ## [10] \"Federalist No. 10\" ##  [1] \"General Introduction\"                                                                           ##  [2] \"Concerning Dangers from Foreign Force and Influence\"                                            ##  [3] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [4] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [5] \"The Same Subject Continued: Concerning Dangers from Foreign Force and Influence\"                ##  [6] \"Concerning Dangers from Dissensions Between the States\"                                         ##  [7] \"The Same Subject Continued: Concerning Dangers from Dissensions Between the States\"             ##  [8] \"PUBLIUS.\"                                                                                       ##  [9] \"The Consequences of Hostilities Between the States\"                                             ## [10] \"PUBLIUS.\"                                                                                       ## [11] \"The Utility of the Union as a Safeguard Against Domestic Faction and Insurrection\"              ## [12] \"PUBLIUS.\"                                                                                       ## [13] \"The Same Subject Continued: The Union as a Safeguard Against Domestic Faction and Insurrection\" ##  [1] \"General Introduction\"                                                                           ##  [2] \"Concerning Dangers from Foreign Force and Influence\"                                            ##  [3] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [4] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [5] \"The Same Subject Continued: Concerning Dangers from Foreign Force and Influence\"                ##  [6] \"Concerning Dangers from Dissensions Between the States\"                                         ##  [7] \"The Same Subject Continued: Concerning Dangers from Dissensions Between the States\"             ##  [8] \"The Consequences of Hostilities Between the States\"                                             ##  [9] \"The Utility of the Union as a Safeguard Against Domestic Faction and Insurrection\"              ## [10] \"The Same Subject Continued: The Union as a Safeguard Against Domestic Faction and Insurrection\" ##  [1] \"General Introduction\"                                                                           ##  [2] \"Concerning Dangers from Foreign Force and Influence\"                                            ##  [3] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [4] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [5] \"The Same Subject Continued: Concerning Dangers from Foreign Force and Influence\"                ##  [6] \"Concerning Dangers from Dissensions Between the States\"                                         ##  [7] \"The Same Subject Continued: Concerning Dangers from Dissensions Between the States\"             ##  [8] \"The Consequences of Hostilities Between the States\"                                             ##  [9] \"The Utility of the Union as a Safeguard Against Domestic Faction and Insurrection\"              ## [10] \"The Same Subject Continued: The Union as a Safeguard Against Domestic Faction and Insurrection\" ##  [1] \"Author: Alexander Hamilton\" \"Author: John Jay\"           ##  [3] \"Author: John Jay\"           \"Author: John Jay\"           ##  [5] \"Author: John Jay\"           \"Author: Alexander Hamilton\" ##  [7] \"Author: Alexander Hamilton\" \"Author: Alexander Hamilton\" ##  [9] \"Author: Alexander Hamilton\" \"Author: James Madison\" ## [1] \"To the People of the State of New York:\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ## [2] \"AFTER an unequivocal experience of the inefficiency of the subsisting federal government, you are called upon to deliberate on a new Constitution for the United States of America. The subject speaks its own importance; comprehending in its consequences nothing less than the existence of the UNION, the safety and welfare of the parts of which it is composed, the fate of an empire in many respects the most interesting in the world. It has been frequently remarked that it seems to have been reserved to the people of this country, by their conduct and example, to decide the important question, whether societies of men are really capable or not of establishing good government from reflection and choice, or whether they are forever destined to depend for their political constitutions on accident and force. If there be any truth in the remark, the crisis at which we are arrived may with propriety be regarded as the era in which that decision is to be made; and a wrong election of the part we shall act may, in this view, deserve to be considered as the general misfortune of mankind.\" ## # A tibble: 163 × 4 ##    number           title                author                     text         ##    <chr>            <chr>                <chr>                      <chr>        ##  1 Federalist No. 1 General Introduction Author: Alexander Hamilton To the Peop… ##  2 Federalist No. 1 General Introduction Author: Alexander Hamilton AFTER an un… ##  3 Federalist No. 1 General Introduction Author: Alexander Hamilton This idea w… ##  4 Federalist No. 1 General Introduction Author: Alexander Hamilton Among the m… ##  5 Federalist No. 1 General Introduction Author: Alexander Hamilton It is not, … ##  6 Federalist No. 1 General Introduction Author: Alexander Hamilton And yet, ho… ##  7 Federalist No. 1 General Introduction Author: Alexander Hamilton In the cour… ##  8 Federalist No. 1 General Introduction Author: Alexander Hamilton I propose, … ##  9 Federalist No. 1 General Introduction Author: Alexander Hamilton THE UTILITY… ## 10 Federalist No. 1 General Introduction Author: Alexander Hamilton In the prog… ## # ℹ 153 more rows ## Rows: 163 ## Columns: 4 ## $ number <chr> \"Federalist No. 1\", \"Federalist No. 1\", \"Federalist No. 1\", \"Fe… ## $ title  <chr> \"General Introduction\", \"General Introduction\", \"General Introd… ## $ author <chr> \"Author: Alexander Hamilton\", \"Author: Alexander Hamilton\", \"Au… ## $ text   <chr> \"To the People of the State of New York:\", \"AFTER an unequivoca… ## Rows: 56 ## Columns: 4 ## $ number <chr> \"Federalist No. 51\", \"Federalist No. 51\", \"Federalist No. 52\", … ## $ title  <chr> \"The Structure of the Government Must Furnish the Proper Checks… ## $ author <chr> \"Author: Alexander Hamilton or James Madison\", \"Author: Alexand… ## $ text   <chr> \"To the People of the State of New York:\", \"TO WHAT expedient, … ## Rows: 1,073 ## Columns: 4 ## $ number <chr> \"Federalist No. 1\", \"Federalist No. 1\", \"Federalist No. 1\", \"Fe… ## $ title  <chr> \"General Introduction\", \"General Introduction\", \"General Introd… ## $ author <chr> \"Author: Alexander Hamilton\", \"Author: Alexander Hamilton\", \"Au… ## $ text   <chr> \"To the People of the State of New York:\", \"AFTER an unequivoca… ## # A tibble: 7 × 2 ##   author                                       author_count ##   <chr>                                               <int> ## 1 Author: Alexander Hamilton                            659 ## 2 Author: Alexander Hamilton and James Madison           25 ## 3 Author: Alexander Hamilton or James Madison            93 ## 4 Author: James Madison                                 147 ## 5 Author: John Jay                                       81 ## 6 Author: Alexander Hamilton                             27 ## 7 Author: Alexander Hamilton and James Madison           41 ## # A tibble: 85 × 2 ##    number            number_count ##    <chr>                    <int> ##  1 Federalist No. 1            11 ##  2 Federalist No. 10           24 ##  3 Federalist No. 11           15 ##  4 Federalist No. 12           13 ##  5 Federalist No. 13            5 ##  6 Federalist No. 14           13 ##  7 Federalist No. 15           16 ##  8 Federalist No. 16           12 ##  9 Federalist No. 17           15 ## 10 Federalist No. 18           21 ## 11 Federalist No. 19           20 ## 12 Federalist No. 2            15 ## 13 Federalist No. 20           25 ## 14 Federalist No. 21           14 ## 15 Federalist No. 22           19 ## 16 Federalist No. 23           13 ## 17 Federalist No. 24           14 ## 18 Federalist No. 25           11 ## 19 Federalist No. 26           15 ## 20 Federalist No. 27            7 ## 21 Federalist No. 28           11 ## 22 Federalist No. 29           14 ## 23 Federalist No. 3            19 ## 24 Federalist No. 30           12 ## 25 Federalist No. 31           13 ## 26 Federalist No. 32            6 ## 27 Federalist No. 33            9 ## 28 Federalist No. 34           12 ## 29 Federalist No. 35           12 ## 30 Federalist No. 36           18 ## 31 Federalist No. 37           17 ## 32 Federalist No. 38           12 ## 33 Federalist No. 39           17 ## 34 Federalist No. 4            18 ## 35 Federalist No. 40            3 ## 36 Federalist No. 41            6 ## 37 Federalist No. 42            7 ## 38 Federalist No. 43            7 ## 39 Federalist No. 44            7 ## 40 Federalist No. 45            8 ## 41 Federalist No. 46            9 ## 42 Federalist No. 47            8 ## 43 Federalist No. 48            7 ## 44 Federalist No. 49            5 ## 45 Federalist No. 5            13 ## 46 Federalist No. 50            7 ## 47 Federalist No. 51            2 ## 48 Federalist No. 52            6 ## 49 Federalist No. 53            4 ## 50 Federalist No. 54            5 ## 51 Federalist No. 55            9 ## 52 Federalist No. 56            4 ## 53 Federalist No. 57            9 ## 54 Federalist No. 58            2 ## 55 Federalist No. 59            2 ## 56 Federalist No. 6            20 ## 57 Federalist No. 60           13 ## 58 Federalist No. 61            7 ## 59 Federalist No. 62           20 ## 60 Federalist No. 63           22 ## 61 Federalist No. 64           16 ## 62 Federalist No. 65           12 ## 63 Federalist No. 66           15 ## 64 Federalist No. 67           12 ## 65 Federalist No. 68           11 ## 66 Federalist No. 69           12 ## 67 Federalist No. 7            11 ## 68 Federalist No. 70           25 ## 69 Federalist No. 71            8 ## 70 Federalist No. 72           15 ## 71 Federalist No. 73           16 ## 72 Federalist No. 74            5 ## 73 Federalist No. 75            9 ## 74 Federalist No. 76           11 ## 75 Federalist No. 77           11 ## 76 Federalist No. 78           22 ## 77 Federalist No. 79            6 ## 78 Federalist No. 8            14 ## 79 Federalist No. 80           22 ## 80 Federalist No. 81           20 ## 81 Federalist No. 82            7 ## 82 Federalist No. 83           38 ## 83 Federalist No. 84           22 ## 84 Federalist No. 85           15 ## 85 Federalist No. 9            18"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"write-the-data","dir":"Articles","previous_headings":"Curate data","what":"Write the data","title":"Web scraping with R","text":"...","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"course-design","dir":"Articles","previous_headings":"","what":"Course Design","title":"Instructor Guide","text":"book designed used textbook course quantitative text analysis. intended readers little experience quantitative text analysis, R programming language. Depending experience level expectations readers, however, may want consider adopting one following course designs using textbook.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"sec-p-basic-intro","dir":"Articles","previous_headings":"Course Design","what":"Basic Introduction","title":"Instructor Guide","text":"Cover chapters 1-5 sequence give readers foundational understanding quantitative text analysis. Culminate course research proposal assignment requires identify interesting linguistic problem, propose ways solving using methods covered class, identify potential data sources. readers little experience R, may want consider using RStudio Cloud platform host course. provide pre-installed R environment allow focus learning material rather troubleshooting.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"sec-p-intermediate-intro","dir":"Articles","previous_headings":"Course Design","what":"Intermediate Introduction","title":"Instructor Guide","text":"Cover chapters 1, 5-10 sequence give readers deeper understanding quantitative text analysis methods. Explore additional case studies dataset examples throughout course wish supplement lectures. Culminate course research project assignment allows readers apply learned linguistic content choice. may consider using RStudio Cloud platform host course, ensure readers access R RStudio computers well.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"sec-p-advanced-intro","dir":"Articles","previous_headings":"Course Design","what":"Advanced Introduction","title":"Instructor Guide","text":"Cover 12 chapters give readers thorough understanding quantitative text analysis concepts techniques. Devote time chapters 5-10 providing demonstrations approach different problems evaluating alternative approaches. Culminate course collaborative research project requires readers work groups conduct comprehensive analysis given dataset. Ensure readers install R RStudio computers need full control coding environment. course designs, strongly recommend evaluate readers' success understanding material providing combination quizzes, lab assignments, programming exercises, written reports. Additionally, encourage readers ask questions1, collaborate peers, seek help ample resources available online encounter scope-limited programming problems.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"slides-decks","dir":"Articles","previous_headings":"Instructor Resources","what":"Slides decks","title":"Instructor Guide","text":"Introduction Text Analysis Context Understanding Data Approaching Analysis Framing Research Data collection Data Curation Data Transformation Exploratory Data Analysis (EDA) Predictive Data Analysis (PDA) Inferential Data Analysis (IDA) Reporting Collaboration","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"lab-exercises","dir":"Articles","previous_headings":"Instructor Resources","what":"Lab exercises","title":"Instructor Guide","text":"following lab exercises available use course. exercise designed completed 50-minute lab session. ....","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"data-sets","dir":"Articles","previous_headings":"Instructor Resources","what":"Data sets","title":"Instructor Guide","text":"relatively openly available datasets allowed shared directly others. include: Project Gutenberg texts Enron Email Dataset languageR package datasets. quanteda package datasets. data sets, may able obtain permission share students course. Please contact data provider directly inquire sharing permissions. process obtaining permission share data can time consuming, recommended begin process well advance start course. resources, however, need instruct students download data . ...","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/qtalrkit.html","id":"installation","dir":"Articles","previous_headings":"qtalrkit package","what":"Installation","title":"Getting started","text":"can install development version qtalrkit GitHub :","code":"install.packages(\"remotes\") library(remotes) install_github(\"qtalr/qtalrkit\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/qtalrkit.html","id":"load","dir":"Articles","previous_headings":"qtalrkit package","what":"Load","title":"Getting started","text":"load package :","code":"library(qtalrkit)"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/qtalrkit.html","id":"installation-1","dir":"Articles","previous_headings":"swirl lessons","what":"Installation","title":"Getting started","text":"swirl lessons can downloaded within R console running: Updating lessons need update lessons, run:","code":"install.packages(\"swirl\") library(\"swirl\") install_course_github(\"qtalr\", \"lessons\") library(\"swirl\")  # Uninstall the course uninstall_course(\"lessons\")  # Reinstall the course install_course_github(\"qtalr\", \"lessons\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/qtalrkit.html","id":"load-and-run","dir":"Articles","previous_headings":"swirl lessons","what":"Load and run","title":"Getting started","text":"load start lesson run: follow instructions get started select lesson.","code":"swirl()"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"0. Writing with code","text":"recipe, introduce concept Literate Programming describe implement concept Quarto. provide demonstration features Quarto describe main structural characteristics Quarto document help get running writing documents combine code prose.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"literate-programming","dir":"Articles","previous_headings":"Concepts and strategies","what":"Literate Programming","title":"0. Writing with code","text":"First introduced Donald Knuth (1984), aim Literate Programming able combine computer code text prose one document. allows analyst run code, view output code, view code , provide prose description one document. way, literate programming document allows presenting analysis way performs computing steps desired presents easily readable format. Literate programming now key component creating sharing reproducible research (Gandrud 2015).","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"quarto","dir":"Articles","previous_headings":"Concepts and strategies","what":"Quarto","title":"0. Writing with code","text":"Quarto specific implementation literate programming paradigm. Figure 1 see example Quarto action. left see Quarto source code, combination text code. right see output Quarto source code HTML document. Figure 1: Quarto source (left) output (right) example. Quarto documents generate various types output: web documents (HTML), PDFs, Word documents, many types output formats based source code. interleaving code prose create variety output documents one attractive aspects literate programming Quarto, also possible create documents code . versatile technology come appreciate. Dive deeper see Quarto action, please check Quarto Gallery variety examples Quarto documents output. Quarto source document plain-text file extension .qmd can opened plain text reader. using RStudio IDE (henceforth RStudio) create, open, edit, generate output .qmd files plain-text reader, TextEdit (MacOS) Notepad (PC) can open files. mind, now move anatomy Quarto document.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"anatomy-of-a-quarto-document","dir":"Articles","previous_headings":"Concepts and strategies > Quarto","what":"Anatomy of a Quarto Document","title":"0. Writing with code","text":"basic level Quarto document contains two components: front-matter section prose section. third component, code block, can interleaved within prose section add code document. look turn.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"front-matter","dir":"Articles","previous_headings":"Concepts and strategies > Quarto > Anatomy of a Quarto Document","what":"Front-matter","title":"0. Writing with code","text":"front matter Quarto document appears, well, front document (top, rather). Referring back Figure 1 see front matter top. creating Quarto document RStudio default attribute keys title, author, format. front matter fenced three dashes ---. values first two keys pretty straightforward can edited needed. value format attribute can also edited tell .qmd file generate output types. Can guess value might use generate PDF document? Yep, just pdf. work Quarto learn use RStudio interface change key-value pairs add others!","code":"--- title: \"Introduction to Quarto\" author: \"Jerid Francom\" format: html ---"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"prose","dir":"Articles","previous_headings":"Concepts and strategies > Quarto > Anatomy of a Quarto Document","what":"Prose","title":"0. Writing with code","text":"Anywhere front matter contained within code block (see ) open prose. prose section(s) added functionality Markdown aware. mean, say? Well, Markdown refers set plain-text formatting conventions produce formatted text output document. quote Wikipedia: Markdown lightweight markup language creating formatted text using plain-text editor. John Gruber Aaron Swartz created Markdown 2004 markup language appealing human readers source code form. Markdown widely used blogging, instant messaging, online forums, collaborative software, documentation pages, readme files. enables us add simple text conventions signal output formatted. Say want make text bold. just add ** around text want appear bold. can also : italics *italics* links [links](http://wfu.edu) strikethrough ~~strikethrough~~ etc. Follow link find information basic Markdown syntax.","code":"**bold text**"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"code-blocks","dir":"Articles","previous_headings":"Concepts and strategies > Quarto > Anatomy of a Quarto Document","what":"Code blocks","title":"0. Writing with code","text":"Code blocks R magic happens. , referring Figure 1 see following code block. code block bound three backticks ```. first backticks curly brackets {} allow us tell Quarto programming language use evaluate (.e. run) code block. cases R, hence opening curly bracket `{r}`. languages can used Quarto, Python, SQL, Bash. previous example, R used simple calculator adding 1 + 1. code block produces. good practice label code blocks. case `#| label: add`. line code entered. mentioned selecting coding language labeling code block, code blocks various options can used determine code block used. common code block options : hiding code: #| echo: false hiding output #| include: false etc.","code":"```{r} 1 + 1 ``` 1 + 1 ## [1] 2 ```{r} #| label: add 1 + 1 ``` ```{r} #| label: add #| echo: false 1 + 1 ``` ## [1] 2 ```{r} #| label: add #| include: false 1 + 1 ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"create-and-render-a-quarto-document","dir":"Articles","previous_headings":"Concepts and strategies > Quarto","what":"Create and render a Quarto document","title":"0. Writing with code","text":"easiest efficient way create Quarto source file use RStudio point--click interface. Just use toolbar create new file select \"Quarto Document...\", seen Figure 2. Figure 2: Creating new Quarto document RStudio. provide dialogue box asking add title author document also allows select type document format output, seen Figure 3. Figure 3: Dialogue box creating new Quarto document RStudio. Enter title author leave format set HTML. clicking 'Create' get Quarto document, Figure 4, default/ boilerplate prose code blocks. prose code blocks can deleted, can start document. Figure 4: Quarto source RStudio. now, leave things see generate output report document. Click \"Render\" RStudio toolbar. render, asked save file give name. done .qmd file render format specified open 'Viewer' pane, seen Figure 5. Figure 5: Quarto source HTML output side--side RStudio. Dive deeper Watch Getting Started Quarto guided tour Quarto (Çetinkaya-Rundel 2023).","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"0. Writing with code","text":"TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"0. Writing with code","text":"concludes introduction literate programming using Quarto. covered basics much explore. preparation Lab 0, ensure completed following: Setup computing environment R RStudio quarto tinytex prepared following: Open RStudio understand basic interface Create, edit, render Quarto documents Use basic Markdown syntax format text first lab, provide guidance access lab materials. lab materials hosted GitHub. GitHub platform hosting code files. access lab materials, need create GitHub account, may sign free account wish1. , however, need verify Git installed computer, , install . start Git. Windows machine, likely need install Git. Download install Git. Go \"Assests\" section page click link latest .exe version. download executable file. Run executable file follow instructions install Git. Mac, first verify Git installed. Open Terminal application (can search using Spotlight). Terminal window, type git --version press enter. Git installed, see message says something like git version 2.43.0. see message, need install Git. Git installed, need clone lab repository. Cloning repository means download files repository computer. clone lab repository, follow steps: navigate lab repository GitHub identify blue button says \"Code\". Make sure dropdown menu selected \"Clone\" \"HTTPS\". Copy URL appears box. Open RStudio select \"New Project...\" \"File\" menu. Select \"Version Control\" \"Git\". Paste URL copied GitHub \"Repository URL\" box. Select location computer like save lab materials. Click \"Create Project\". process create folder computer lab materials. able open edit lab materials RStudio. mind, ready move Lab 0.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"1. Academic writing with Quarto","text":"implementation literate programming using course Quarto R. seen previously, Quarto provides ability combine prose code single document. powerful strategy creating reproducible documents can easily updated shared. common type writing academia research paper. recipe explore use Quarto include common elements found research papers. include: Numbered sections Table contents Cross-referencing tables figures -line citations references list Lab 1 opportunity practice concepts article summary includes features using Quarto.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"concepts-and-strategies","dir":"Articles","previous_headings":"","what":"Concepts and strategies","title":"1. Academic writing with Quarto","text":"many style components use Quarto, part addressed front-matter section part addressed prose section / code block sections. refresh memory, front-matter fenced three dashes (---) set document attributes. prose section write text document. code block section write code executed fenced three backticks (```) name code interpreter {r} (R us). mind look elements turn.","code":"--- title: \"My document title\" format: pdf ---  This is the prose section.  ```{r} #| label: example-code-block 1 + 1 ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"numbered-sections","dir":"Articles","previous_headings":"Concepts and strategies","what":"Numbered sections","title":"1. Academic writing with Quarto","text":"number sections Quarto, use number_sections key value yes. set front-matter section, nested value document type rendered. example, number sections PDF document, set number-sections key true front-matter section follows: Headers prose section numbered automatically. example, following markdown: render :  can also control depth numbering setting number-depth key front-matter section. example, number sections subsections, subsubsections, set number-depth key 2 follows: Now first second headers numbered formated third subsequent headers formatted. reason want turn numbering specific header, can add {.unnumbered} end header. example, following markdown: particularly useful academic writing want add reference, materials, section numbered end document. Warning Note header unnumbered, next header numbered unnumbered header exist. can unexpected results children unnumbered header.","code":"--- title: \"My document title\" format:    pdf:     number-sections: true --- # Section  ## Subsection  ### Subsubsection  #### Subsubsubsection  ##### Subsubsubsubsection --- title: \"My document title\" format:    pdf:     number-sections: true     number-depth: 2 --- # Section {.unnumbered}"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"table-of-contents","dir":"Articles","previous_headings":"Concepts and strategies","what":"Table of contents","title":"1. Academic writing with Quarto","text":"longer documents including table contents can useful way help readers navigate document. include table contents Quarto, use toc key value true. , front-matter section, nested format value, seen : Tip PDF Word document outputs, table contents automatically generated placed beginning document. HTML documents, table contents placed sidebar default. headers numbered, appeared numbered table contents. unnnumbered header, appear section number. section numbering, can also control depth table contents setting toc-depth key front-matter section. example, include sections subsections, subsubsections, set toc-depth key 2 follows: section numbering can avoid listing header table contents adding {.unlisted} end header.","code":"--- title: \"My document title\" format:    pdf:     toc: true --- --- title: \"My document title\" format:    pdf:     toc: true     toc-depth: 2 ---"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"cross-referencing-tables-and-figures","dir":"Articles","previous_headings":"Concepts and strategies","what":"Cross-referencing tables and figures","title":"1. Academic writing with Quarto","text":"Another key element academic writing using cross-references tables figures. allows us refer table figure number without manually update number add remove table figure. case, need add anything front-matter section. Instead, modify keys code block section code-generated table figure. cross-reference table figure, need add prefix label key's value. prefix, either tbl- fig-, indicates whether label table figure. Additionally, table figure captions can added tbl-cap fig-cap keys, respectively. look basic figure can cross-reference. following code block generate simple scatterplot. Figure 1: scatterplot Figure 1 see scatterplot. .... tables generated R, process similar figures. difference use tbl- prefix label value tbl-cap key instead fig-cap key caption. can also create tables using markdown syntax. case, format little different. Consider Table 1, example. Table 1:  simple table","code":"```{r} #| label: fig-scatterplot #| fig-cap: \"A scatterplot\"  plot(x = 1:10, y = 1:10) ```  In @fig-scatterplot we see a scatterplot. .... | Column 1 | Column 2 | Column 3 | |----------|----------|----------| | A        | B        | C        | | D        | E        | F        |  : A simple table {#tbl-table-1}"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"in-line-citations-and-references-list","dir":"Articles","previous_headings":"Concepts and strategies","what":"In-line citations and references list","title":"1. Academic writing with Quarto","text":"last element cover adding citations references list Quarto document. add citations need three things: bibliography file reference bibliography file front-matter section citation prose section contained bibliography file bibliography file plain text file contains citations want use document. file requires extension .bib formatted using BibTeX format. BibTeX reference syntax commonly used academia. take look sample file, bibliography.bib, contains single reference. file can see reference includes information type publication, title, author, year, journal, volume, number, pages, DOI. can find BibTeX formatted references almost everywhere can find scholarly work. example, Google Scholar, Web Science, Scopus provide BibTeX formatted references. Additionally, many journals provide BibTeX formatted references articles publish. Dive deeper Managing references can challenge begin amass large number . number tools can help manage references. example, Zotero free, open-source reference manager can help organize references generate BibTeX formatted references. Zotero also browser extension allows easily add references Zotero library browser. Furthermore, Zotero can connected RStudio facilitate incorporation BibTeX formatted references Quarto document. See RStudio documentation information. front-matter Quarto document, need add reference bibliography file. done using bibliography key. example, bibliography file called bibliography.bib located directory Quarto document, add following front-matter section: bibliography file reference bibliography file front-matter section, can now add citations document. , use @ symbol followed citation key prose section. example, cite tidyverse2019 reference bibliography.bib file, add @tidyverse2019 prose section follows: citation appear rendered document. citation (tidyverse2019?). automatically, rendering document, references list added end document. reason citations document, good idea include header section # References end document. Tip number ways inline citations appear. example, parentheses, multiple citations, year, adding page number, etc.. information format citations, see Quarto documentation.","code":"@Article{tidyverse2019,   title = {Welcome to the {tidyverse}},   author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},   year = {2019},   journal = {Journal of Open Source Software},   volume = {4},   number = {43},   pages = {1686},   doi = {10.21105/joss.01686}, } --- title: \"My document title\" format: pdf bibliography: bibliography.bib --- This is a citation to @tidyverse2019."},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"1. Academic writing with Quarto","text":"Consider following front-matter sections, B. B Choose whether following statements true false. TRUEFALSE Section numbering included PDF output B. TRUEFALSE Section numbering applied first three levels headers PDF output B. TRUEFALSE table contents included PDF output B. TRUEFALSE table contents included PDF output B, include first two levels headers. Now respond following questions. @tbl-scatterplot@fig-scatterplot@scatterplot cross-reference figure label fig-scatterplot. front-matter key include path file contains BibTeX formatted references.","code":"--- title: \"My document title\" format:    pdf:     number-sections: true     number-depth: 3     toc: false --- --- title: \"My document title\" format:    pdf:     number-sections: true     toc: true     toc-depth: 2 ---"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"1. Academic writing with Quarto","text":"rounds introduction academic writing Quarto. Lab 1 opportunity practice concepts article summary includes features using Quarto. preparation Lab 1, ensure prepared following: PDF document Word document document numbered sections document table contents document path bibliography file Add inline citation prose section Quarto document Also, since article summary, prepared : research question data used methods used results/ findings study BibTeX formatted reference article","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-10.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"10. Inferential Statistics","text":"(... examples ...) TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-11.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"11. Research write-ups","text":"(... examples ...) TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-12.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"12. Research sharing and collaboration","text":"(... examples ...) TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"2. Reading, inspecting, and writing data","text":"Recipes guides process reading, inspecting, writing data using R packages functions Quarto environment. learn effectively combine code narrative create reproducible document can shared others. cover following topics: Loading packages R session Reading data R read_*() functions Inspecting data dplyr functions Writing data file write_*() functions Lab 2, able apply skills employ Quarto skills create well-documented reproducible document.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"quarto-documents-and-code-blocks","dir":"Articles","previous_headings":"Concepts and strategies","what":"Quarto documents and code blocks","title":"2. Reading, inspecting, and writing data","text":"Ask remember Lab 0 1, Quarto documents can combine prose code. prose written Markdown code written R1. code contained code blocks, opened three backticks (`), name programming language, r, curly braces {r} three backticks (`) close block. example, following minimal Quarto document contains R code block: Code blocks various options can added using key-value pairs prefixed #|. common key-value pairs use Recipe : label: unique name code block. used reference code block. echo: boolean value (true false) determines whether code displayed output document. include: boolean value (true false) determines whether output code displayed output document. message: boolean value (true false) determines whether messages code displayed output document.","code":"--- title: My Quarto Document format: pdf ---  # Goals  This script ...  ```{r} #| label: code-block-name  # R code goes here ```  As you can see in the code block, the ..."},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"setting-up-the-environment","dir":"Articles","previous_headings":"Concepts and strategies","what":"Setting up the environment","title":"2. Reading, inspecting, and writing data","text":"can read, inspect, write data, need load packages contain functions use. use readr package read data R write data disk dplyr package inspect transform (subset) data. ways load packages R session. common way use library() function. library() function loads package R session stops script package available current computing environment. example, following code block loads readr dplyr packages R session: code block assumes readr dplyr packages installed current computing environment. packages installed, code block stop display error message, : error can addressed installing missing package install.packages(\"readr\") re-running code block. ideal reproducibility, however, code block stop package installed. consider reproducible approach later course. Dive deeper interested learning safeguarding package loading reproducible way, see renv package. renv package project-oriented workflow create reproducible environment R projects. information, see renv documentation.","code":"```{r} #| label: load-packages  # Load packages library(readr) # for reading and writing data library(dplyr) # for inspecting and transforming data ``` Error in library(readr) : there is no package called ‘readr’"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"understanding-the-data","dir":"Articles","previous_headings":"Concepts and strategies","what":"Understanding the data","title":"2. Reading, inspecting, and writing data","text":"Now environment set , can read data R. , make sure understand data looking data documentation. dataset read R session based Brown Corpus (Francis Kuçera 1961). created data origin file contains data documentation Brown Corpus, can see Table 1. Table 1: Data origin file Brown Corpus. data origin file provides overview original data source. case, dataset read R subset Brown Corpus aggregate use passive voice. data developed authors corpora package (Evert 2023). exported data CSV file, read R. data dictionary describes dataset read appears Table 2. Table 2: Data dictionary file Brown Corpus. information, now position read inspect dataset.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"reading-data-into-r-with-readr","dir":"Articles","previous_headings":"Concepts and strategies","what":"Reading data into R with readr","title":"2. Reading, inspecting, and writing data","text":"now prepared Quarto document loading packages use reviewed data documentation understand data read R. now ready read data R. R provides number functions read data many types R. explore many types data datasets course. now, focus reading rectangular data R. Rectangular data data organized rows columns, spreadsheet. One common file formats rectangular data comma-separated values (CSV) file. CSV files text files lines represent rows commas separate columns data. example, sample CSV file snippet contains three rows three columns data: CSV file type delimited file, means data separated delimiter. case CSV file, delimiter comma. types delimited files use different delimiters, tab-separated values (TSV) files use tab character delimiter, even pipe (|) semicolon (;). readr package provides functions read rectangular data R. read_csv() function reads CSV files, read_tsv() function reads TSV files, read_delim() function reads types delimited files. use read_csv() function read brown_passives_curated.csv file R. use file = argument specify path file. Now, file \"path\" location file computer. can specify path two ways: Relative path: relative path path file relative current working directory. current working directory directory R session running. Absolute path: absolute path path file root directory computer. purpose, relative path better option portable. example, share code someone else, may different absolute path file. However, likely relative path file. say directory structure project follows: case, relative path reading-inspecting-writing.qmd brown_passives_curated.csv file ../data/derived/brown_passives_curated.csv. .. means \"go one directory\" rest path path file project/ directory. mind, can read brown_passives_curated.csv file R following code block: Running code chunk Quarto document read data R assign brown_passives_df variable. also show code used read data R. Furthermore, functions display messages output. example, read_csv() function display message various parsing options used read data R, seen : information can helpful interactive session, read_csv() tells us dimensions data data types column. output necessary, unnecessarily verbose reproducible document. can hide messages produced function using message = false key-value pair code block. example, following code block read data R assign brown_passives_df variable without displaying messages: messages displayed document output.","code":"\"word\",\"frequency\",\"part_of_speech\" \"the\",69971,\"article\" \"of\",36412,\"preposition\" \"and\",28853,\"conjunction\" project/ ├── data/ │   ├── original/ │   │   └── brown_passives_do.csv │   └── derived/ │       └── brown_passives_curated.csv └── scripts/     └── reading-inspecting-writing.qmd ```{r} #| label: read-dataset-brown-passives-curated  # Read the dataset brown_passives_df <-   read_csv(file = \"../data/derived/brown_passives_curated.csv\") ``` ## Rows: 15 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \",\" ## chr (2): cat, name ## dbl (3): passive, n_w, n_s ##  ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ```{r} #| label: read-dataset-brown-passives-curated #| message: false  # Read the dataset brown_passives_df <-   read_csv(file = \"../data/derived/brown_passives_curated.csv\") ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"inspecting-data-with-dplyr","dir":"Articles","previous_headings":"Concepts and strategies","what":"Inspecting data with dplyr","title":"2. Reading, inspecting, and writing data","text":"objective section demonstrate inspect transform (subset) data using dplyr package. use dplyr package inspect data read R previous section. Reading CSV file R create data frame object. Thus, assigned result brown_passives_df. df suffix common naming convention rectangular data frames. good practice use consistent naming convention objects code. makes easier understand code avoid errors. get overview data using glimpse() function dplyr package. glimpse() function displays dimensions data frame data types column. want , tabular-like view data, can simply print data frame console. worth mentioning, readr functions return tibbles, gain benefits tibbles read data R readr functions, one worry printing data frame console, document, print data. default, printing tibbles return first 10 rows columns, unless columns numerous display width-wise. dplyr also provides set slice_*() functions allow us display data tabular fashion, additional options. three slice_*() functions cover : slice_head(): Select first n rows data frame. slice_tail(): Select last n rows data frame. slice_sample(): Select random sample n rows data frame. example, following code block select first 5 rows data frame: can also select last 5 rows data frame slice_tail() function: Finally, can select random sample 5 rows data frame slice_sample() function: functions can helpful get sense data different ways. combination arrange() function, can also sort data frame column columns select first last rows. example, following code block sort data frame passive column ascending order select first 5 rows: want sort descending order, can surround column name desc(), arrange(desc(passive)). Now, previous code block want, readable. Enter pipe operator. pipe operator |> operator allows us chain output one function input another function. allows us write readable code. result code makes sense. can read code left right, top bottom, order functions executed. Dive deeper native R pipe |> introduced R 4.1.0. using earlier version R, can use magrittr package load pipe operator %>%. certain advantages using magrittr pipe operator, including ability use pipe operator pass arguments functions placeholders. information, see magrittr documentation. addition legible, using pipe function line allows us add comments line code. example, following code block previous code block, comments added line: good practice follow writing code. makes code readable easier understand others future self!","code":"# Preview glimpse(brown_passives_df) ## Rows: 15 ## Columns: 5 ## $ cat     <chr> \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"J\", \"K\", \"L\", \"M\", \"N… ## $ passive <dbl> 892, 543, 283, 351, 853, 1034, 1460, 837, 2423, 352, 265, 104,… ## $ n_w     <dbl> 101196, 61535, 40749, 39029, 82010, 110363, 173017, 69446, 181… ## $ n_s     <dbl> 3684, 2399, 1459, 1372, 3286, 4387, 6537, 2012, 6311, 3983, 36… ## $ name    <chr> \"press reportage\", \"press editorial\", \"press reviews\", \"religi… # Print the data frame brown_passives_df ## # A tibble: 15 × 5 ##    cat   passive    n_w   n_s name             ##    <chr>   <dbl>  <dbl> <dbl> <chr>            ##  1 A         892 101196  3684 press reportage  ##  2 B         543  61535  2399 press editorial  ##  3 C         283  40749  1459 press reviews    ##  4 D         351  39029  1372 religion         ##  5 E         853  82010  3286 skills / hobbies ##  6 F        1034 110363  4387 popular lore     ##  7 G        1460 173017  6537 belles lettres   ##  8 H         837  69446  2012 miscellaneous    ##  9 J        2423 181426  6311 learned          ## 10 K         352  68599  3983 general fiction  ## # ℹ 5 more rows # Select the first 5 rows slice_head(brown_passives_df, n = 5) ## # A tibble: 5 × 5 ##   cat   passive    n_w   n_s name             ##   <chr>   <dbl>  <dbl> <dbl> <chr>            ## 1 A         892 101196  3684 press reportage  ## 2 B         543  61535  2399 press editorial  ## 3 C         283  40749  1459 press reviews    ## 4 D         351  39029  1372 religion         ## 5 E         853  82010  3286 skills / hobbies # Select the last 5 rows slice_tail(brown_passives_df, n = 5) ## # A tibble: 5 × 5 ##   cat   passive   n_w   n_s name            ##   <chr>   <dbl> <dbl> <dbl> <chr>           ## 1 L         265 57624  3673 detective       ## 2 M         104 14433   873 science fiction ## 3 N         290 69909  4438 adventure       ## 4 P         290 70476  4187 romance         ## 5 R         146 21757   975 humour # Select a random sample of 5 rows slice_sample(brown_passives_df, n = 5) ## # A tibble: 5 × 5 ##   cat   passive    n_w   n_s name           ##   <chr>   <dbl>  <dbl> <dbl> <chr>          ## 1 D         351  39029  1372 religion       ## 2 G        1460 173017  6537 belles lettres ## 3 H         837  69446  2012 miscellaneous  ## 4 N         290  69909  4438 adventure      ## 5 R         146  21757   975 humour # Sort by the `passive` column and select the first 5 rows slice_head(arrange(brown_passives_df, passive), n = 5) ## # A tibble: 5 × 5 ##   cat   passive   n_w   n_s name            ##   <chr>   <dbl> <dbl> <dbl> <chr>           ## 1 M         104 14433   873 science fiction ## 2 R         146 21757   975 humour          ## 3 L         265 57624  3673 detective       ## 4 C         283 40749  1459 press reviews   ## 5 N         290 69909  4438 adventure # Sort by the `passive` column and select the first 5 rows brown_passives_df |>   arrange(passive) |>   slice_head(n = 5) ## # A tibble: 5 × 5 ##   cat   passive   n_w   n_s name            ##   <chr>   <dbl> <dbl> <dbl> <chr>           ## 1 M         104 14433   873 science fiction ## 2 R         146 21757   975 humour          ## 3 L         265 57624  3673 detective       ## 4 C         283 40749  1459 press reviews   ## 5 N         290 69909  4438 adventure # Sort by the passive column and select the first 5 rows brown_passives_df |> # Pass the data frame to the next function   arrange(passive) |> # Sort the data frame by the passive column   slice_head(n = 5) # Select the first 5 rows"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"subsetting-data-with-dplyr","dir":"Articles","previous_headings":"Concepts and strategies","what":"Subsetting data with dplyr","title":"2. Reading, inspecting, and writing data","text":"Now sense data, can subset data create variations original data frame. can subset data frame selecting columns / rows. R lesson \"Packages Functions\", saw base R provides bracket ([]) operator subset data frames. dplyr package provides functions subset data frames can readable easier use. first look selecting columns. select() function allows us select columns name. example, following code block select passive n_w columns data frame: Beyond selecting columns, can also reorder columns rename columns. example, following code block select passive n_w columns, rename n_w column num_words, reorder columns num_words first column: Dive deeper select() also provides number helper functions select columns. example, can use starts_with() function inside select() call select columns start certain string. can select columns vector type using (.character). information, see select() documentation use ?select command R console. selecting columns others, effectively dropped columns select. effective drop columns name, can use select() function - operator. example, following code block drop cat column data frame: now turn attention subsetting rows. filter() function allows us select rows logical condition. example, following code block select rows values passive column less < 1,000: can also use filter() function select rows character string. example, following code block select rows values name column equal religion: inequality operator != can used character strings well. include multiple values, can use %% operator. case can pass vector values filter() function. example, following code block select rows values name column equal religion learned: Dive deeper sophisticated subsetting, can use str_detect() function stringr package select rows values name column contain certain string. approach enhanced later course learn regular expressions.","code":"# Select the `passive` and `n_w` columns select(brown_passives_df, passive, n_w) ## # A tibble: 15 × 2 ##    passive    n_w ##      <dbl>  <dbl> ##  1     892 101196 ##  2     543  61535 ##  3     283  40749 ##  4     351  39029 ##  5     853  82010 ##  6    1034 110363 ##  7    1460 173017 ##  8     837  69446 ##  9    2423 181426 ## 10     352  68599 ## # ℹ 5 more rows # Select rename and reorder columns brown_passives_df |>   select(num_words = n_w, passive) ## # A tibble: 15 × 2 ##    num_words passive ##        <dbl>   <dbl> ##  1    101196     892 ##  2     61535     543 ##  3     40749     283 ##  4     39029     351 ##  5     82010     853 ##  6    110363    1034 ##  7    173017    1460 ##  8     69446     837 ##  9    181426    2423 ## 10     68599     352 ## # ℹ 5 more rows # Drop the `n_w` column brown_passives_df |>   select(-cat) ## # A tibble: 15 × 4 ##    passive    n_w   n_s name             ##      <dbl>  <dbl> <dbl> <chr>            ##  1     892 101196  3684 press reportage  ##  2     543  61535  2399 press editorial  ##  3     283  40749  1459 press reviews    ##  4     351  39029  1372 religion         ##  5     853  82010  3286 skills / hobbies ##  6    1034 110363  4387 popular lore     ##  7    1460 173017  6537 belles lettres   ##  8     837  69446  2012 miscellaneous    ##  9    2423 181426  6311 learned          ## 10     352  68599  3983 general fiction  ## # ℹ 5 more rows # Select rows where `passive` is less than 1,000 brown_passives_df |>   filter(passive < 1000) ## # A tibble: 12 × 5 ##    cat   passive    n_w   n_s name             ##    <chr>   <dbl>  <dbl> <dbl> <chr>            ##  1 A         892 101196  3684 press reportage  ##  2 B         543  61535  2399 press editorial  ##  3 C         283  40749  1459 press reviews    ##  4 D         351  39029  1372 religion         ##  5 E         853  82010  3286 skills / hobbies ##  6 H         837  69446  2012 miscellaneous    ##  7 K         352  68599  3983 general fiction  ##  8 L         265  57624  3673 detective        ##  9 M         104  14433   873 science fiction  ## 10 N         290  69909  4438 adventure        ## # ℹ 2 more rows # Select rows where `name` is equal to `religion` brown_passives_df |>   filter(name == \"religion\") ## # A tibble: 1 × 5 ##   cat   passive   n_w   n_s name     ##   <chr>   <dbl> <dbl> <dbl> <chr>    ## 1 D         351 39029  1372 religion # Select multiple values brown_passives_df |>   filter(name %in% c(\"religion\", \"learned\", \"detective\")) ## # A tibble: 3 × 5 ##   cat   passive    n_w   n_s name      ##   <chr>   <dbl>  <dbl> <dbl> <chr>     ## 1 D         351  39029  1372 religion  ## 2 J        2423 181426  6311 learned   ## 3 L         265  57624  3673 detective"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"writing-data-to-a-file-with-readr","dir":"Articles","previous_headings":"Concepts and strategies","what":"Writing data to a file with readr","title":"2. Reading, inspecting, and writing data","text":"Finally, can write data, including data frames, file write_*() functions readr package. write_*() functions include: write_csv(): Write data frame CSV file. write_tsv(): Write data frame TSV file. write_delim(): Write data frame delimited file specified delimiter (|, ;, etc). create distinct data frame one read R, subset brown_passives_df data frame columns rows create new data frame contains passive, n_w, name columns rows values passive column greater > 1,000 assign brown_passives_subset_df. Now following code block write brown_passives_subset_df data frame CSV file given specified file path: Given example directory structure saw earlier, new file appears data/derived/ directory. much learn reading, inspecting, writing data R. introduce functions techniques coming lessons. now, learned read, inspect, write data using R functions Quarto code blocks!","code":"# Subset the data frame brown_passives_subset_df <-   brown_passives_df |>   select(passive, n_w, name) |>   filter(passive > 1000) # Write the data frame to a CSV file write_csv(   x = brown_passives_subset_df,   file = \"../data/derived/brown_passives_subset.csv\" ) project/ ├── data/ │   ├── original/ │   │   └── brown_passives_do.csv │   └── derived/ │       ├── brown_passives_curated.csv │       ├── brown_passives_curated_dd.csv │       └── brown_passives_subset.csv └── scripts/     └── reading-inspecting-writing.qmd"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"2. Reading, inspecting, and writing data","text":"TRUEFALSE readr package provides functions read rectangular data R. echomessageinclude option code block determines whether code displayed output document. TRUEFALSE dplyr package provides functions create data dictionaries. read_csv()read_tsv()read_delim() used read tab-separated values (TSV) files. function dplyr used select columns name? select()filter()slice_head() TRUEFALSE R pipe operator |> allows us chain output one function input another function.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"2. Reading, inspecting, and writing data","text":"Lab 2 opportunity apply skills learned Recipe create Quarto document reads, inspects, writes data. addition knowledge skills developed Labs 0 1, complete Lab 2, need able : Create code blocks Quarto document Understand purpose label, echo, message, include options code block Load packages R session library() Understand read create file relative file paths Read data R read_csv() function Inspect data frames dplyr functions glimpse(), slice_head(), slice_tail(), slice_sample(), arrange(). Use |> pipe operator chain functions together. Subset data frames dplyr functions select() filter(). Write data frames file write_csv() function.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"3. Descriptive assessment of datasets","text":"Recipe explore appropriate methods summarizing variables datasets given number informational values variable(s). build understanding summarize data using statistics, tables, plots. cover following topics: Summary overviews datasets skimr Summary statistics dplyr Creating Quarto tables knitr Creating Quarto plots ggplot2 Lab 3, put skills practice provide descriptive assessment dataset includes statistics, tables, plots using Quarto R.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"concepts-and-strategies","dir":"Articles","previous_headings":"","what":"Concepts and strategies","title":"3. Descriptive assessment of datasets","text":"Recipe, use PassiveBrownFam dataset corpora package (Evert 2023). dataset contains information passive voice usage Brown family corpora. dataset contains 11 variables 2,449 observations. assigned dataset object brown_fam_df made minor modifications variable names improve readability dataset. can learn variables reading dataset documentation ?corpora::PassiveBrownFam.","code":"# Load packages library(dplyr)  # Read the dataset from the `corpora` package brown_fam_df <-   corpora::PassiveBrownFam |> # reference the dataset   as_tibble() # convert to a tibble  # Rename variables brown_fam_df <-   brown_fam_df |> # pass the original dataset   rename( # rename variables: new_name = old_name     lang_variety = lang,     num_words = n.words,     active_verbs = act,     passive_verbs = pass,     total_verbs = verbs,     percent_passive = p.pass   )  # Preview glimpse(brown_fam_df) ## Rows: 2,499 ## Columns: 11 ## $ id              <chr> \"brown_A01\", \"brown_A02\", \"brown_A03\", \"brown_A04\", \"b… ## $ corpus          <fct> Brown, Brown, Brown, Brown, Brown, Brown, Brown, Brown… ## $ section         <fct> A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, … ## $ genre           <fct> press reportage, press reportage, press reportage, pre… ## $ period          <fct> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, … ## $ lang_variety    <fct> AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE,… ## $ num_words       <int> 2080, 2116, 2051, 2095, 2111, 2102, 2099, 2069, 2058, … ## $ active_verbs    <int> 164, 154, 135, 128, 170, 166, 165, 163, 153, 169, 132,… ## $ passive_verbs   <int> 40, 25, 34, 25, 32, 21, 31, 19, 39, 23, 17, 10, 15, 26… ## $ total_verbs     <int> 204, 179, 169, 153, 202, 187, 196, 182, 192, 192, 149,… ## $ percent_passive <dbl> 19.61, 13.97, 20.12, 16.34, 15.84, 11.23, 15.82, 10.44…"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"statistical-overviews","dir":"Articles","previous_headings":"Concepts and strategies","what":"Statistical overviews","title":"3. Descriptive assessment of datasets","text":"Understanding data utmost importance , , analysis. get know data inspecting data origin, dictionary, structure, move summarizing data. statistical overview data good place start gives us sense variables variable types dataset. can use skimr package create statistical overview data, using convienent skim() function. create statistical overview brown_fam_df dataset. output skim() function contains lot information essentially two parts: summary dataset summary variable dataset. summary variables, however, grouped variable type. Remember, variables data frame vector vector type. already learned different types vectors R, including character, numeric, logical. dataset, presented new type vector: factor. factor essentially character vector contains set discrete values, levels. Factors can ordered unordered can contain levels present data. Now, looking variable types, can see 1 character variable, 5 factor variables, 5 numeric variables. variable types assume different set summary statistics. example, can calculate mean numeric variable character variable. , can count number unique values character variable numeric variable. variables, skim() also provide number missing values percent non-missing values. Inspecting entire dataset good place start point often want focus set variables. can add yank() function extract statistical overview set variables variable types. extract statistical overview numeric variables brown_fam_df dataset.","code":"# Load packages library(skimr)  # Create a statistical overview of the `brown_fam_df` dataset skim(brown_fam_df) ── Data Summary ────────────────────────                            Values       Name                       brown_fam_df Number of rows             2499         Number of columns          11           _______________________                 Column type frequency:                    character                1              factor                   5              numeric                  5            ________________________                Group variables            None          ── Variable type: character ───────────────────────────────────────────────────────────────────────   skim_variable n_missing complete_rate min max empty n_unique whitespace 1 id                    0             1   7   9     0     2499          0  ── Variable type: factor ──────────────────────────────────────────────────────────────────────────   skim_variable n_missing complete_rate ordered n_unique top_counts                             1 corpus                0             1 FALSE          5 BLO: 500, Bro: 500, LOB: 500, FLO: 500 2 section               0             1 FALSE         15 J: 400, G: 381, F: 228, A: 220         3 genre                 0             1 FALSE         15 lea: 400, bel: 381, pop: 228, pre: 220 4 period                0             1 FALSE          3 196: 1000, 199: 999, 193: 500          5 lang_variety          0             1 FALSE          2 BrE: 1500, AmE: 999                     ── Variable type: numeric ─────────────────────────────────────────────────────────────────────────   skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50    p75   p100 hist  1 num_words               0             1 2165.  97.8  1406     2127    2163   2200   4397   ▁▇▁▁▁ 2 active_verbs            0             1  179.  56.6    39      139     170    214    551   ▃▇▂▁▁ 3 passive_verbs           0             1   25.7 12.9     2       16      23     32     86   ▆▇▂▁▁ 4 total_verbs             0             1  204.  49.1    66      170     196    234    571   ▃▇▂▁▁ 5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1   18.2   67.7 ▇▅▁▁▁ # Extract the statistical overview of the numeric variables brown_fam_df |>   skim() |>   yank(\"numeric\") ── Variable type: numeric ─────────────────────────────────────────────────────────────────────────   skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50    p75   p100 hist  1 num_words               0             1 2165.  97.8  1406     2127    2163   2200   4397   ▁▇▁▁▁ 2 active_verbs            0             1  179.  56.6    39      139     170    214    551   ▃▇▂▁▁ 3 passive_verbs           0             1   25.7 12.9     2       16      23     32     86   ▆▇▂▁▁ 4 total_verbs             0             1  204.  49.1    66      170     196    234    571   ▃▇▂▁▁ 5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1   18.2   67.7 ▇▅▁▁▁"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"summary-statistics-of-particular-variables","dir":"Articles","previous_headings":"Concepts and strategies","what":"Summary statistics of particular variables","title":"3. Descriptive assessment of datasets","text":"summary statistics useful preliminary interactive use, oftent case want focus particular variable set variables potential relationships variables. can use dplyr package calculate summary statistics particular variable set variables. can use group_by() function group data particular variable variables. can use summarize() function calculate summary statistics grouped data. example, calculate mean median percent_passive variable brown_fam_df dataset grouped lang_variety variable. result 2x3 data frame includes mean median percent_passive variable two levels lang_variety variable. group_by() function can also used group multiple variables. example, calculate mean median percent_passive variable brown_fam_df dataset grouped lang_variety genre variables. numeric variables, percent_passive, number summary statistics can calculate. seen R functions mean median can also calculate standard deviation (sd()), variance (var()), minimum (min()), maximum (max()), interquartile range (IQR()), median absolute deviation (mad()), quantiles (quantile()). calculations make sense numeric variables character variables. character variables, factors, summary statistics limited. can calculate number observations (n()) / number unique values (n_distinct()). now summarize number observations n() grouped genre variable brown_fam_df dataset. Just , can add multiple grouping variables group_by(). add lang_variety grouping calculate number observations n() grouped genre lang_variety variables brown_fam_df dataset. Tip result calculating number observations character factor variable known frequency table. Grouping two categorical variables known cross-tabulation contingency table. Now, can also pipe results group_by() summarize() another function. can say sort, select, filter results. can also perform another summary function. important, however, remember result group_by() produces grouped data frame. Subsequent functions applied grouped data frame. can lead unexpected results original grouping relevant subsequent function. avoid , can use ungroup() function remove grouping relevant grouped summary statistics calculated. return calculating number observations n() grouped genre lang_variety variables brown_fam_df dataset. add another summary uses n variable calculate mean median number observations. use ungroup() function, mean median calculated genre collapsed across lang_variety. Therefore see mean median calculated number documents corpus 15 genres. use ungroup() function, mean median calculated genres. Note use ungroup() function summaries clear grouping calculating mean median. Now see mean median calculated across genres. leave section, look ways create frequency contingency tables character factor variables. shortcut calculate frequency table character factor variable use count() function dplyr package. calculate number observations grouped genre variable brown_fam_df dataset. can also add multiple grouping variables count() create contingency tables. add lang_variety grouping create cross-tabulation genre lang_variety variables brown_fam_df dataset. Note results count() grouped need use ungroup() function calculating subsequent summary statistics. Another way create frequency contingency tables use tabyl() function janitor package (Firke 2023). create frequency table genre variable brown_fam_df dataset. addition providing frequency counts, tabyl() function also provides percent observations level variable. , can add three grouping variables tabyl() well. add lang_variety grouping create contingency table genre lang_variety variables brown_fam_df dataset. results include percent observations level variable clear calculate percent observations level variable multiple grouping variables. must specify want calculate percent observations row column. janitor package includes variety adorn_*() functions add additional information results tabyl(), including percentages, frequency, totals. adorn_percentages(): add percentages results tabyl() adorn_pct_formatting(): format percentages include % sign adorn_ns(): add frequency results tabyl() adorn_rounding(): round results tabyl() adorn_totals(): add totals results tabyl() , return adding percentages results genre lang_variety cross-tabulation. calculate -column percentages observations give ue percent observations level genre variable level lang_varity variable. words, aiming assess degree distribution genre similar different across lang_variety. can see assigned results cross-tabulation object brown_genre_lang_ct. important note results tabyl() data frames, albeit special class tabyl. Therefore, can apply subsequent operations results tabyl() data frame. However, must pay attention variable types results tabyl(). see look like numeric values results tabyl() actually character values. Just something aware working results tabyl().","code":"# Mean and median of `percent_passive` grouped by `lang_variety` brown_fam_df |>   group_by(lang_variety) |>   summarize(     mean_percent_passive = mean(percent_passive),     median_percent_passive = median(percent_passive)   ) ## # A tibble: 2 × 3 ##   lang_variety mean_percent_passive median_percent_passive ##   <fct>                       <dbl>                  <dbl> ## 1 AmE                          12.9                   11.0 ## 2 BrE                          14.8                   13.3 # Mean and median of `percent_passive` grouped by `lang_variety` and `genre` brown_fam_df |>   group_by(lang_variety, genre) |>   summarize(     mean_percent_passive = mean(percent_passive),     median_percent_passive = median(percent_passive)   ) ## # A tibble: 30 × 4 ##    lang_variety genre            mean_percent_passive median_percent_passive ##    <fct>        <fct>                           <dbl>                  <dbl> ##  1 AmE          press reportage                 11.5                   11.0  ##  2 AmE          press editorial                 10.6                   10.1  ##  3 AmE          press reviews                    9.54                   9.77 ##  4 AmE          religion                        14.3                   14.3  ##  5 AmE          skills / hobbies                14.9                   13.9  ##  6 AmE          popular lore                    14.0                   12.7  ##  7 AmE          belles lettres                  12.0                   11.7  ##  8 AmE          miscellaneous                   23.5                   23.3  ##  9 AmE          learned                         21.3                   18.3  ## 10 AmE          general fiction                  6.22                   5.89 ## # ℹ 20 more rows # Frequency table for `genre` brown_fam_df |>   group_by(genre) |>   summarize(     n = n(),   ) ## # A tibble: 15 × 2 ##    genre                n ##    <fct>            <int> ##  1 press reportage    220 ##  2 press editorial    135 ##  3 press reviews       85 ##  4 religion            85 ##  5 skills / hobbies   186 ##  6 popular lore       228 ##  7 belles lettres     381 ##  8 miscellaneous      150 ##  9 learned            400 ## 10 general fiction    145 ## # ℹ 5 more rows # Cross-tabulation for `genre` and `lang_variety` brown_fam_df |>   group_by(genre, lang_variety) |>   summarize(     n = n(),   ) ## # A tibble: 30 × 3 ##    genre            lang_variety     n ##    <fct>            <fct>        <int> ##  1 press reportage  AmE             88 ##  2 press reportage  BrE            132 ##  3 press editorial  AmE             54 ##  4 press editorial  BrE             81 ##  5 press reviews    AmE             34 ##  6 press reviews    BrE             51 ##  7 religion         AmE             34 ##  8 religion         BrE             51 ##  9 skills / hobbies AmE             72 ## 10 skills / hobbies BrE            114 ## # ℹ 20 more rows # Mean and median of `n` grouped by `genre` brown_fam_df |>   group_by(genre, lang_variety) |>   summarize(     n = n(),   ) |>   summarize(     mean_n = mean(n),     median_n = median(n)   ) ## # A tibble: 15 × 3 ##    genre            mean_n median_n ##    <fct>             <dbl>    <dbl> ##  1 press reportage   110      110   ##  2 press editorial    67.5     67.5 ##  3 press reviews      42.5     42.5 ##  4 religion           42.5     42.5 ##  5 skills / hobbies   93       93   ##  6 popular lore      114      114   ##  7 belles lettres    190.     190.  ##  8 miscellaneous      75       75   ##  9 learned           200      200   ## 10 general fiction    72.5     72.5 ## # ℹ 5 more rows # Number of observations for each `genre` and `lang_variety` brown_fam_df |>   group_by(genre, lang_variety) |>   summarize(     n = n(),   ) |>   ungroup() |>   summarize(     mean_n = mean(n),     median_n = median(n)   ) ## # A tibble: 1 × 2 ##   mean_n median_n ##    <dbl>    <dbl> ## 1   83.3       72 # Frequency table for `genre` brown_fam_df |>   count(genre) ## # A tibble: 15 × 2 ##    genre                n ##    <fct>            <int> ##  1 press reportage    220 ##  2 press editorial    135 ##  3 press reviews       85 ##  4 religion            85 ##  5 skills / hobbies   186 ##  6 popular lore       228 ##  7 belles lettres     381 ##  8 miscellaneous      150 ##  9 learned            400 ## 10 general fiction    145 ## # ℹ 5 more rows # Cross-tabulation for `genre` and `lang_variety` brown_fam_df |>   count(genre, lang_variety) ## # A tibble: 30 × 3 ##    genre            lang_variety     n ##    <fct>            <fct>        <int> ##  1 press reportage  AmE             88 ##  2 press reportage  BrE            132 ##  3 press editorial  AmE             54 ##  4 press editorial  BrE             81 ##  5 press reviews    AmE             34 ##  6 press reviews    BrE             51 ##  7 religion         AmE             34 ##  8 religion         BrE             51 ##  9 skills / hobbies AmE             72 ## 10 skills / hobbies BrE            114 ## # ℹ 20 more rows # Frequency table for `genre`  # Load packages library(janitor)  brown_fam_df |>   tabyl(genre) ## # A tibble: 15 × 3 ##    genre                n percent ##    <fct>            <int>   <dbl> ##  1 press reportage    220  0.0880 ##  2 press editorial    135  0.0540 ##  3 press reviews       85  0.0340 ##  4 religion            85  0.0340 ##  5 skills / hobbies   186  0.0744 ##  6 popular lore       228  0.0912 ##  7 belles lettres     381  0.152  ##  8 miscellaneous      150  0.0600 ##  9 learned            400  0.160  ## 10 general fiction    145  0.0580 ## # ℹ 5 more rows # Cross-tabulation for `genre` and `lang_variety` brown_fam_df |>   tabyl(genre, lang_variety) ## # A tibble: 15 × 3 ##    genre              AmE   BrE ##    <fct>            <dbl> <dbl> ##  1 press reportage     88   132 ##  2 press editorial     54    81 ##  3 press reviews       34    51 ##  4 religion            34    51 ##  5 skills / hobbies    72   114 ##  6 popular lore        96   132 ##  7 belles lettres     150   231 ##  8 miscellaneous       60    90 ##  9 learned            160   240 ## 10 general fiction     58    87 ## # ℹ 5 more rows # Cross-tabulation for `genre` and `lang_variety` brown_genre_lang_ct <-   brown_fam_df |>   tabyl(genre, lang_variety) |> # genre x lang_variety   adorn_percentages(\"col\") |> # by-column percentages   adorn_pct_formatting() |> # format percentages   adorn_ns(\"front\") # add frequency (in front)  # View brown_genre_lang_ct ## # A tibble: 15 × 3 ##    genre            AmE         BrE         ##    <fct>            <chr>       <chr>       ##  1 press reportage  88  (8.8%)  132  (8.8%) ##  2 press editorial  54  (5.4%)  81  (5.4%)  ##  3 press reviews    34  (3.4%)  51  (3.4%)  ##  4 religion         34  (3.4%)  51  (3.4%)  ##  5 skills / hobbies 72  (7.2%)  114  (7.6%) ##  6 popular lore     96  (9.6%)  132  (8.8%) ##  7 belles lettres   150 (15.0%) 231 (15.4%) ##  8 miscellaneous    60  (6.0%)  90  (6.0%)  ##  9 learned          160 (16.0%) 240 (16.0%) ## 10 general fiction  58  (5.8%)  87  (5.8%)  ## # ℹ 5 more rows # Class of `brown_genre_lang_ct` class(brown_genre_lang_ct) ## [1] \"tabyl\"      \"data.frame\" # Variable types of `brown_genre_lang_ct` glimpse(brown_genre_lang_ct) ## Rows: 15 ## Columns: 3 ## $ genre <fct> press reportage, press editorial, press reviews, religion, skill… ## $ AmE   <chr> \"88  (8.8%)\", \"54  (5.4%)\", \"34  (3.4%)\", \"34  (3.4%)\", \"72  (7.… ## $ BrE   <chr> \"132  (8.8%)\", \"81  (5.4%)\", \"51  (3.4%)\", \"51  (3.4%)\", \"114  (…"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"creating-quarto-tables","dir":"Articles","previous_headings":"Concepts and strategies","what":"Creating Quarto tables","title":"3. Descriptive assessment of datasets","text":"Summarizing data useful understanding data part analysis also communicating data reports, manuscripts, presentations. One way communicate summary statistics tables. Quarto, can use knitr package (Xie 2023) combination code block options produce formatted tables can cross-reference prose sections. work brown_genre_lang_ct object created previous section. create table Quarto, use kable() function. kable() function takes data frame (matrix) argument. format argument derived Quarto document format ('html', 'pdf', etc.). add caption table enable cross-referencing, use code block options label tbl-cap. label option takes label prefixed tbl- create cross-reference table. tbl-cap option takes caption table, quotation marks. Now can cross-reference table @tbl-brown-genre-lang-ct syntax. following Quarto document produce following prose cross-reference formatted table output. see Table 1, distribution genre similar across lang_variety. Table 1: Cross-tabulation genre lang_variety Dive deeper kableExtra package (Zhu 2021) provides additional functionality formatting tables Quarto.","code":"# Load packages library(knitr)  # Create a table in Quarto kable(brown_genre_lang_ct) ```{r} #| label: tbl-brown-genre-lang-ct #| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"  # Create a table in Quarto kable(brown_genre_lang_ct) ``` As we see in @tbl-brown-genre-lang-ct, the distribution of `genre` is similar across `lang_variety`.  ```{r} #| label: tbl-brown-genre-lang-ct #| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"  # Print cross-tabulation kable(brown_genre_lang_ct) ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"creating-quarto-plots","dir":"Articles","previous_headings":"Concepts and strategies","what":"Creating Quarto plots","title":"3. Descriptive assessment of datasets","text":"tables useful communicating summary statistics numeric character variables, plots useful communicating relationships variables especially one variables numeric. Furthermore, complex relationships, plots can effective tables. Quarto, can use ggplot2 package (Wickham et al. 2023) combination code block options produce formatted plots can cross-reference prose sections. see action simple histogram percent_passive variable brown_fam_df dataset. Quarto document produce following prose cross-reference formatted plot output. see Figure 1, distribution percent_passive skewed right. Figure 1: Histogram percent_passive ggplot2 package implements 'Grammar Graphics' approach creating plots. approach based idea plots can broken components, layers, layer can manipulated independently. main components data, aesthetics, geometries. Data data frame contains variables plotted. Aesthetics variables mapped x-axis, y-axis (well color, shape, size, etc.). Geometries visual elements used represent data, points, lines, bars, etc.. discussed R lesson \"Visual Summaries\", aes() function used map variables aesthetics can added ggplot() function geom_*() function depending whether aesthetic mapped geometries specific geometry, respectively. Take look following stages earlier plot tabs .","code":"As we see in @fig-brown-fam-percent-passive-hist, the distribution of `percent_passive` is skewed to the right.  ```{r} #| label: fig-brown-fam-percent-passive-hist #| fig-cap: \"Histogram of `percent_passive`\"  # Create a histogram in Quarto ggplot(brown_fam_df) +   geom_histogram(aes(x = percent_passive)) ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"stages","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Stages","title":"3. Descriptive assessment of datasets","text":"Data Aesthetics Geometries data layer produce plot foundation plot.  aesthetics layer produce plot maps variables aesthetics used plot.  geometries layer produces plot connecting data aesthetics layers particular way specified geometries, case histogram.","code":"# Data layer ggplot(brown_fam_df) # Aesthetics layer ggplot(brown_fam_df, aes(x = percent_passive)) # Geometries layer ggplot(brown_fam_df, aes(x = percent_passive)) +   geom_histogram()"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"choosing-the-right-plot","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Choosing the right plot","title":"3. Descriptive assessment of datasets","text":"Just tables, type summary choose communicate plot depends type variables working relationships variables. included examples plots can used communicate different types variables relationships.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"single-numeric-variable","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Single numeric variable","title":"3. Descriptive assessment of datasets","text":"Histogram Density plot","code":"# Histogram ggplot(brown_fam_df) +   geom_histogram(aes(x = percent_passive)) # Density plot ggplot(brown_fam_df) +   geom_density(aes(x = percent_passive))"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"numeric-and-categorical-variables","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Numeric and categorical variables","title":"3. Descriptive assessment of datasets","text":"Density plot Boxplot Violin plot","code":"# Density plot ggplot(brown_fam_df) +   geom_density(     aes(       x = percent_passive,       fill = lang_variety     ),     alpha = 0.5 # adds transparency   ) # Boxplot ggplot(brown_fam_df) +   geom_boxplot(     aes(       x = lang_variety,       y = percent_passive     )   ) # Violin plot ggplot(brown_fam_df) +   geom_violin(     aes(       x = lang_variety,       y = percent_passive     )   )"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"two-numeric-variables","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Two numeric variables","title":"3. Descriptive assessment of datasets","text":"Scatterplot Scatterplot regression line","code":"# Scatterplot ggplot(brown_fam_df) +   geom_point(     aes(       x = active_verbs,       y = passive_verbs     )   ) # Scatterplot with regression line ggplot(   brown_fam_df,   aes(     x = active_verbs,     y = passive_verbs   ) ) +   geom_point() +   geom_smooth(method = \"lm\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"other-variable-combinations","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Other variable combinations","title":"3. Descriptive assessment of datasets","text":"examples, looked common variable combinations one two variable plots. sophisticated plots can used variable combinations using ggplot2. now, leave another time.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"3. Descriptive assessment of datasets","text":"factor character vector augmented include information discrete values, levels, vector. TRUEFALSE difference frequency table contingency table? frequency table cross-tabulation two categorical variables.contingency table cross-tabulation two categorical variables. skimrdplyrggplot2knitr package used create formatted tables R. add geometry layer, geom_histogram(), ggplot object |> operator used. TRUEFALSE visualize relationship two numeric variables, histogramdensity plotboxplotviolin plotscatterplot often used. aes() function added ggplot() function, aesthetic mapped geometries. TRUEFALSE","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"3. Descriptive assessment of datasets","text":"beginning Lab 3, learners comfortable skills knowledge developed previous recipes labs. lab, chance use skills introduced Recipe provide descriptive assessment dataset includes statistics, tables, plots using Quarto R. additional skills knowledge need complete Lab 3 include: Summarizing data skimr Summarizing data dplyr Creating Quarto tables knitr Creating Quarto plots ggplot2","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"4. Scaffolding a research project","text":"recipe, learn scaffold research project use tools resources available us manage research projects. build understanding computing environment structure reproducible projects introduce new features Git GitHub. cover following topics: Understanding components reproducible project Connecting computing environment reproducible project management Understanding workflow project structure Using Git GitHub manage project Lab 4, apply learned recipe scaffold research project forking, cloning, editing, commiting, pushing repository GitHub.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"understanding-components-of-a-reproducible-project","dir":"Articles","previous_headings":"Concepts and strategies","what":"Understanding components of a reproducible project","title":"4. Scaffolding a research project","text":"Reproducible projects composed two main components: computing environment project structure. computing environment hardware, operating system, software use work project structure organization files folders make project. can see Figure 1, components subcomponents nested within . Figure 1: Components reproducible project lesson computing environment, learned importance understanding computing environment find information computing environment inspecting R session, see . learned, computing environment used create project different computing environment used reproduce project, risk project reproducible. tackle components Figure 1 , however, instead focus project structure. Later , experience, address components.","code":"─ Session info ──────────────────────────────────────────────────────  setting  value  version  R version 4.3.2 (2023-10-31)  os       macOS Ventura 13.6.1  system   x86_64, darwin22.6.0  ui       unknown  language (EN)  collate  en_US.UTF-8  ctype    en_US.UTF-8  tz       America/New_York  date     2023-11-26  pandoc   3.1.9 @ /usr/local/bin/pandoc  ─ Packages ──────────────────────────────────────────────────────  package     * version date (UTC) lib source  cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.2)  jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.2)  rlang         1.1.2   2023-11-04 [1] CRAN (R 4.3.2)   [1] /Users/francojc/R/Library  [2] /usr/local/Cellar/r/4.3.2/lib/R/library"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"approaching-reproducible-project-management","dir":"Articles","previous_headings":"Concepts and strategies","what":"Approaching reproducible project management","title":"4. Scaffolding a research project","text":"project structure organization files folders make project. minimal project structure includes separation input, process, output research, documentation project, reproduce project, project files . project structure important helps us organize work helps others understand . concensus best project structure . However, principles covered Chapter 4 can guide us developing project structure organizes work makes work understandable others. principles met, can add additional structure project meet project-specific needs. create reproducible project structure, need create directory set files meet principles minimal reproducible framework. project, back directories files / share others number ways. Although approach already good step right direction, error prone likely lead inconsistencies across projects. better approach develop, adopt, project structure template can used projects, use version control track changes project, upload project remote repository backed (including version history) can shared others efficiently. later approach one use subsequent lessons, recipes, labs course.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"leveraging-git-and-github-for-reproducible-project-management","dir":"Articles","previous_headings":"Concepts and strategies","what":"Leveraging Git and GitHub for reproducible project management","title":"4. Scaffolding a research project","text":"point, somewhat familiar Git Github. likely used Git copy download repository GitHub, say example labs course. However, going behind scenes likely still bit mystery. section, demystify Git GitHub, bit, learn use together manage project different scenarios. Tip Verify working version Git computing environment make sure GitHub account. can refer Guide 2 information . rewind bit review Git Github work together. Git version control system allows us track changes project files. command line tool, much like R, installed software. Also like R, can interact Git graphical user interface (GUI), RStudio. Directory file tracking Git can added project time. tracked project called repository, repo short. used computing environment, repo called local repository. many benefits using Git track changes local repository, including ability revert previous versions files, create edit parallel copies files selectively integrate , much . real power Git realized combination GitHub. Github cloud-based remote repository allows us store git-tracked projects. web-based platform requires account use. signed , can connect Git Github create remote repositories upload local repositories . can also download remote repositories computing environment. many, many features Github offers, now focus key features help us manage projects. Figure 2, provide schematic look relationship Github Git three common scenarios. Figure 2: Key features GitHub Scenario : Clone remote repository scenario, locate remote repository Github someone made publically available. clone (copy download) repository computing environment. repository cloned locally, can edit files see fit. essence, just downloading group files folders computing environment Github. scenario using course download lab repositories. Steps: Locate remote repository Github someone made publically available copy clone URL. Open RStudio create new project version control. Paste clone URL repository URL field. Choose project parent directory (project saved). (optional) Rename project directory. Scenario B: Fork clone remote repository scenario differs two respects. First, first fork (copy ) remote repository Github account cloning computing environment. Second, commit (log edits) changes git tracking system push (sync ) changes remote repository. case, just downloading group files folders. setting link person's remote repository remote repository. use link, want , can pull (sync ) changes made person's remote repository remote repository. can useful want keep remote repository date person's remote repository. Furthermore, link allows us propose changes person's remote repository pull request --request person pull changes remote repository. can useful want collaborate person project. Pull pull request advanced features address point. second difference using Git track changes local repository push changes remote repository. key feature allows us keep track changes project files folders revert previous versions needed. also allows us share project others collaborate . Steps: Locate remote repository Github someone made publically available click fork button. Still Github, choose new account owner forked repository. forked repository, copy clone URL. Open RStudio create new project version control. Paste clone URL repository URL field. Choose project parent directory (project saved). (optional) Rename project directory. Make changes project files folders. Commit changes git tracking system. Push changes remote repository. Scenario C: Create/ Join clone remote repository scenario similar B, instead forking remote repository, create new remote repository Github clone computing environment. commit push changes remote repository. case, creating new remote repository using Git track changes local repository push changes remote repository. scenario common work projects want collaborate others project. latter case, create remote repository invite others collaborate . Everyone permissions remote repository can clone computing environment, make changes, push changes remote repository. allows everyone work project keep track changes project files folders. working multiple people project, can imagine working project locally working project locally, might make changes files folders. push changes remote repository, risk changes conflict . Git Github features help us manage conflicts (pull, fetch, merge, etc.), address point either. Steps: Create new remote repository Github accept invitation collaborate remote repository. Copy clone URL. Open RStudio create new project version control. Paste clone URL repository URL field. Choose project parent directory (project saved). (optional) Rename project directory. Make changes project files folders. Commit changes git tracking system. Push changes remote repository.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"4. Scaffolding a research project","text":"recipe, reviewed components reproducible research projects: computing environment project structure. computing environment hardware, operating system, software use work project structure organization files folders make project. Furthermore, learned Git Github can used together manage project different scenarios.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"4. Scaffolding a research project","text":"Select component computing environment following related : Windows 10 hardwareoperating systemsoftware R version 4.3.1 hardwareoperating systemsoftware dplyr_1.1.4 hardwareoperating systemsoftware Select Git name following actions: Copy download remote repository computing environment cloneforkcommitpush Log edits git tracking system cloneforkcommitpush Sync changes remote repository cloneforkcommitpush","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"4. Scaffolding a research project","text":"lab 4, apply learned recipe scaffold research project forking cloning research project template repository Github. edit project files folders, commit changes git tracking system, push changes remote repository Github. beginning lab, make sure comfortable following: Cloning remote repository computing environment Creating editing files folders, particular Quarto documents. additional knowledge skills need complete lab covered recipe include: Understanding components reproducible project Understanding importance project structure reproducible project management Forking, cloning, editing, commiting, pushing repository","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"5. Collecting and documenting data","text":"point, now strong undertanding foundations programming R data science workflow. Previous lessons, recipes, labs focused developing skills chapters aimed provide conceptual framework understanding steps data science workflow. now turn applying conceptual knowledge technical skills accomplish tasks data science workflow. recipe, focus acquiring data text analysis project. cover following topics: Finding data sources Data collection strategies Data documentation Along way put practice foundational R skills also continue work newly introduced skills control statements custom functions. Lab 4, apply learned far acquire data text analysis project.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"finding-data-sources","dir":"Articles","previous_headings":"Concepts and strategies","what":"Finding data sources","title":"5. Collecting and documenting data","text":"find data sources, best research question mind. help narrow search data sources. However, finding data sources can also good way generate research questions. either case, takes sleuthing find data sources work research question. addition, data source , also need consider permissions licensing data source. best consider early process avoid surprises later. Finally, also need consider data format used analysis. can case data source seems ideal, data format conducive analysis like . Tip Consult Identifying data data sources guide ideas find data sources. recipe, consider hypothetical reseach aimed exploring potential similarities differences lexical, syntactic, / stylistic features American English literature mid 19th century. Dive deeper interested understanding literary analysis perspective text analysis, highly recommend Matthew Jockers' book Text Analysis R Students Literature (Jockers 2014). book great resource understanding apply text analysis literary analysis. Project Gutenberg great source data research question. Project Gutenberg volunteer effort digitize archive cultural works. great majority works Project Gutenberg database public domain United States. means works can freely used shared. Furthermore, gutenbergr package provides API accessing Project Gutenberg database. means can use R access Project Gutenberg database download text metadata works interested . gutenbergr package also provides number data frames can help us identify works interested .","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"data-collection-strategy","dir":"Articles","previous_headings":"Concepts and strategies","what":"Data collection strategy","title":"5. Collecting and documenting data","text":"now turn data collection strategy. number data collection strategies can used acquire data text analysis project. chapter, covered manual programmatic downloads APIs. use R package provide API accessing data source. Dive deeper interested learning another data collection strategy, web scraping, suggest look Web scraping R guide. load dplyr, readr gutenbergr packages prepare data collection process. main workhorse gutenbergr package gutenberg_download(). required argument id(s) used Project Gutenberg index works database. function download text work(s) return data frame gutenberg id text work(s). find gutenberg ids? manual method go Project Gutenberg website search work interested . example, say interested work \"Tale Two Cities\" Charles Dickens. can search work Project Gutenberg website click link work. url work : https://www.gutenberg.org/ebooks/98. gutenberg id number end url, case 98. work individual works, just download text Project Gutenberg website? works Project Gutenberg perfectly fine. can share text others license works Project Gutenberg public domain. However, interested downloading multiple works? number works increases, time takes manually download work increases. Furthermore, gutenbergr package provides number additional attributes can downloaded organized along side text. Finally, results gutenberg_download() function returned data frame can easily manipulated analyzed R. data acquisition plan, want collect works number authors. best leverge gutenbergr package download works interested . need know gutenberg ids works interested . Convienently, gutenbergr package also includes number data frames contain meta data works Project Gutenberg database. data frames include meta data works Project Gutenberg database (gutenberg_metadata), authors (gutenberg_authors), subjects (gutenberg_subjects). take look structure data frames. overvew, can see 72,569 works Project Gutenberg database. can also see 23,980 authors 231,741 subjects. dicussed, work Project Gutenberg database gutenberg id. gutenberg_id appears gutenberg_metadata also gutenberg_subjects data frame. common attribute means work particular gutenberg id can linked subject(s) associated work. Another important attribute gutenberg_author_id links work author(s) work. Yes, author name gutenberg_metadata data frame, gutenberg_author_id can used link work gutenberg_authors data frame contains additional information authors. Tip gutenbergr package periodically updated. check see data frame last updated run: now describe attributes useful data acquisition plan. gutenberg_subjects data frame, subject_type subject. subject_type type subject classification system used classify work. tabulate column, see two types subject classification systems used: Library Congress Classification (lcc) Library Congress Subject Headings (lcsh). subject column contains subject code work. lsch subject code descriptive character string lcc subject code id character string combination letters (numbers) Library Congress uses classify works. data acquistion plan, use lcc subject classification system select works Library Congress Classification English Literature (PR) American Literature (PS). gutenberg_authors data frame, birthdate deathdate attributes. attributes useful filtering authors lived mid 19th century. overview gutenbergr package data frames contains, can now begin develop data acquisition plan. Select authors lived mid 19th century gutenberg_authors data frame. Select works Library Congress Classification English Literature (PR) American Literature (PS) gutenberg_subjects data frame. Select works gutenberg_metadata associated authors subjects selected steps 1 2. Download text metadata works selected step 3 using gutenberg_download() function. Write data disk appropriate format.","code":"library(dplyr)      # data manipulation library(readr)      # data import/ export library(gutenbergr) # Project Gutenberg API glimpse(gutenberg_metadata) ## Rows: 72,569 ## Columns: 8 ## $ gutenberg_id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,… ## $ title               <chr> \"The Declaration of Independence of the United Sta… ## $ author              <chr> \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo… ## $ gutenberg_author_id <int> 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 8, … ## $ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e… ## $ gutenberg_bookshelf <chr> \"Politics/American Revolutionary War/United States… ## $ rights              <chr> \"Public domain in the USA.\", \"Public domain in the… ## $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR… glimpse(gutenberg_authors) ## Rows: 23,980 ## Columns: 7 ## $ gutenberg_author_id <int> 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… ## $ author              <chr> \"United States\", \"Lincoln, Abraham\", \"Henry, Patri… ## $ alias               <chr> \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",… ## $ birthdate           <int> NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18… ## $ deathdate           <int> NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18… ## $ wikipedia           <chr> \"https://en.wikipedia.org/wiki/United_States\", \"ht… ## $ aliases             <chr> \"U.S.A.\", \"United States President (1861-1865)/Lin… glimpse(gutenberg_subjects) ## Rows: 231,741 ## Columns: 3 ## $ gutenberg_id <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, … ## $ subject_type <chr> \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc… ## $ subject      <chr> \"United States -- History -- Revolution, 1775-1783 -- Sou… attr(gutenberg_metadata, \"date_updated\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"data-collection","dir":"Articles","previous_headings":"Concepts and strategies","what":"Data collection","title":"5. Collecting and documenting data","text":"take steps turn. First, need select authors lived mid 19th century gutenberg_authors data frame. use filter() function. pass gutenberg_authors data frame filter() function use birthdate column select authors born 1800 died 1880 --year chosen mid 19th century generally considered period 1830 1870. assign result variable name authors. ! now data frame authors lived mid 19th century, 787 authors total. span subjects languages, final number authors working . next step select works Library Congress Classification English Literature (PR) American Literature (PS) gutenberg_subjects data frame. use filter() function . pass gutenberg_subjects data frame filter() function use subject_type subject columns select works associated Library Congress Classification English Literature (PR) American Literature (PS). assign result variable name subjects. Now data frame subjects interested . inspect data frame see many works subject. next step subset gutenberg_metadata data frame select works authors subjects selected previous steps. , use filter() . pass gutenberg_metadata data frame filter() function use gutenberg_author_id gutenberg_id columns select works associated authors subjects selected previous steps. assign result variable name works. Filtering gutenberg_metadata data frame authors subjects selected previous steps, now data frame 1,014 works. final number works working can now download text metadata works using gutenberg_download() function. things note gutenberg_download() function. First, vectorized, , can take single value multiple values argument gutenberg_id. good passing vector gutenberg ids function. small fraction works Project Gutenberg public domain therefore downloaded, documented rights column. Furthermore, works text available, seen has_text column. Finally, gutenberg_download() function returns data frame gutenberg id text work(s) --can also select additional attributes returned passing character vector attribute names argument meta_fields. column names gutenberg_metadata data frame contains available attributes. mind, quick test download works. select first 5 works works data frame fit criteria download text metadata works using gutenberg_download() function. assign result variable name works_sample. inspect works_sample data frame. First, output can see meta data attributes returned. Second, can see text column contains values line text (delimited carriage return) 5 works downloaded, even blank lines. make sure correct number works, can use count() function count number works gutenberg_id. Yes, 5 works can see many lines works. now run code entire works data frame write data disk like : accomplish primary goal data acquisition plan. However, key functionality missing like make code reproducible-friendly. First, checking see data already exists disk. already run code script, likely want run . Second, may want use code different parameters, example, may want retrieve different subject codes, different time periods, languages. three additional features can accomplished writing custom function. take look code written far see can turn custom function.","code":"authors <-   gutenberg_authors |>   filter(     birthdate > 1800,     deathdate < 1880   ) subjects <-   gutenberg_subjects |>   filter(     subject_type == \"lcc\",     subject %in% c(\"PR\", \"PS\")   ) subjects |>   count(subject) ## # A tibble: 2 × 2 ##   subject     n ##   <chr>   <int> ## 1 PR       9926 ## 2 PS      10953 works <-   gutenberg_metadata |>   filter(     gutenberg_author_id %in% authors$gutenberg_author_id,     gutenberg_id %in% subjects$gutenberg_id   )  works ## # A tibble: 1,014 × 8 ##    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf ##           <int> <chr>    <chr>                <int> <chr>    <chr>               ##  1           33 The Sca… Hawth…                  28 en       \"Harvard Classics/… ##  2           46 A Chris… Dicke…                  37 en       \"Children's Litera… ##  3           71 On the … Thore…                  54 en       \"\"                  ##  4           77 The Hou… Hawth…                  28 en       \"Best Books Ever L… ##  5           98 A Tale … Dicke…                  37 en       \"Historical Fictio… ##  6          205 Walden,… Thore…                  54 en       \"\"                  ##  7          258 Poems b… Gordo…                 145 en       \"\"                  ##  8          271 Black B… Sewel…                 154 en       \"Best Books Ever L… ##  9          292 Beauty … Taylo…                 167 en       \"\"                  ## 10          394 Cranford Gaske…                 220 en       \"\"                  ## # ℹ 1,004 more rows ## # ℹ 2 more variables: rights <chr>, has_text <lgl> works_sample <-   works |>   filter(     rights == \"Public domain in the USA.\",     has_text == TRUE   ) |>   slice_head(n = 5) |>   gutenberg_download(     meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")   )  works_sample ## # A tibble: 38,227 × 6 ##    gutenberg_id text        title author gutenberg_author_id gutenberg_bookshelf ##           <int> <chr>       <chr> <chr>                <int> <chr>               ##  1           33 \"The Scarl… The … Hawth…                  28 Harvard Classics/M… ##  2           33 \"\"          The … Hawth…                  28 Harvard Classics/M… ##  3           33 \"by Nathan… The … Hawth…                  28 Harvard Classics/M… ##  4           33 \"\"          The … Hawth…                  28 Harvard Classics/M… ##  5           33 \"\"          The … Hawth…                  28 Harvard Classics/M… ##  6           33 \"Contents\"  The … Hawth…                  28 Harvard Classics/M… ##  7           33 \"\"          The … Hawth…                  28 Harvard Classics/M… ##  8           33 \" THE CUST… The … Hawth…                  28 Harvard Classics/M… ##  9           33 \" THE SCAR… The … Hawth…                  28 Harvard Classics/M… ## 10           33 \" I. THE P… The … Hawth…                  28 Harvard Classics/M… ## # ℹ 38,217 more rows works_sample |>   count(gutenberg_id) ## # A tibble: 5 × 2 ##   gutenberg_id     n ##          <int> <int> ## 1           33  8212 ## 2           46  3842 ## 3          258 11050 ## 4          271  5997 ## 5          292  9126 works |>   filter(     rights == \"Public domain in the USA.\",     has_text == TRUE   ) |>   gutenberg_download(     meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")   ) |>   write_csv(file = \"data/original/gutenberg/works.csv\") # Get authors within years authors <-   gutenberg_authors |>   filter(     birthdate > 1800,     deathdate < 1880   ) # Get LCC subjects subjects <-   gutenberg_subjects |>   filter(     subject_type == \"lcc\",     subject %in% c(\"PR\", \"PS\")   ) # Get works based on authors and subjects works <-   gutenberg_metadata |>   filter(     gutenberg_author_id %in% authors$gutenberg_author_id,     gutenberg_id %in% subjects$gutenberg_id   ) # Download works works |>   filter(     rights == \"Public domain in the USA.\",     has_text == TRUE   ) |>   gutenberg_download(     meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")   ) |>   write_csv(file = \"data/original/gutenberg/works.csv\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"build-the-custom-function","dir":"Articles","previous_headings":"Concepts and strategies > Data collection","what":"Build the custom function","title":"5. Collecting and documenting data","text":"Function name Function arguments Function code: comments Function code: packages Function code: data check Function code: authors Function code: subject Function code: works Function code: download Function code: write start create function creating name calling function() function. name function get_gutenberg_works(). Now need think arguments like pass function can used customize data acquisition process. First, want check see data already exists disk. need pass path data file function. name argument target_file. Next, want pass subject code works associated . name argument lcc_subject. Finally, want pass birth year death year authors associated . name arguments birth_year death_year. now turn code. like start creating comments describe steps inside function adding code. packages want make sure installed loaded. use pacman package . use p_load() function install load packages. pass character vector package names p_load() function. need create code check data exists. use statement . data exist, print message console data already exists stop function. data exist, create directory structure continue data acquisition process. use fs package code load library top function. now add code get authors within years. now use birth_year death_year arguments filter gutenberg_authors data frame. Using lcc_subject argument, now filter gutenberg_subjects data frame. use authors subjects data frames filter gutenberg_metadata data frame . now use works data frame download text metadata works using gutenberg_download() function assign results. Finally, write results data frame disk using write_csv() function target_file argument.","code":"get_gutenberg_works <- function() {  } get_gutenberg_works <- function(target_file) {  } get_gutenberg_works <- function(target_file, lcc_subject) {  } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {  } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages    # Check to see if the data already exists    # Get authors within years    # Get LCC subjects    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   pacman::p_load(dplyr, gutenbergr, readr)    # Check to see if the data already exists    # Get authors within years    # Get LCC subjects    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   pacman::p_load(dplyr, gutenbergr, readr, fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years    # Get LCC subjects    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   pacman::p_load(dplyr, gutenbergr, readr, fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   pacman::p_load(dplyr, gutenbergr, readr, fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects   subjects <-     gutenberg_subjects |>     filter(       subject_type == \"lcc\",       subject %in% lcc_subject     )    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   pacman::p_load(dplyr, gutenbergr, readr, fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects   subjects <-     gutenberg_subjects |>     filter(       subject_type == \"lcc\",       subject %in% lcc_subject     )    # Get works based on authors and subjects   works <-     gutenberg_metadata |>     filter(       gutenberg_author_id %in% authors$gutenberg_author_id,       gutenberg_id %in% subjects$gutenberg_id     )    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   pacman::p_load(dplyr, gutenbergr, readr, fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects   subjects <-     gutenberg_subjects |>     filter(       subject_type == \"lcc\",       subject %in% lcc_subject     )    # Get works based on authors and subjects   works <-     gutenberg_metadata |>     filter(       gutenberg_author_id %in% authors$gutenberg_author_id,       gutenberg_id %in% subjects$gutenberg_id     )    # Download works   results <-     works |>     filter(       rights == \"Public domain in the USA.\",       has_text == TRUE     ) |>     gutenberg_download(       meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")     )    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   pacman::p_load(dplyr, gutenbergr, readr, fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects   subjects <-     gutenberg_subjects |>     filter(       subject_type == \"lcc\",       subject %in% lcc_subject     )    # Get works based on authors and subjects   works <-     gutenberg_metadata |>     filter(       gutenberg_author_id %in% authors$gutenberg_author_id,       gutenberg_id %in% subjects$gutenberg_id     )    # Download works   results <-     works |>     filter(       rights == \"Public domain in the USA.\",       has_text == TRUE     ) |>     gutenberg_download(       meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")     )    # Write works to disk   write_csv(results, file = target_file) }"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"using-the-custom-function","dir":"Articles","previous_headings":"Concepts and strategies > Data collection","what":"Using the custom function","title":"5. Collecting and documenting data","text":"now function, get_gutenberg_works(), can use acquire works Project Gutenberg given LCC code authors lived given time period. now flexible function can use acquire data. can add function script use , can add separate script source script want use . Another option add function package. great option plan use function multiple projects share others. Since already created package book, qtalrkit, added function, additional functionality, package. modified function create directory structure data file already exist. also create file name data file based arguments passed function.","code":"# Source function source(\"get_gutenberg_works.R\")  # Get works for PR and PS for authors born between 1800 and 1880 get_gutenberg_works(   target_file = \"data/original/gutenberg/works.csv\",   lcc_subject = c(\"PR\", \"PS\"),   birth_year = 1800,   death_year = 1880 ) # Load package library(qtalrkit)  # Get works for fiction for authors born between 1870 and 1920 get_gutenberg_works(   target_dir = \"data/original/gutenberg/\",   lcc_subject = \"PZ\",   birth_year = 1870,   death_year = 1920 )"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"data-documentation","dir":"Articles","previous_headings":"Concepts and strategies","what":"Data documentation","title":"5. Collecting and documenting data","text":"Finding data sources collecting data important steps acquisition process. However, also important document data collection process. important , others, can reproduce data collection process. data acquisition, documentation includes code, code comments, prose process file used acquire data also data origin file. data origin file text file describes data source data collection process. qtalrkit package includes function, create_data_origin(), can used scaffold data origin file. simply takes file path creates data origin file CSV format. edit file ensure contains information needed document data. Make sure file near data file easy find.","code":"attribute,description Resource name,The name of the resource. Data source,\"URL, DOI, etc.\" Data sampling frame,\"Language, language variety, modality, genre, etc.\" Data collection date(s),The dates the data was collected. Data format,\".txt, .csv, .xml, .html, etc.\" Data schema,\"Relationships between data elements: files, folders, etc.\" License,\"CC BY, CC BY-SA, etc.\" Attribution,Citation information. data   ├── analysis/   ├── derived/   └── original/       ├── works_do.csv       └── gutenberg/           ├── works_pr.csv           └── works_ps.csv"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"5. Collecting and documenting data","text":"recipe, covered acquiring data text analysis project. used gutenbergr package acquire works Project Gutenberg. exploring resources available, established acquisition plan. used R implement plan. make code reproducible-friendly, wrote custom function acquire data. Finally, discussed importance documenting data collection process introduced data origin file.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"5. Collecting and documenting data","text":"chapter recipe, strategies acquiring data discussed. following discussed strategy acquiring data? Direct downloadProgrammatic downloadAPIsWeb scraping recipe, used gutenbergr package acquire works Project Gutenberg. name function used acquire actual text? gutenberg_metatagutenberg_get()gutenberg_search()gutenberg_download() TrueFalse custom function really necessary writting R package. writing custom function, first step? Write codeWrite commentsLoad packagesCreate function arguments mean say function 'vectorized' R? function returns vectorThe function can take vector argumentThe function can take vector operates element vector Tidyverse package allows us apply non-vectorized functions vectors? dplyrstringrreadrpurrr","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"6. Organizing and documenting data","text":"Skills: Literal characters: , b, c Special sequences: \\d, \\w, \\s Metacharacters: ., ^, $, *, +, ?, \\ Sets: [abc], [^abc], [-z], [-zA-Z] dplyr verbs: select(), filter(), arrange(), mutate(), summarise() tidyselect: starts_with(), ends_with(), contains(), matches(), one_of(), everything() ... Packages: stringr dplyr tidyselect purrr","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"reading-data","dir":"Articles","previous_headings":"Cases: > Semi-structured: SWDA (.utt) files","what":"Reading data","title":"6. Organizing and documenting data","text":"cover basics reading semi-structured data","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"orientation","dir":"Articles","previous_headings":"Cases: > Semi-structured: SWDA (.utt) files","what":"Orientation","title":"6. Organizing and documenting data","text":"SWDA data mentioned, reference needed. example work Switchboard Dialog Act Corpus (SDAC) extends Switchboard Corpus speech act annotation. (ADD CITATION) main directory structure SDAC data looks like : README file contains basic information resource, doc/ directory contains detailed information dialog annotations, following directories prefixed sw... contain individual conversation files. peek internal structure first couple directories. take look first conversation file (sw_0001_4325.utt) see structured. things take note . First see conversation files meta-data header offset conversation text line = characters. Second header contains meta-information various types. Third, text interleaved annotation scheme. information may readily understandable, various pieces meta-data header, get better understanding information encoded take look README file. file get birds eye view going . short, data includes 1155 telephone conversations two people annotated 42 'DAMSL' dialog act labels. README file refers us doc/manual.august1.html file information scheme. point open doc/manual.august1.html file browser investigation. find 'DAMSL' stands 'Discourse Annotation Markup System Labeling' first characters line conversation text correspond one combination labels utterance. first utterances : utterance also labeled speaker ('' 'B'), speaker turn ('1', '2', '3', etc.), utterance within turn ('utt1', 'utt2', etc.). annotation provided withing utterance, enough get us started conversations. Now turn meta-data header. see information creation file: 'FILENAME', 'TOPIC', 'DATE', etc. doc/manual.august1.html file much say information returned LDC Documentation found information Online Documentation section. poking around documentation discovered meta-data speaker corpus found caller_tab.csv file. tabular file contain column names, caller_doc.txt . inspecting files manually comparing information conversation file noticed 'FILENAME' information contained three pieces useful information delimited underscores _. first information document id (4325), second third correspond speaker number: first speaker (1632) second speaker B (1519). sum, 1155 conversation files. file two parts, header text section, separated line = characters. header section contains 'FILENAME' line document id, ids speaker speaker B. text section annotated DAMSL tags beginning line, followed speaker, turn number, utterance number, utterance text. knowledge hand, set create tidy dataset following column structure:","code":"data/ ├── derived/ └── original/     └── sdac/         ├── README         ├── doc/         ├── sw00utt/         ├── sw01utt/         ├── sw02utt/         ├── sw03utt/         ├── sw04utt/         ├── sw05utt/         ├── sw06utt/         ├── sw07utt/         ├── sw08utt/         ├── sw09utt/         ├── sw10utt/         ├── sw11utt/         ├── sw12utt/         └── sw13utt/ ├── README ├── doc │   └── manual.august1.html ├── sw00utt │   ├── sw_0001_4325.utt │   ├── sw_0002_4330.utt │   ├── sw_0003_4103.utt │   ├── sw_0004_4327.utt │   ├── sw_0005_4646.utt o = \"Other\" qw = \"Wh-Question\" qy^d = \"Declarative Yes-No-Question\" + = \"Segment (multi-utterance)\" *x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*   FILENAME:   4325_1632_1519 TOPIC#:     323 DATE:       920323 TRANSCRIBER:    glp"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"tidy-the-data","dir":"Articles","previous_headings":"Cases: > Semi-structured: SWDA (.utt) files","what":"Tidy the data","title":"6. Organizing and documenting data","text":"Add idealized structure SWDA dataset begin reading one conversation files R character vector using read_lines() function readr package. isolate vector element contains document speaker ids, use str_detect() stringr package. function takes two arguments, string pattern, returns logical value, TRUE pattern matched FALSE . can use output function, , subset doc character vector return vector element (line) contains digits_digits_digits regular expression. expression combines digit matching operator \\\\d + operator match 1 contiguous digits. separate three groups \\\\d+ underscores _. result \\\\d+_\\\\d+_\\\\d+. {{< fa regular hand-point->}} Tip Regular Expressions powerful pattern matching syntax. used extensively text manipulation see . develop regular expressions, helpful tool allows interactively test pattern matching. stringr package handy function str_view() str_view_all() allow interactive pattern matching. good website practice Regular Expressions RegEx101. can also install regexplain package R get access useful RStudio Addin. next step extract three digit sequences correspond doc_id, speaker_a_id, speaker_b_id. First extract pattern identified str_extract() can break single character vector multiple parts based underscore _. str_split() function takes string pattern use split character vector. return list character vectors. [] introduced lists earlier material (swirl Objects), least. list special object type R. unordered collection objects whose lengths can differ (contrast data frame collection objects whose lengths --hence tabular format). case list length 1, whose sole element character vector length 3 --one element per segment returned split. desired result cases pass multiple character vectors str_split() function want results conflated single character vector blurring distinction individual character vectors. like conflate, flatten list, can use unlist() function. flatten list case, single character vector, assign result doc_speaker_info. doc_speaker_info now character vector length three. subset elements assign meaningful variable names can conveniently use later tidying process. next step isolate text section extracting rest document. noted previously, sequence = separates header section text section. need index point character vector doc line occurs subset doc point end character vector. first find point = sequence occurs. use str_detect() function find pattern looking (contiguous sequence =), pass logical result () function return element index number match. file 31 index doc = sequence occurs. Now important keep mind working single file sdac/ data. need cautious create pattern may matched multiple times another document corpus. =+ pattern match =, ==, ===, etc. implausible believe might = character line one files. update regular expression avoid potential scenario matching sequences three =. case make use curly bracket operators {}. get result file, safeguard bit unlikely find multiple matches ===, ====, etc. 31 index = sequence, want next line start reading text section. increment index 1. index end text simply length doc vector. can use length() function get index. now bookends, speak, text section. extract text subset doc vector indices. text extra whitespace lines blank lines well. cleaning moving forward organize data. get rid whitespace use str_trim() function default remove leading trailing whitespace line. remove blank lines use logical expression subset text vector. text != \"\" means return TRUE lines blank, FALSE . first step towards tidy dataset now combine doc_id element text data frame. data now data frame, time parse text column extract damsl tags, speaker, speaker turn, utterance number, utterance text separate columns. make extensive use regular expressions. aim find consistent pattern distinguishes piece information text given row data$text extract . best way learn regular expressions use . end included link interactive regular expression practice website regex101. Open site copy text 'TEST STRING' field. Now manually type following regular expressions 'REGULAR EXPRESSION' field one--one (separate line). Notice matched type finished typing. can find exactly component parts expression toggling top right icon window hovering mouse relevant parts expression. can now see, regular expressions match damsl tags, speaker speaker turn, utterance number, utterance text. apply expressions data extract information separate columns make use mutate() str_extract() functions. mutate() take data frame create new columns values match extract row data frame str_extract(). Notice str_extract() different str_extract_all(). work mutate() row evaluated turn, therefore need make one match per row data$text. chained steps code , dropping original text column select(-text), overwriting data results. Tip One twist notice regular expressions R require double backslashes (\\\\\\\\) programming environments use single backslash (\\\\). couple things left columns extracted text move finishing tidy dataset. First, need separate speaker_turn column speaker turn_num columns second need remove unwanted characters damsl_tag, utterance_num, utterance_text columns. separate values column two columns use separate() function. takes column separate character vector names new columns create. default values input column separated non-alphanumeric characters. case means . separator. remove unwanted leading trailing whitespace apply str_trim() function. removing characters matching character(s) replace empty string (\"\") str_replace() function. , chained functions together overwritten data results. round tidy dataset single conversation file connect speaker_a_id speaker_b_id speaker B current dataset adding new column speaker_id. case_when() function exactly : allows us map rows speaker value \"\" speaker_a_id rows value \"B\" speaker_b_id. now tidy dataset set create. dataset includes one conversation file! want apply code 1155 conversation files sdac/ corpus. approach create custom function groups code done single file iterative send file corpus function combine results one data frame. custom function extra code print progress message file runs. sanity check run extract_sdac_metadata() function conversation file just working make sure works expected. Looks good! now time create vector paths conversation files. fs::dir_ls() interfaces OS file system return paths files specified directory. also add pattern match conversation files (regexp = \\\\.utt$) accidentally include files corpus. recurse set TRUE means get full path file. o pass conversation file vector paths conversation files iteratively extract_sdac_metadata() function use map(). apply function conversation file return data frame . bind_rows() join resulting data frames rows give us single tidy dataset 1155 conversations. Note lot processing going patient. now see nrow(sdac) observations (individual utterances dataset).","code":"o          A.1 utt1: Okay.  / qw          A.1 utt2: {D So, } qy^d          B.2 utt1: [ [ I guess, + +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? / +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. / qy          A.5 utt1: Does it say something? / sd          B.6 utt1: I think it usually does.  / ad          B.6 utt2: You might try, {F uh, }  / h          B.6 utt3: I don't know,  / ad          B.6 utt4: hold it down a little longer,  / ^.+?\\s [AB]\\.\\d+ utt\\d+ :.+$ ../data/original/sdac/sw00utt/sw_0001_4325.utt ../data/original/sdac/sw00utt/sw_0002_4330.utt ../data/original/sdac/sw00utt/sw_0003_4103.utt ../data/original/sdac/sw00utt/sw_0004_4327.utt ../data/original/sdac/sw00utt/sw_0005_4646.utt ../data/original/sdac/sw00utt/sw_0006_4108.utt"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"write-datasets","dir":"Articles","previous_headings":"Cases: > Semi-structured: SWDA (.utt) files","what":"Write datasets","title":"6. Organizing and documenting data","text":"previous cases, write dataset disk prepare next step text analysis project. directory structure now looks like :","code":"data/ ├── derived/ │   └── sdac/ │       └── sdac_curated.csv └── original/     └── sdac/         ├── README         ├── doc/         ├── sw00utt/         ├── sw01utt/         ├── sw02utt/         ├── sw03utt/         ├── sw04utt/         ├── sw05utt/         ├── sw06utt/         ├── sw07utt/         ├── sw08utt/         ├── sw09utt/         ├── sw10utt/         ├── sw11utt/         ├── sw12utt/         └── sw13utt/"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"6. Organizing and documenting data","text":"(... examples ...) TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-7.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"7. Transforming and documenting data","text":"(... examples ...) TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"8. Exploratory Data Analysis","text":"(... examples ...) TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"9. Predictive Modeling","text":"(... examples ...) TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jerid Francom. Author, maintainer.","code":""},{"path":"https://qtalr.github.io/qtalrkit/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Francom J (2023). qtalrkit: Quantitative Text Analysis Linguists Resource Kit. R package version 0.0.4.0, https://qtalr.github.io/qtalrkit/, https://github.com/qtalr/qtalrkit.","code":"@Manual{,   title = {qtalrkit: Quantitative Text Analysis for Linguists Resource Kit},   author = {Jerid Francom},   year = {2023},   note = {R package version 0.0.4.0, https://qtalr.github.io/qtalrkit/},   url = {https://github.com/qtalr/qtalrkit}, }"},{"path":"https://qtalr.github.io/qtalrkit/index.html","id":"quantitative-text-analysis-for-linguistics-resources-kit","dir":"","previous_headings":"","what":"Quantitative Text Analysis for Linguists Resource Kit","title":"Quantitative Text Analysis for Linguists Resource Kit","text":"goal qtalrkit provide supporting resources book “Introduction Quantitative Text Analysis Linguistics: Reproducible Research using R”.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/add_pkg_to_bib.html","id":null,"dir":"Reference","previous_headings":"","what":"Add package to BibTeX file — add_pkg_to_bib","title":"Add package to BibTeX file — add_pkg_to_bib","text":"function adds package BibTeX file. uses knitr::write_bib function write package name file.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/add_pkg_to_bib.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add package to BibTeX file — add_pkg_to_bib","text":"","code":"add_pkg_to_bib(pkg_name, bib_file = \"packages.bib\")"},{"path":"https://qtalr.github.io/qtalrkit/reference/add_pkg_to_bib.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add package to BibTeX file — add_pkg_to_bib","text":"pkg_name name package add BibTeX file. bib_file name BibTeX file write .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/add_pkg_to_bib.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add package to BibTeX file — add_pkg_to_bib","text":"","code":"if (FALSE) { add_pkg_to_bib(\"dplyr\", \"my_bib_file.bib\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"function calculates various association metrics (PMI, Dice's Coefficient, Lambda-Rank) bigrams given corpus. data frame must contain document token indices, well 'type' variable representing tokens.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"","code":"calc_assoc_metrics(   data,   doc_index,   token_index,   type,   association = \"all\",   verbose = FALSE )"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"data data frame containing corpus. doc_index string name column 'data' represents document index. token_index string name column 'data' represents token index. type string name column 'data' represents tokens terms. association character vector specifying metrics calculate. Can combination 'pmi' (Pointwise Mutual Information), 'dice_coeff' (Dice's Coefficient), 'g_score' (G-score), '' (calculate metrics). Default ''. verbose logical value indicating whether keep intermediate probability columns ('p_xy', 'p_x', 'p_y') result. Default FALSE.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"data frame one row per bigram columns calculated metric. 'verbose' TRUE, intermediate probabilities used calculations also included.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"","code":"if (FALSE) { library(dplyr) data <- tibble::tibble(   doc_index = c(1, 1, 1, 2),   token_index = c(1, 2, 3, 1),   type = c(\"word1\", \"word2\", \"word3\", \"word2\") ) calc_assoc_metrics(data, doc_index, token_index, type) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Document Frequency (DF) — calc_df","title":"Calculate Document Frequency (DF) — calc_df","text":"function internal use .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Document Frequency (DF) — calc_df","text":"","code":"calc_df(tdm)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Document Frequency (DF) — calc_df","text":"tdm term-document matrix (TDM) row represents type column represents document.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Document Frequency (DF) — calc_df","text":"numeric vector containing Document Frequency 'DF' type TDM.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Document Frequency (DF) — calc_df","text":"function calculates Document Frequency 'DF' type (e.g., term, lemma) term-document matrix (TDM). intended used within package calc_dispersion_metrics function.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"function internal use .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"","code":"calc_dp(tdm_normalized, corpus_parts)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"tdm_normalized normalized term-document matrix (TDM) row represents type column represents document. values proportions type's frequency total frequency across documents. corpus_parts numeric vector containing proportions document corpus, used calculate Deviation Proportions (DP).","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"numeric vector containing Deviation Proportions (DP) type TDM.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"function calculates Deviation Proportions (DP) based Gries' Deviation Proportions method. intended used within package, particularly calc_dispersion_metrics function.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Inverse Document Frequency (IDF) — calc_idf","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"function internal use .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"","code":"calc_idf(tdm)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"tdm term-document matrix (TDM) row represents type column represents document.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"numeric vector containing Inverse Document Frequency 'DF' type TDM.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"function calculates Inverse Document Frequency 'IDF' type (e.g., term, lemma) term-document matrix (TDM). intended used within package calc_dispersion_metrics function.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"function takes categorical variable input calculates normalized entropy variable. normalized entropy measure amount uncertainty randomness variable, normalized maximum possible entropy variable.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"","code":"calc_normalized_entropy(x)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"x categorical variable.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"normalized entropy variable.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"","code":"if (FALSE) { # Calculate the normalized entropy of a vector of categorical data x <- c(\"A\", \"B\", \"B\", \"C\", \"C\", \"C\", \"D\", \"D\", \"D\", \"D\") calc_normalized_entropy(x) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Type Metrics for Text Data — calc_type_metrics","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"function calculates type metrics tokenized text data.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"","code":"calc_type_metrics(data, type, documents, frequency = NULL, dispersion = NULL)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"data data frame containing tokenized text data type variable data contains type (e.g., term, lemma) analyze. documents variable data contains document IDs. frequency character vector indicating frequency metrics use. NULL (default), type n returned. options: '', 'rf' calculates relative frequency, 'orf' calculates observed relative frequency. Can specify multiple options: c(\"rf\", \"orf\"). dispersion character vector indicating dispersion metrics use. NULL (default), type n returned. options: '', 'df' calculates Document Frequency. 'idf' calculates Inverse Document Frequency. 'dp' calculates Gries' Deviation Proportions. Can specify multiple options: c(\"df\", \"idf\").","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"data frame columns: type: unique types input data. n: frequency type across documents. Optionally (based frequency dispersion arguments): rf: relative frequency type across documents. orf: observed relative frequency (per 100) type across documents. df: document frequency type. idf: inverse document frequency type. dp: Gries' Deviation Proportions type.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"Gries, Stefan Th. (2023). Statistical Methods Corpus Linguistics. Readings Corpus Linguistics: Teaching Research Guide Scholars Nigeria Beyond, pp. 78-114.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"","code":"if (FALSE) { data <- data.frame(   term = c(\"word1\", \"word1\", \"word2\", \"word2\", \"word2\", \"word3\"),   documents = c(\"doc1\", \"doc2\", \"doc1\", \"doc1\", \"doc2\", \"doc2\") ) calc_type_metrics(   data = data,   type = term,   documents = documents,   frequency = c(\"rf\", \"orf\"),   dispersion = c(\"df\", \"idf\") ) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/confirm_permission.html","id":null,"dir":"Reference","previous_headings":"","what":"Confirm permission to use data — confirm_permission","title":"Confirm permission to use data — confirm_permission","text":"function internal use .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/confirm_permission.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confirm permission to use data — confirm_permission","text":"","code":"confirm_permission()"},{"path":"https://qtalr.github.io/qtalrkit/reference/confirm_permission.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confirm permission to use data — confirm_permission","text":"TRUE user confirms permission, FALSE otherwise","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/confirm_permission.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Confirm permission to use data — confirm_permission","text":"function confirms user permission use data. , script returns FALSE stops.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a data dictionary for a given data frame. — create_data_dictionary","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"function takes data frame creates data dictionary. data dictionary includes variable name, human-readable name, variable type, description. model specified, function uses OpenAI's API generate information based characteristics data frame.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"","code":"create_data_dictionary(   data,   file_path,   model = NULL,   sample_n = 5,   grouping = NULL,   force = FALSE )"},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"data data frame create data dictionary . file_path file path save data dictionary . model ID OpenAI chat completion models use generating descriptions. NULL (default), scaffolding data dictionary created. sample_n number rows sample data frame use input model. Default NULL. grouping character vector column names group sampling rows data frame model. Default NULL. force TRUE, overwrite file file_path already exists. Default FALSE.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"data frame containing variable name, human-readable name, variable type, description variable input data frame.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"","code":"if (FALSE) { data(mtcars) create_data_dictionary(mtcars, \"mtcars_data_dictionary.csv\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":null,"dir":"Reference","previous_headings":"","what":"Create data origin file — create_data_origin","title":"Create data origin file — create_data_origin","text":"function creates data frame attributes origin data, writes CSV file specified file path, returns data frame.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create data origin file — create_data_origin","text":"","code":"create_data_origin(file_path)"},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create data origin file — create_data_origin","text":"file_path character string specifying file path data origin file saved.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create data origin file — create_data_origin","text":"tibble containing data origin information.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create data origin file — create_data_origin","text":"","code":"if (FALSE) { create_data_origin(\"data_origin.csv\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify Outliers in a Numeric Variable — find_outliers","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"function identifies outliers numeric variable data.frame using interquartile range (IQR) method.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"","code":"find_outliers(data, variable_name)"},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"data data.frame object. variable_name symbol representing numeric variable data.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"data.frame containing outliers variable_name.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"","code":"if (FALSE) { data(mtcars) find_outliers(mtcars, mpg) find_outliers(mtcars, wt) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a Compressed File and Decompress its Contents — get_compressed_data","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"Possible file types include .zip, .gz, .tar, .tgz","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"","code":"get_compressed_data(url, target_dir, force = FALSE, confirmed = FALSE)"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"url character vector representing full url compressed file target_dir directory compressed file downloaded force optional argument forcefully overwrites existing data confirmed TRUE, user confirmed permission use data. FALSE, function prompt user confirm permission. Setting TRUE useful reproducible workflows.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"Download extract compressed data file","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"","code":"if (FALSE) { get_compressed_data(url = \"http://www.test.com/file.zip\", target_dir = \"./\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"Retrieves Gutenberg works based specified criteria saves data CSV file.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"","code":"get_gutenberg_works(   target_dir,   lcc_subject,   birth_year = NULL,   death_year = NULL,   force = FALSE,   confirmed = FALSE )"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"target_dir directory CSV file saved. lcc_subject character vector specifying Library Congress Classification (LCC) subjects filter works. birth_year optional integer specifying minimum birth year authors include. death_year optional integer specifying maximum death year authors include. force logical value indicating whether overwrite existing data already exists. confirmed logical value indicating whether skip confirmation prompt number works greater 1000.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"None","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"function retrieves Gutenberg works based specified LCC subjects optional author birth death years. checks data already exists target directory provides option overwrite . function also creates target directory exist. number works greater 1000 'confirmed' parameter set TRUE, prompts user confirmation. retrieved works filtered based public domain rights USA availability text. resulting works downloaded saved CSV file target directory. information Library Congress Classification (LCC) subjects, refer Library Congress Classification Guide.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"","code":"if (FALSE) { # Retrieve works with LCC subject \"Political Theory\" and save to \"/path/to/works_fiction.csv\" get_gutenberg_works(\"/path/to\", \"JC\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Downloads TalkBank data and saves it to disk — get_talkbank_data","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"Downloads utterances, transcripts, participants, tokens, token types data TalkBank database saves disk specified target directory.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"","code":"get_talkbank_data(   corpus_name,   corpus_path,   target_dir,   force = FALSE,   confirmed = FALSE )"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"corpus_name name TalkBank corpus download data . corpus_path path TalkBank corpus download data . target_dir directory save downloaded data . force TRUE, data downloaded even already exists disk. confirmed TRUE, user confirmed permission use data. FALSE, function prompt user confirm permission. Setting TRUE useful reproducible workflows.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"message indicating whether data acquired already existed disk.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"","code":"if (FALSE) { # Download CABNC data from the Conversation Bank to a directory called \"data\" get_talkbank_data(corpus_name = \"ca\", corpus_path = c(\"ca\", \"CABNC\"), target_dir = \"data\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/qtalrkit-package.html","id":null,"dir":"Reference","previous_headings":"","what":"qtalrkit: Quantitative Text Analysis for Linguists Resource Kit — qtalrkit-package","title":"qtalrkit: Quantitative Text Analysis for Linguists Resource Kit — qtalrkit-package","text":"Support package textbook \"Introduction Quantitative Text Analysis Linguists: Reproducible Research using R\". Includes access interactive code exercises demos, data, misc functions.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/reference/qtalrkit-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"qtalrkit: Quantitative Text Analysis for Linguists Resource Kit — qtalrkit-package","text":"Maintainer: Jerid Francom francojc@wfu.edu (ORCID)","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0040","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.4.0","title":"qtalrkit 0.0.4.0","text":"Adds get_gutenberg_works() function import data Project Gutenberg","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003400","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.400","title":"qtalrkit 0.0.3.400","text":"Fixes warnings calc_assoc_metrics() Updates Date DESCRIPTION file Adds test-add_pkg_to_bib.R","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.000","title":"qtalrkit 0.0.3.000","text":"Adds calc_assoc_metrics calculate (pmi, dice, G) given type bigram","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003210","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.210","title":"qtalrkit 0.0.3.210","text":"Removes calc_dispersion_metrics() function replaces calc_type_metric() includes frequency dispersion metrics.","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003200","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.200","title":"qtalrkit 0.0.3.200","text":"Fixes bug get_compressed_data() caused function create dot file copies original files","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003100","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.100","title":"qtalrkit 0.0.3.100","text":"Adds idf measure calc_dispersion_metrics()","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003000-1","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.000","title":"qtalrkit 0.0.3.000","text":"Adds calc_dispersion_metrics() function calculate dispersion metrics","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-002000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.2.000","title":"qtalrkit 0.0.2.000","text":"Added get_talkbank_data() function import data TalkBank Added internal confirm_permissions() function confirm users aware permissions required use data Updated get_*() functions use confirm_permissions() internally Changed get_outliers() find_outliers() consistent functions","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019400","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9400","title":"qtalrkit 0.0.1.9400","text":"Updated create_data_dictionary() provide default scaffold structure data dictionary, lieu OpenAI model. scaffold updated manually user.","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019300","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9300","title":"qtalrkit 0.0.1.9300","text":"Added create_data_origin() function. creates .csv file scaffold data origin file","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019200","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9200","title":"qtalrkit 0.0.1.9200","text":"Adds project template RStudio: “Minimal Reproducible Project”","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019100","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9100","title":"qtalrkit 0.0.1.9100","text":"Adjusted create_data_dictionary() produce results line QTALR textbook","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9000","title":"qtalrkit 0.0.1.9000","text":"Added get_outliers() function Added Instructor Guide","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0010000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.0000","title":"qtalrkit 0.0.1.0000","text":"Added R tutorial 0","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0009000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.0.9000","title":"qtalrkit 0.0.0.9000","text":"Added NEWS.md file track changes package.","code":""}]
