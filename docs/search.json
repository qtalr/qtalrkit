[{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/_R_ideas.html","id":"templates","dir":"","previous_headings":"","what":"Templates","title":"Ideas to include","text":"Create project templates (https://rstudio.github.io/rstudio-extensions/rstudio_project_templates.html)","code":""},{"path":"https://qtalr.github.io/qtalrkit/_R_ideas.html","id":"functions","dir":"","previous_headings":"","what":"Functions","title":"Ideas to include","text":"create_codebook() - create codebook script dataset.","code":""},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Setting up an R environment","text":"guide, explore options setting R environment. discuss local, remote, virtual environments. advantages shortcomings. Outcomes Understand different options working R Learn advantages disadvantages option Choose option best fits needs","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"local-environments","dir":"Articles","previous_headings":"Environment setups","what":"Local environments","title":"Setting up an R environment","text":"Choosing work R locally means install R IDE local computer. approach offers following advantages: Fast responsive performance reliance internet connectivity Flexibility customize environment main disadvantages working locally : need install R IDE local computer, manage software environment, manage backups version control collaborative projects. can challenge new users, number resources available help get started troubleshoot issues may encounter. get started, install R CRAN. can download latest version R operating system . installed R, need install IDE. recommend RStudio, free open-source IDE R. RStudio provides number features make easier work R. projects require collaboration, recommend using version control system Git hosting service GitHub GitLab. tools help manage code collaborate others.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"remote-environments","dir":"Articles","previous_headings":"Environment setups","what":"Remote environments","title":"Setting up an R environment","text":"can also choose work R cloud, remote environment. number cloud-based options working R, including Posit Cloud Microsoft Azure. options provide pre-configured R environment can access computer internet connection. Posit Cloud provides environment can create, edit, run R projects anywhere internet access. offers several advantages: need install R RStudio locally Access projects device Collaborate others real-time Easily share work drawbacks working cloud include: Reliance stable internet connection Potential latency performance issues Limited customization options compared local setup get started Posit Cloud, need create account. can sign free account . created account, see list spaces. default personal workspace, can also join invited spaces. Visit Guide documentation learn features Posit Cloud.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"virtual-environments","dir":"Articles","previous_headings":"Environment setups","what":"Virtual environments","title":"Setting up an R environment","text":"new R, may want consider working cloud get started. plan continue work R future, likely want install R IDE local computer explore using virtual environment. Virtual environments, Docker, provide way use pre-configured computing environment create can share others. Virtual environments good option want ensure everyone research group working computing environment. Pre-configured virtual environments exist R Rocker project can used locally cloud. Docker platform allows create, deploy, run applications containers. Rocker collection Docker images specifically designed running R. Docker enables package application along dependencies container, ensuring run consistently across different environments. Rocker extends concept R, providing pre-built Docker images various R configurations. Using Docker Rocker offers several benefits: Reproducible environments Simplified dependency management Easy deployment scaling drawbacks using Docker Rocker include: Learning curve setting managing Docker containers Increased memory resource requirements Potential compatibility issues certain packages libraries start using Docker Rocker, follow steps: Install Docker local machine Pull desired Rocker image Docker Hub Run container using pulled image Access RStudio browser http://localhost:8787 log username rstudio password set","code":"docker pull rocker/rstudio docker run -d -p 8787:8787 -e PASSWORD=your_password --name rstudio_container rocker/rstudio"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-1.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Setting up an R environment","text":"guide, discussed strategies working R. three options offer unique advantages. ??, summarize characteristics, benefits, drawbacks option. Table 1:  Comparison different environments working R RStudio Give try see one works best needs! Remember, can always switch different environments needs change.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Setting up Git and GitHub","text":"guide, cover basics setting Git GitHub. also cover basics using Git GitHub manage project. guide intended beginners new Git GitHub. also intended new using Git GitHub R. Outcomes Recognize purpose Git GitHub Establish working Git GitHub environment Recognize basic Git GitHub workflow managing project","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"what-is-git-github-and-why-should-i-use-it","dir":"Articles","previous_headings":"","what":"What is Git, Github? And why should I use it?","title":"Setting up Git and GitHub","text":"Git version control system. allows track changes files folders time. also allows collaborate others projects. Think MS Word's \"Track Changes\" feature steroids. Git command line tool, also graphical user interfaces (GUIs) interact Git user-friendly way. Git great tool managing projects. especially useful managing projects involve multiple people. GitHub web-based hosting service Git repositories. allows store Git repositories cloud. also allows collaborate others projects sharing projects. GitHub great place store share code. also great place find code others shared. Combining Git GitHub allows store Git managed project repositories cloud. means can access repositories anywhere. also means can collaborate others projects. can also use GitHub share code others. especially useful making projects reproducible.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"install-and-setup-git","dir":"Articles","previous_headings":"How do I set up Git?","what":"Install and setup Git","title":"Setting up Git and GitHub","text":"process installation setup differ based operating system using. using Windows machine, likely need install Git. using Mac Linux machine, likely already Git installed. Windows users need install Git. can downloading installer https://git-scm.com/downloads. downloaded installer, need run . need follow instructions installer complete installation. Mac Linux users can verify Git installed opening terminal window typing git --version. Git installed, see version number. Git installed, see error message. Git installed, need set Git configuration. defaults fine, need set name email address, address used create GitHub account. can opening terminal window typing following commands: Alternatively, can use usethis package R set Git environment. Open RStudio run following code console:","code":"git config --global user.name \"Your Name\" git config --global user.email \"your.email@email.edu\" if (!require(\"usethis\")) {   install.packages(\"usethis\") } library(usethis)  use_git_config(user.name = \"Your Name\", user.email = \"your.email@school.edu\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"how-do-i-set-up-github","dir":"Articles","previous_headings":"","what":"How do I set up GitHub?","title":"Setting up Git and GitHub","text":"set GitHub, need create account. can <github.com>. service free extra features available students educators. created account, able create repositories. can also create organizations teams. can use collaborate others projects.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-2.html","id":"how-do-i-manage-my-project-with-git-and-github","dir":"Articles","previous_headings":"","what":"How do I manage my project with Git and GitHub?","title":"Setting up Git and GitHub","text":"Git installation Github account set , can manage local remote repositories. can command line, recommedable use Git pane RStudio interface Git GitHub. can perform various key operations. introduce operations related common tasks textbook. . Copy remote lab repository local machine: Navigate repository GitHub Click 'Code' button copy clone URL (https) File > New Project > Version Control > Git Paste URL repository Choose directory location project Create Project B. Fork clone remote template repository local machine: Navigate repository GitHub Click 'Fork' button Select account fork repository forked repository, click 'Code' button copy clone URL (https) File > New Project > Version Control > Git Paste URL forked repository Choose directory location project Create Project C. Create/ Join remote repository clone local machine: Click 'New' button top right corner repositories listing page Name repository Select 'Public' 'Private' Select 'Initialize repository README' Click 'Create Repository' new repository, click 'Code' button copy clone URL (https) File > New Project > Version Control > Git Paste URL repository Choose directory location project Create Project Tip Bryan Hester (2020) excellent reference resource things Git GitHub R users.","code":""},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Creating reproducible examples","text":"guide, explore create reproducible examples using reprex package (Bryan et al. 2022) R. Reproducible examples essential effective communication collaboration among data scientists statisticians. make great R reproducible example Outcomes ...","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"what-is-a-reproducible-example","dir":"Articles","previous_headings":"Introduction","what":"What is a Reproducible Example?","title":"Creating reproducible examples","text":"Reproducible examples crucial effectively communicating problems, solutions, ideas world data science. post, discuss importance reproducible examples demonstrate create using reprex package R. reproducible example, often referred \"reprex,\" minimal, self-contained piece code demonstrates specific issue concept. include: brief description problem question necessary data reproduce issue R code used generate output actual output, including error messages warnings","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"why-use-the-reprex-package","dir":"Articles","previous_headings":"Introduction","what":"Why Use the reprex Package?","title":"Creating reproducible examples","text":"reprex package R streamlines process creating reproducible examples : Automatically capturing code, input data, output Formatting example easy sharing various platforms (e.g., GitHub, Stack Overflow) Encouraging best practices creating clear concise examples","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"installing-and-loading-the-reprex-package","dir":"Articles","previous_headings":"Introduction","what":"Installing and Loading the reprex Package","title":"Creating reproducible examples","text":"get started reprex package, first install CRAN load R session:","code":"install.packages(\"reprex\") library(reprex)"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"creating-a-reproducible-example-with-reprex","dir":"Articles","previous_headings":"Introduction","what":"Creating a Reproducible Example with reprex","title":"Creating reproducible examples","text":"section, demonstrate create reproducible example using reprex package.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"basic-usage","dir":"Articles","previous_headings":"Introduction > Creating a Reproducible Example with reprex","what":"Basic Usage","title":"Creating reproducible examples","text":"create simple reprex, write R code call reprex() function: generate formatted output includes code, input data, results.","code":"library(reprex)  code <- ' x <- 1:10 mean(x) '  reprex(input = code)"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"customizing-output-format","dir":"Articles","previous_headings":"Introduction > Creating a Reproducible Example with reprex","what":"Customizing Output Format","title":"Creating reproducible examples","text":"can customize output format reprex specifying venue argument. example, create reprex suitable GitHub, use:","code":"reprex(input = code, venue = \"gh\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"including-data","dir":"Articles","previous_headings":"Introduction > Creating a Reproducible Example with reprex","what":"Including Data","title":"Creating reproducible examples","text":"example requires specific data, can include using dput() function: incorporate data reprex, allowing others reproduce example easily.","code":"data <- data.frame(x = 1:10, y = 11:20) data_dput <- dput(data)  code_with_data <- ' data <- {{ data_dput }} plot(data$x, data$y) '  reprex(input = code_with_data)"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"sharing-your-reproducible-example","dir":"Articles","previous_headings":"Introduction","what":"Sharing Your Reproducible Example","title":"Creating reproducible examples","text":"created reprex, can share various platforms GitHub, Stack Overflow, via email. formatted output generated reprex package ensures example easy read understand.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"conclusion","dir":"Articles","previous_headings":"Introduction","what":"Conclusion","title":"Creating reproducible examples","text":"blog post, discussed importance reproducible examples demonstrated create using reprex package R. creating clear concise reprexes, can effectively communicate problems, solutions, ideas peers collaborators. Give reprex package try see can improve workflow!","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-3.html","id":"references","dir":"Articles","previous_headings":"Introduction","what":"References","title":"Creating reproducible examples","text":"StackOverflow: R, Git, RStudio, GitHub Reddit: R, Git, RStudio, Github RStudio Community https://reprex.tidyverse.org/ https://github.com/MilesMcBain/datapasta Datapasta package allows copy paste data frames RStudio reprex. useful tool creating reproducible examples. example use datapasta create reprex.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"repositories","dir":"Articles","previous_headings":"Published","what":"Repositories","title":"Identifying data and data sources","text":"Language-dedicated repositories great source data language research. included listing commonly used repositories. Table 1: Data repositories","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"corpora-and-datasets","dir":"Articles","previous_headings":"Published","what":"Corpora and datasets","title":"Identifying data and data sources","text":"included listing corpora datasets available language research. list exhaustive, includes common corpora datasets used language research. Table 2: Corpora language datasets","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"data-sharing-platforms","dir":"Articles","previous_headings":"Published","what":"Data sharing platforms","title":"Identifying data and data sources","text":"https://dataverse.org/ https://osf.io/ https://www.zenodo.org/ https://figshare.com/ https://www.researchgate.net/ https://www.researchsquare.com/","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"aggregated-listings","dir":"Articles","previous_headings":"Published","what":"Aggregated listings","title":"Identifying data and data sources","text":"list data available language research constantly growing. document wide variety resources. Table 3 included attempts others provide summary corpus data language resources available. Table 3: Aggregated listings language corpora datasets","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"application-programming-interfaces-apis","dir":"Articles","previous_headings":"Custom-built","what":"Application programming interfaces (APIs)","title":"Identifying data and data sources","text":"many APIs available accessing language corpora datasets. included R packages provide access resources. Table 4: R Package APIs language corpora datasets.)","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-4.html","id":"other-language-resources","dir":"Articles","previous_headings":"","what":"Other language resources","title":"Identifying data and data sources","text":"Data language research limited (primary) text sources. sources may include processed data previous research; word lists, linguistic features, etc.. Alone combination text sources data can rich viable source data research project. Table 5: language resources","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"html-language-of-the-web","dir":"Articles","previous_headings":"","what":"HTML: language of the web","title":"Web scraping with R","text":"HTML cousin XML (eXtensible Markup Language) organizes web documents hierarchical format read browser navigate web. Take example toy webpage created demonstration (fig-example-webpage?). Figure 1: Example web page. file accessed browser render webpage test.html plain-text format seen (exm-html-structure?). element file delineated opening closing HTML tag, <head><\/head>. Tags nested within tags create structural hierarchy. Tags can take class id labels distinguish tags often contain attributes dictate tag behave according Cascading Style Sheet (CSS) rules rendered visually browser. example, two <div> tags toy example: one label class = \"intro\" class = \"conc\". <div> tags often used separate sections webpage may require special visual formatting. <> tag, hand, creates web link. part tag's function, requires attribute href= web protocol --case link email address mailto:francojc@wfu.edu. often , however, href= contains URL (Uniform Resource Locator). working example might look like : <href=\"https://francojc.github.io/\">homepage<\/>. {{< fa medal >}} Dive deeper Cascading Style Sheets (CSS) used dictate HTML elements rendered visually browser. example, div tag class attribute intro targeted CSS rule render text larger font size bold. CSS rules often written separate file linked HTML file. web scraping purposes, however, interested visual rendering HTML file, rather structure HTML file. tag attributes can provide useful information parsing HTML files. aim web scrape download HTML file(s) contain data interested . include information may ultimately need, downloading raw source HTML effectively creating local archive, copy, webpage. Thus, webpage updated removed web, still access data accessed. Later curation process parse (.e. read extract) target information relevant research hand. However, often useful parse raw HTML process acquiring data interested harvesting data multiple pages like use HTML structure guide data extraction (.e. URLs pages) provide preliminary background working HTML, use toy example demonstrate read parse HTML using R. use rvest(Wickham 2022) package. First, install/load package, , read parse HTML character vector named web_file assigning result html. (exm-read-html-toy?) read_html() retrieves raw HTML makes accessible parsing R. subtype XML, read_html() converts raw HTML object class xml_document, can see calling class() html object (exm-class-html-toy-class?). object class xml_document represents HTML tag node. tag nodes elements can accessed using html_elements() function specifying tag/node/element isolate. Notice output (exm-parse-html-toy-1?) returned div tags respective children, tags contained within. isolate one tags class, add class name tag separating .. Great. Now say want drill isolate subordinate <p> nodes. can add p node filter, (exm-parse-html-toy-3?). extract text contained within node use html_text() function. result (exm-parse-html-toy-4?) character vector two elements corresponding text contained <p> tag. paying close attention might noticed second element vector includes extra whitespace period. trim leading trailing whitespace text can add trim = TRUE argument html_text(), (exm-parse-html-toy-5?). basic understanding read parse HTML, can now turn realistic example.","code":"<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\"> <html>   <head>     <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />     <title>My website<\/title>   <\/head>   <body>     <div class=\"intro\">       <p>Welcome!<\/p>       <p>This is my first website.<\/p>     <\/div>     <table>       <tr>         <td>Contact me:<\/td>         <td>           <a href=\"mailto:francojc@wfu.edu\">francojc@wfu.edu<\/a>         <\/td>       <\/tr>     <\/table>     <div class=\"conc\">       <p>Good-bye!<\/p>     <\/div>   <\/body> <\/html> ## {html_document} ## <html> ## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ... ## [2] <body>\\n    <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is  ... ## [1] \"xml_document\" \"xml_node\" ## {xml_nodeset (2)} ## [1] <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is my first web ... ## [2] <div class=\"conc\">\\n      <p>Good-bye!<\/p>\\n    <\/div> ## {xml_nodeset (1)} ## [1] <div class=\"intro\">\\n      <p>Welcome!<\/p>\\n      <p>This is my first web ... ## {xml_nodeset (2)} ## [1] <p>Welcome!<\/p> ## [2] <p>This is my first website.<\/p> ## [1] \"Welcome!\"                  \"This is my first website.\" ## [1] \"Welcome!\"                  \"This is my first website.\""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"federalist-papers","dir":"Articles","previous_headings":"Acquire data from the web","what":"Federalist Papers","title":"Web scraping with R","text":"Say investigate authorship question Federalist Papers following footsteps Mosteller Wallace (1963). want scrape text Federalist Papers Library Congress website. main page Federalist Papers located https://guides.loc.gov/federalist-papers/full-text can seen (fig-web-scrape-screenshot?). Figure 2: Screenshot Library Congress website Federalist Papers main page contains links text 85 papers. goal scrape archive raw HTML page parse HTML extract links papers. scrape archive raw HTML 85 papers. first step web scrape investigate site page(s) want scrape ascertain licensing restrictions. Many, websites, include plain text file robots.txt root main URL. file declares webpages 'robot' (including web scraping scripts) can access. can use robotstxt package find URLs accessible 1. next step read parse raw HTML. can using read_html() function. point captured raw HTML assigning object named html. archive raw HTML file project directory. can using write_html() function xml2 package (R-xml2?). update project directory structure can seen (exm-fed-project-dir?). Now, also want scrape HTML contains pages corresponding 85 Federalist Papers. Perusing main page can see papers organized nine groups, e.g. \"Federalist Nos. 1-10\". aim scrape HTML nine pages. can using rvest package, need identify HTML elements contain URLs first main webpage html. helpful use browser inspect specific elements webpage, much toy example (exm-html-structure?). view raw displayed HTML, browser equipped command can enable hovering mouse element page want target using right click select \"Inspect\" (Chrome) \"Inspect Element\" (Safari, Brave). split browser window vertical horizontally showing displayed raw HTML underlying webpage. can see HTML elements contain URLs (fig-fed-inspect?). Figure 3: Screenshot HTML elements containing URLs Federalist Papers take closer look source HTML (fig-fed-inspect-source?) can inspect elements contain URLs divise strategy isolating extracted. Figure 4: Screenshot source HTML Federalist Papers (fig-fed-inspect-source?) can see HTML elements contain URLs nested within <ul> element. <ul> element set class attributes (.s-lg-subtab-ul, nav, nav-pills, nav-stacked). one unique <ul> element can use isolate element. search HTML <ul> elements page. output (exm-fed-papers-loc-url-ul?) shows 4 <ul> elements page. little hard see output, second <ul> element one targeting, contains class attributes identified (fig-fed-inspect-source?). can use html_attr() function extract class attribute <ul> elements see one unique <ul> element want isolate. Effectively case, second <ul> element output (exm-fed-papers-loc-url-ul-class?) unique class attribute, .s-lg-subtab-ul. can use isolate element using html_nodes() function. pipe another html_nodes() function isolate <li> elements nested within <ul> element. See (exm-fed-papers-loc-url-ul-class-s-lg-subtab-ul?). Great. Now, get URLs add another html_nodes() function (exm-fed-papers-loc-url-ul-class-s-lg-subtab-ul?) isolate <> elements nested within <li> elements function html_attr() extract value attribute. case, attribute <> elements want href. See (exm-fed-papers-loc-url-ul-li-?). can assign URLs variable, fed_urls. URLs hand, can now retrieve HTML nine pages. can, course, manually, (exm-fed-papers-rewrite-html-manual?). tedious error prone, furthermore, scale well. 1000 URLs retrieve HTML , write 1000 lines code. Instead, can write function us. See (exm-fed-papers-rewrite-function?). function (exm-fed-papers-rewrite-function?) takes URL argument. creates file name path URL. file name last part URL, extension .html. file path path federalist_papers directory data/original directory. function reads HTML URL writes file. (exm-fed-papers-rewrite-function?) might seem like step (exm-fed-papers-rewrite-html-manual?), . can now use walk() function purrr iterate URLs fed_urls apply read_write_html() function URL. See (exm-fed-papers-rewrite-map?). {{< fa regular hand-point->}} Tip processing multiple webpages, often important manage load server. R, can use Sys.sleep() introduce short delays requests. helps reduce server load iterating list webpages. example, can use Sys.sleep(1) function introduce 1 second delay requests. Another tip use message() function print status message console. can helpful processing large number webpages. result (exm-fed-papers-rewrite-map?) can seen project directory (exm-fed-papers-directory?). course, finish acquisition process, need ensure documented code created data origin file. Since created resource much information use document. Keep mind data origin file written way transparent researcher -collaborators general research community. section, built previously introduced R coding concepts employed various others process acquiring data web. also considered topics general nature concern interacting data found internet. likely appreciate, web scraping often requires knowledge familiarity R well web technologies. Rest assured, however, practice increase confidence abilities. encourage practice websites.","code":"## [1] TRUE ## {html_document} ## <html lang=\"en\"> ## [1] <head>\\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\">\\n<meta http ... ## [2] <body class=\"s-lg-guide-body\">\\r\\n<a id=\"s-lg-public-skiplink\" class=\"ale ... data/ |── analysis/ ├── derived/ └── original/     └── federalist_papers/         └── main.html ## {xml_nodeset (4)} ## [1] <ul class=\"nav nav-pills nav-stacked split-button-nav\" role=\"menu\">\\n<li  ... ## [2] <ul class=\"s-lg-subtab-ul nav nav-pills nav-stacked\">\\n<li class=\"\"><a ti ... ## [3] <ul id=\"s-lg-page-prevnext\" class=\"pager s-lib-hide\">\\n<li class=\"previou ... ## [4] <ul id=\"s-lg-guide-header-attributes\" class=\"\">\\n<li id=\"s-lg-guide-heade ... ## [1] \"nav nav-pills nav-stacked split-button-nav\" ## [2] \"s-lg-subtab-ul nav nav-pills nav-stacked\"   ## [3] \"pager s-lib-hide\"                           ## [4] \"\" ## {xml_nodeset (9)} ## [1] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [2] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [3] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [4] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [5] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [6] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [7] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [8] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [9] <li class=\"\"><a title=\"\" href=\"https://guides.loc.gov/federalist-papers/t ... ## [1] \"https://guides.loc.gov/federalist-papers/text-1-10\"  ## [2] \"https://guides.loc.gov/federalist-papers/text-11-20\" ## [3] \"https://guides.loc.gov/federalist-papers/text-21-30\" ## [4] \"https://guides.loc.gov/federalist-papers/text-31-40\" ## [5] \"https://guides.loc.gov/federalist-papers/text-41-50\" ## [6] \"https://guides.loc.gov/federalist-papers/text-51-60\" ## [7] \"https://guides.loc.gov/federalist-papers/text-61-70\" ## [8] \"https://guides.loc.gov/federalist-papers/text-71-80\" ## [9] \"https://guides.loc.gov/federalist-papers/text-81-85\" read_write_html <- function(url) {   Sys.sleep(1) # 1 second delay   # ... } read_write_html <- function(url) {   Sys.sleep(1) # 1 second delay   message(\"Processing \", url) # Prints: \"Processing https://www.example.com\"   # ... } data/ |-- analysis/ |-- derived/ └── original/     └── federalist_papers/         |-- main.html         |-- text-1-10.html         |-- text-11-20.html         |-- text-21-30.html         |-- text-31-40.html         |-- text-41-50.html         |-- text-51-60.html         |-- text-61-70.html         |-- text-71-80.html         └── text-81-85.html"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"orientation","dir":"Articles","previous_headings":"Curate data","what":"Orientation","title":"Web scraping with R","text":"provide example curation process using semi-structured data, work Federalist Papers data acquired web scrape Library Congress website (sec-acquire-data?). data stored series HTML files, seen (exm-cd-federalist-data-files?). data origin file, fed_papers_do.csv (tbl-cd-fed-data-origin?), gives us overview data. file structure data origin description, can surmise working HTML files contain 85 Federalist Papers. 85 papers grouped 9 files, meaning file multiple papers contained within. can also see HTML files named according range papers contained within file. example, text-1-10.html file contains first 10 papers. also good idea inspect data files . Since HTML files, can open web browser. (fig-cd-federalist-html?) shows text-1-10.html file opened web browser. Figure 5: text-1-10.html file opened web browser. point want think curated dataset look like terms rows columns. columns, helpful think variables can extract Federalist Papers. example, can extract paper number, paper title, paper's author(s), venue paper published, paper's text. variables, paper number, title, author(s) metadata paper, venue metadata publication paper, leave venue curated dataset. rows, can think unit analysis . want conduct text analysis Federalist Papers predict author paper based features text, unit analysis paper. Now, can envision case row paper, may case structure papers, namely paragraphs, use us. keep mind work curation process. information mind, idealized version curated dataset shown (tbl-cd-fed-data-idealized?).","code":"data/ ├── analysis/ ├── derived/ └── original/     │── fed_papers_do.csv     └── federalist_papers/         ├── main.html         ├── text-1-10.html         ├── text-11-20.html         ├── text-21-30.html         ├── text-31-40.html         ├── text-41-50.html         ├── text-51-60.html         ├── text-61-70.html         ├── text-71-80.html         └── text-81-85.html"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"tidy-the-data","dir":"Articles","previous_headings":"Curate data","what":"Tidy the data","title":"Web scraping with R","text":"idealized dataset structure guide work. extract data metadata files need take closer look structure HTML documents. start HTML file opened browser (fig-cd-federalist-html?) look HTML source code browser's inspect tool. (fig-cd-federalist-html-inspect?) shows HTML source code first paper text-1-10.html file. Figure 6: HTML source code first paper text-1-10.html file. structure HTML files suggests desired content within div tag, forms kind box around paper. multiple div tags file, need find way identify div tag contains desired content, content. can see div tag want two class attributes, s-lib-box s-lib-box-std. div tag contains h2 tag paper number appears. first p tag contains title paper. second p tag contains venue paper. third p tag contains author paper. remaining p tags contain text paper. closer view div tag shown (fig-cd-federalist-html-inspect-2?). Figure 7: closer view div tag containing first paper text-1-10.html file. read text-1-10.html file use testing ground extracting relevant information. Load rvest package read file read_html(), (exm-cd-federalist-html?). Given discovered HTML inspection, extract div tags s-lib-box s-lib-box-std class attributes. can use html_elements() function extract div tags append .s-lib-box.s-lib-box-std html_elements() function specify class attributes. (exm-cd-federalist-html-div?) shows result assigned object called fed_divs. Using html_attr(\"class\") function can see fed_divs object file contains 11 div tags, div tags want, one also includes s-lib-floating-box class. can exclude adding :(.s-lib-floating-box) CSS selector. worth checking one two HTML files see consistent pattern. inspection HTML files data, turns first HTML file .s-lib-floating-box class. CSS solution might way go. alternative, R-side solution use div.s-lib-box, , subset fed_divs vector exclude first element, extract div tag located. (exm-cd-federalist-html-div-subset?) shows result assigned object called fed_divs. Assuming moment solution (exm-cd-federalist-html-div-subset?) way go, can move forward extract paper number, title, author, text div tag fed_divs. can use html_elements() function extract h2 tag, contains paper number, later work p tags, contain title, author, text. already isolated relevant div tags, using fed_divs object can now continue use html_elements() function extract HTML elements within. paper number h2 tag, immediately div tag. can use html_element() function extract single h2 tag div fed_divs, (exm-cd-federalist-html-h2?). result vector contains h2 tag div tag fed_divs, complete HTML tags attributes. can use html_text() function extract text h2 tag, (exm-cd-federalist-html-h2-text?). included str_trim() function stringr package remove potential whitespace text code (exm-cd-federalist-html-h2-text?). result vector paper numbers, can later leverage create new column dataset. Next, can extract p tags fed_divs object. p tags contain title, author, text. noted , first p tag inside div tag contains paper title. Targeting p tag requires use CSS selector, :nth-child(). CSS selector allows us arbitrarily select p tag want order appears. case, want first p tag, can use :nth-child(1). (exm-cd-federalist-html-p-title?) shows result extracting text assigned object called titles. {{< fa medal >}} Dive deeper CSS selectors powerful tool extracting data HTML files. :nth-child() selector just one many. information CSS selectors, see W3Schools CSS Selector Reference. rvest package supports many, CSS selectors. Consult rvest::html_elements() documentation information. get vector length 13, 10. Scanning output can see likely offender 'PUBLIUS.' text. can exclude adding :(:contains(\"PUBLIUS.\")) CSS selector, (exm-cd-federalist-html-p-title-subset?). works, solution 'brittle', meaning potentially overspecific easily break. example, text title happens include 'PUBLIUS.' excluded. Furthermore, extra p tag contains text 'PUBLIUS.' included. can make solution robust looking general solution. One possibility anchor p tags div tag .clearfix class appears directly (>) p tags want, (exm-cd-federalist-html-p-title-subset-2?). Now 10 titles. Moving author, assume use approach changing nth-child argument 3. However, inspecting HTML reveals author always third p tag, sometimes second. consistent, however, author always preceded text 'Author:'. can use :contains() CSS selector select p tag contains text 'Author:'. result (exm-cd-federalist-html-p-author?), now able extract paper number, title author. dataset taking shape, can appreciate (tbl-cd-federalist-html-dataset-preview?). last step extract text paper. contents papers contained p tags follow p tag author. nice able target p tags follow author p tag. However, CSS selector allows us . alternative read p tags div.clearfix tag select p tags follow author p tag. approach requires additional step. need conduct process paper HTML file separate str_which() function identify index p tag contains author. can use str_subset() function select p tags follow author p tag. gets text single paper within single div tag. can wrap function apply div tag. Now can see text looks like first paper. Putting together first HTML file get following. can test first HTML file. try fifth HTML file, just make sure. Now can apply function HTML files. can data checks make sure right number rows columns.","code":"## {html_document} ## <html lang=\"en\"> ## [1] <head>\\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\">\\n<meta http ... ## [2] <body class=\"s-lg-guide-body\">\\r\\n<a id=\"s-lg-public-skiplink\" class=\"ale ... ##  [1] \"s-lib-box s-lib-box-std s-lib-floating-box\" ##  [2] \"s-lib-box s-lib-box-std\"                    ##  [3] \"s-lib-box s-lib-box-std\"                    ##  [4] \"s-lib-box s-lib-box-std\"                    ##  [5] \"s-lib-box s-lib-box-std\"                    ##  [6] \"s-lib-box s-lib-box-std\"                    ##  [7] \"s-lib-box s-lib-box-std\"                    ##  [8] \"s-lib-box s-lib-box-std\"                    ##  [9] \"s-lib-box s-lib-box-std\"                    ## [10] \"s-lib-box s-lib-box-std\"                    ## [11] \"s-lib-box s-lib-box-std\" ## {xml_nodeset (10)} ##  [1] <h2 class=\"s-lib-box-title\">Federalist No. 1\\n                           ... ##  [2] <h2 class=\"s-lib-box-title\">Federalist No. 2\\n                           ... ##  [3] <h2 class=\"s-lib-box-title\">Federalist No. 3\\n                           ... ##  [4] <h2 class=\"s-lib-box-title\">Federalist No. 4\\n                           ... ##  [5] <h2 class=\"s-lib-box-title\">Federalist No. 5\\n                           ... ##  [6] <h2 class=\"s-lib-box-title\">Federalist No. 6\\n                           ... ##  [7] <h2 class=\"s-lib-box-title\">Federalist No. 7\\n                           ... ##  [8] <h2 class=\"s-lib-box-title\">Federalist No. 8\\n                           ... ##  [9] <h2 class=\"s-lib-box-title\">Federalist No. 9\\n                           ... ## [10] <h2 class=\"s-lib-box-title\">Federalist No. 10\\n                          ... ##  [1] \"Federalist No. 1\"  \"Federalist No. 2\"  \"Federalist No. 3\"  ##  [4] \"Federalist No. 4\"  \"Federalist No. 5\"  \"Federalist No. 6\"  ##  [7] \"Federalist No. 7\"  \"Federalist No. 8\"  \"Federalist No. 9\"  ## [10] \"Federalist No. 10\" ##  [1] \"General Introduction\"                                                                           ##  [2] \"Concerning Dangers from Foreign Force and Influence\"                                            ##  [3] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [4] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [5] \"The Same Subject Continued: Concerning Dangers from Foreign Force and Influence\"                ##  [6] \"Concerning Dangers from Dissensions Between the States\"                                         ##  [7] \"The Same Subject Continued: Concerning Dangers from Dissensions Between the States\"             ##  [8] \"PUBLIUS.\"                                                                                       ##  [9] \"The Consequences of Hostilities Between the States\"                                             ## [10] \"PUBLIUS.\"                                                                                       ## [11] \"The Utility of the Union as a Safeguard Against Domestic Faction and Insurrection\"              ## [12] \"PUBLIUS.\"                                                                                       ## [13] \"The Same Subject Continued: The Union as a Safeguard Against Domestic Faction and Insurrection\" ##  [1] \"General Introduction\"                                                                           ##  [2] \"Concerning Dangers from Foreign Force and Influence\"                                            ##  [3] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [4] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [5] \"The Same Subject Continued: Concerning Dangers from Foreign Force and Influence\"                ##  [6] \"Concerning Dangers from Dissensions Between the States\"                                         ##  [7] \"The Same Subject Continued: Concerning Dangers from Dissensions Between the States\"             ##  [8] \"The Consequences of Hostilities Between the States\"                                             ##  [9] \"The Utility of the Union as a Safeguard Against Domestic Faction and Insurrection\"              ## [10] \"The Same Subject Continued: The Union as a Safeguard Against Domestic Faction and Insurrection\" ##  [1] \"General Introduction\"                                                                           ##  [2] \"Concerning Dangers from Foreign Force and Influence\"                                            ##  [3] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [4] \"The Same Subject Continued: Concerning Dangers From Foreign Force and Influence\"                ##  [5] \"The Same Subject Continued: Concerning Dangers from Foreign Force and Influence\"                ##  [6] \"Concerning Dangers from Dissensions Between the States\"                                         ##  [7] \"The Same Subject Continued: Concerning Dangers from Dissensions Between the States\"             ##  [8] \"The Consequences of Hostilities Between the States\"                                             ##  [9] \"The Utility of the Union as a Safeguard Against Domestic Faction and Insurrection\"              ## [10] \"The Same Subject Continued: The Union as a Safeguard Against Domestic Faction and Insurrection\" ##  [1] \"Author: Alexander Hamilton\" \"Author: John Jay\"           ##  [3] \"Author: John Jay\"           \"Author: John Jay\"           ##  [5] \"Author: John Jay\"           \"Author: Alexander Hamilton\" ##  [7] \"Author: Alexander Hamilton\" \"Author: Alexander Hamilton\" ##  [9] \"Author: Alexander Hamilton\" \"Author: James Madison\" ## [1] \"To the People of the State of New York:\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ## [2] \"AFTER an unequivocal experience of the inefficiency of the subsisting federal government, you are called upon to deliberate on a new Constitution for the United States of America. The subject speaks its own importance; comprehending in its consequences nothing less than the existence of the UNION, the safety and welfare of the parts of which it is composed, the fate of an empire in many respects the most interesting in the world. It has been frequently remarked that it seems to have been reserved to the people of this country, by their conduct and example, to decide the important question, whether societies of men are really capable or not of establishing good government from reflection and choice, or whether they are forever destined to depend for their political constitutions on accident and force. If there be any truth in the remark, the crisis at which we are arrived may with propriety be regarded as the era in which that decision is to be made; and a wrong election of the part we shall act may, in this view, deserve to be considered as the general misfortune of mankind.\" ## # A tibble: 163 × 4 ##    number           title                author                     text         ##    <chr>            <chr>                <chr>                      <chr>        ##  1 Federalist No. 1 General Introduction Author: Alexander Hamilton To the Peop… ##  2 Federalist No. 1 General Introduction Author: Alexander Hamilton AFTER an un… ##  3 Federalist No. 1 General Introduction Author: Alexander Hamilton This idea w… ##  4 Federalist No. 1 General Introduction Author: Alexander Hamilton Among the m… ##  5 Federalist No. 1 General Introduction Author: Alexander Hamilton It is not, … ##  6 Federalist No. 1 General Introduction Author: Alexander Hamilton And yet, ho… ##  7 Federalist No. 1 General Introduction Author: Alexander Hamilton In the cour… ##  8 Federalist No. 1 General Introduction Author: Alexander Hamilton I propose, … ##  9 Federalist No. 1 General Introduction Author: Alexander Hamilton THE UTILITY… ## 10 Federalist No. 1 General Introduction Author: Alexander Hamilton In the prog… ## # ℹ 153 more rows ## Rows: 163 ## Columns: 4 ## $ number <chr> \"Federalist No. 1\", \"Federalist No. 1\", \"Federalist No. 1\", \"Fe… ## $ title  <chr> \"General Introduction\", \"General Introduction\", \"General Introd… ## $ author <chr> \"Author: Alexander Hamilton\", \"Author: Alexander Hamilton\", \"Au… ## $ text   <chr> \"To the People of the State of New York:\", \"AFTER an unequivoca… ## Rows: 56 ## Columns: 4 ## $ number <chr> \"Federalist No. 51\", \"Federalist No. 51\", \"Federalist No. 52\", … ## $ title  <chr> \"The Structure of the Government Must Furnish the Proper Checks… ## $ author <chr> \"Author: Alexander Hamilton or James Madison\", \"Author: Alexand… ## $ text   <chr> \"To the People of the State of New York:\", \"TO WHAT expedient, … ## Rows: 1,073 ## Columns: 4 ## $ number <chr> \"Federalist No. 1\", \"Federalist No. 1\", \"Federalist No. 1\", \"Fe… ## $ title  <chr> \"General Introduction\", \"General Introduction\", \"General Introd… ## $ author <chr> \"Author: Alexander Hamilton\", \"Author: Alexander Hamilton\", \"Au… ## $ text   <chr> \"To the People of the State of New York:\", \"AFTER an unequivoca… ## # A tibble: 7 × 2 ##   author                                       author_count ##   <chr>                                               <int> ## 1 Author: Alexander Hamilton                            659 ## 2 Author: Alexander Hamilton and James Madison           25 ## 3 Author: Alexander Hamilton or James Madison            93 ## 4 Author: James Madison                                 147 ## 5 Author: John Jay                                       81 ## 6 Author: Alexander Hamilton                             27 ## 7 Author: Alexander Hamilton and James Madison           41 ## # A tibble: 85 × 2 ##    number            number_count ##    <chr>                    <int> ##  1 Federalist No. 1            11 ##  2 Federalist No. 10           24 ##  3 Federalist No. 11           15 ##  4 Federalist No. 12           13 ##  5 Federalist No. 13            5 ##  6 Federalist No. 14           13 ##  7 Federalist No. 15           16 ##  8 Federalist No. 16           12 ##  9 Federalist No. 17           15 ## 10 Federalist No. 18           21 ## 11 Federalist No. 19           20 ## 12 Federalist No. 2            15 ## 13 Federalist No. 20           25 ## 14 Federalist No. 21           14 ## 15 Federalist No. 22           19 ## 16 Federalist No. 23           13 ## 17 Federalist No. 24           14 ## 18 Federalist No. 25           11 ## 19 Federalist No. 26           15 ## 20 Federalist No. 27            7 ## 21 Federalist No. 28           11 ## 22 Federalist No. 29           14 ## 23 Federalist No. 3            19 ## 24 Federalist No. 30           12 ## 25 Federalist No. 31           13 ## 26 Federalist No. 32            6 ## 27 Federalist No. 33            9 ## 28 Federalist No. 34           12 ## 29 Federalist No. 35           12 ## 30 Federalist No. 36           18 ## 31 Federalist No. 37           17 ## 32 Federalist No. 38           12 ## 33 Federalist No. 39           17 ## 34 Federalist No. 4            18 ## 35 Federalist No. 40            3 ## 36 Federalist No. 41            6 ## 37 Federalist No. 42            7 ## 38 Federalist No. 43            7 ## 39 Federalist No. 44            7 ## 40 Federalist No. 45            8 ## 41 Federalist No. 46            9 ## 42 Federalist No. 47            8 ## 43 Federalist No. 48            7 ## 44 Federalist No. 49            5 ## 45 Federalist No. 5            13 ## 46 Federalist No. 50            7 ## 47 Federalist No. 51            2 ## 48 Federalist No. 52            6 ## 49 Federalist No. 53            4 ## 50 Federalist No. 54            5 ## 51 Federalist No. 55            9 ## 52 Federalist No. 56            4 ## 53 Federalist No. 57            9 ## 54 Federalist No. 58            2 ## 55 Federalist No. 59            2 ## 56 Federalist No. 6            20 ## 57 Federalist No. 60           13 ## 58 Federalist No. 61            7 ## 59 Federalist No. 62           20 ## 60 Federalist No. 63           22 ## 61 Federalist No. 64           16 ## 62 Federalist No. 65           12 ## 63 Federalist No. 66           15 ## 64 Federalist No. 67           12 ## 65 Federalist No. 68           11 ## 66 Federalist No. 69           12 ## 67 Federalist No. 7            11 ## 68 Federalist No. 70           25 ## 69 Federalist No. 71            8 ## 70 Federalist No. 72           15 ## 71 Federalist No. 73           16 ## 72 Federalist No. 74            5 ## 73 Federalist No. 75            9 ## 74 Federalist No. 76           11 ## 75 Federalist No. 77           11 ## 76 Federalist No. 78           22 ## 77 Federalist No. 79            6 ## 78 Federalist No. 8            14 ## 79 Federalist No. 80           22 ## 80 Federalist No. 81           20 ## 81 Federalist No. 82            7 ## 82 Federalist No. 83           38 ## 83 Federalist No. 84           22 ## 84 Federalist No. 85           15 ## 85 Federalist No. 9            18"},{"path":"https://qtalr.github.io/qtalrkit/articles/guide-5.html","id":"write-the-data","dir":"Articles","previous_headings":"Curate data","what":"Write the data","title":"Web scraping with R","text":"...","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"course-design","dir":"Articles","previous_headings":"","what":"Course Design","title":"Instructor Guide","text":"book designed used textbook course quantitative text analysis. intended readers little experience quantitative text analysis, R programming language. Depending experience level expectations readers, however, may want consider adopting one following course designs using textbook.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"sec-p-basic-intro","dir":"Articles","previous_headings":"Course Design","what":"Basic Introduction","title":"Instructor Guide","text":"Cover chapters 1-5 sequence give readers foundational understanding quantitative text analysis. Culminate course research proposal assignment requires identify interesting linguistic problem, propose ways solving using methods covered class, identify potential data sources. readers little experience R, may want consider using RStudio Cloud platform host course. provide pre-installed R environment allow focus learning material rather troubleshooting.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"sec-p-intermediate-intro","dir":"Articles","previous_headings":"Course Design","what":"Intermediate Introduction","title":"Instructor Guide","text":"Cover chapters 1, 5-10 sequence give readers deeper understanding quantitative text analysis methods. Explore additional case studies dataset examples throughout course wish supplement lectures. Culminate course research project assignment allows readers apply learned linguistic content choice. may consider using RStudio Cloud platform host course, ensure readers access R RStudio computers well.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"sec-p-advanced-intro","dir":"Articles","previous_headings":"Course Design","what":"Advanced Introduction","title":"Instructor Guide","text":"Cover 12 chapters give readers thorough understanding quantitative text analysis concepts techniques. Devote time chapters 5-10 providing demonstrations approach different problems evaluating alternative approaches. Culminate course collaborative research project requires readers work groups conduct comprehensive analysis given dataset. Ensure readers install R RStudio computers need full control coding environment. course designs, strongly recommend evaluate readers' success understanding material providing combination quizzes, lab assignments, programming exercises, written reports. Additionally, encourage readers ask questions1, collaborate peers, seek help ample resources available online encounter scope-limited programming problems.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"slides-decks","dir":"Articles","previous_headings":"Instructor Resources","what":"Slides decks","title":"Instructor Guide","text":"Introduction Text Analysis Context Understanding Data Approaching Analysis Framing Research Data collection Data Curation Data Transformation Exploratory Data Analysis (EDA) Predictive Data Analysis (PDA) Inferential Data Analysis (IDA) Reporting Collaboration","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"lab-exercises","dir":"Articles","previous_headings":"Instructor Resources","what":"Lab exercises","title":"Instructor Guide","text":"following lab exercises available use course. exercise designed completed 50-minute lab session. ....","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/instructor-guide.html","id":"data-sets","dir":"Articles","previous_headings":"Instructor Resources","what":"Data sets","title":"Instructor Guide","text":"relatively openly available datasets allowed shared directly others. include: Project Gutenberg texts Enron Email Dataset languageR package datasets. quanteda package datasets. data sets, may able obtain permission share students course. Please contact data provider directly inquire sharing permissions. process obtaining permission share data can time consuming, recommended begin process well advance start course. resources, however, need instruct students download data . ...","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/qtalrkit.html","id":"installation","dir":"Articles","previous_headings":"qtalrkit package","what":"Installation","title":"Getting started","text":"can install development version qtalrkit GitHub :","code":"install.packages(\"remotes\") library(remotes) install_github(\"qtalr/qtalrkit\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/qtalrkit.html","id":"load","dir":"Articles","previous_headings":"qtalrkit package","what":"Load","title":"Getting started","text":"load package :","code":"library(qtalrkit)"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/qtalrkit.html","id":"installation-1","dir":"Articles","previous_headings":"swirl lessons","what":"Installation","title":"Getting started","text":"swirl lessons can downloaded within R console running: Updating lessons need update lessons, run:","code":"install.packages(\"swirl\") library(\"swirl\") install_course_github(\"qtalr\", \"lessons\") library(\"swirl\")  # Uninstall the course uninstall_course(\"lessons\")  # Reinstall the course install_course_github(\"qtalr\", \"lessons\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/qtalrkit.html","id":"load-and-run","dir":"Articles","previous_headings":"swirl lessons","what":"Load and run","title":"Getting started","text":"load start lesson run: follow instructions get started select lesson.","code":"swirl()"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"0. Writing with code","text":"recipe, introduce concept Literate Programming describe implement concept Quarto. provide demonstration features Quarto describe main structural characteristics Quarto document help get running writing documents combine code prose.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"literate-programming","dir":"Articles","previous_headings":"Concepts and strategies","what":"Literate Programming","title":"0. Writing with code","text":"First introduced Donald Knuth (1984), aim Literate Programming able combine computer code text prose one document. allows analyst run code, view output code, view code , provide prose description one document. way, literate programming document allows presenting analysis way performs computing steps desired presents easily readable format. Literate programming now key component creating sharing reproducible research (Gandrud 2015).","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"quarto","dir":"Articles","previous_headings":"Concepts and strategies","what":"Quarto","title":"0. Writing with code","text":"Quarto specific implementation literate programming paradigm. Figure 1 see example Quarto action. left see Quarto source code, combination text code. right see output Quarto source code HTML document. Figure 1: Quarto source (left) output (right) example. Quarto documents generate various types output: web documents (HTML), PDFs, Word documents, many types output formats based source code. interleaving code prose create variety output documents one attractive aspects literate programming Quarto, also possible create documents code . versatile technology come appreciate. Dive deeper see Quarto action, please check Quarto Gallery variety examples Quarto documents output. Quarto source document plain-text file extension .qmd can opened plain text reader. using RStudio IDE (henceforth RStudio) create, open, edit, generate output .qmd files plain-text reader, TextEdit (MacOS) Notepad (PC) can open files. mind, now move anatomy Quarto document.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"anatomy-of-a-quarto-document","dir":"Articles","previous_headings":"Concepts and strategies > Quarto","what":"Anatomy of a Quarto Document","title":"0. Writing with code","text":"basic level Quarto document contains two components: front-matter section prose section. third component, code block, can interleaved within prose section add code document. look turn.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"front-matter","dir":"Articles","previous_headings":"Concepts and strategies > Quarto > Anatomy of a Quarto Document","what":"Front-matter","title":"0. Writing with code","text":"front matter Quarto document appears, well, front document (top, rather). Referring back Figure 1 see front matter top. creating Quarto document RStudio default attribute keys title, author, format. front matter fenced three dashes ---. values first two keys pretty straightforward can edited needed. value format attribute can also edited tell .qmd file generate output types. Can guess value might use generate PDF document? Yep, just pdf. work Quarto learn use RStudio interface change key-value pairs add others!","code":"--- title: \"Introduction to Quarto\" author: \"Jerid Francom\" format: html ---"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"prose","dir":"Articles","previous_headings":"Concepts and strategies > Quarto > Anatomy of a Quarto Document","what":"Prose","title":"0. Writing with code","text":"Anywhere front matter contained within code block (see ) open prose. prose section(s) added functionality Markdown aware. mean, say? Well, Markdown refers set plain-text formatting conventions produce formatted text output document. quote Wikipedia: Markdown lightweight markup language creating formatted text using plain-text editor. John Gruber Aaron Swartz created Markdown 2004 markup language appealing human readers source code form. Markdown widely used blogging, instant messaging, online forums, collaborative software, documentation pages, readme files. enables us add simple text conventions signal output formatted. Say want make text bold. just add ** around text want appear bold. can also : italics *italics* links [links](http://wfu.edu) strikethrough ~~strikethrough~~ etc. Follow link find information basic Markdown syntax.","code":"**bold text**"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"code-blocks","dir":"Articles","previous_headings":"Concepts and strategies > Quarto > Anatomy of a Quarto Document","what":"Code blocks","title":"0. Writing with code","text":"Code blocks R magic happens. , referring Figure 1 see following code block. code block bound three backticks ```. first backticks curly brackets {} allow us tell Quarto programming language use evaluate (.e. run) code block. cases R, hence opening curly bracket `{r}`. languages can used Quarto, Python, SQL, Bash. previous example, R used simple calculator adding 1 + 1. code block produces. good practice label code blocks. case `#| label: add`. line code entered. mentioned selecting coding language labeling code block, code blocks various options can used determine code block used. common code block options : hiding code: #| echo: false hiding output #| include: false etc.","code":"```{r} 1 + 1 ``` 1 + 1 ## [1] 2 ```{r} #| label: add 1 + 1 ``` ```{r} #| label: add #| echo: false 1 + 1 ``` ## [1] 2 ```{r} #| label: add #| include: false 1 + 1 ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"create-and-render-a-quarto-document","dir":"Articles","previous_headings":"Concepts and strategies > Quarto","what":"Create and render a Quarto document","title":"0. Writing with code","text":"easiest efficient way create Quarto source file use RStudio point--click interface. Just use toolbar create new file select \"Quarto Document...\", seen Figure 2. Figure 2: Creating new Quarto document RStudio. provide dialogue box asking add title author document also allows select type document format output, seen Figure 3. Figure 3: Dialogue box creating new Quarto document RStudio. Enter title author leave format set HTML. clicking 'Create' get Quarto document, Figure 4, default/ boilerplate prose code blocks. prose code blocks can deleted, can start document. Figure 4: Quarto source RStudio. now, leave things see generate output report document. Click \"Render\" RStudio toolbar. render, asked save file give name. done .qmd file render format specified open 'Viewer' pane, seen Figure 5. Figure 5: Quarto source HTML output side--side RStudio. Dive deeper Watch Getting Started Quarto guided tour Quarto (Çetinkaya-Rundel 2023).","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"0. Writing with code","text":"TRUEFALSE Literate Programming, first introduced Donald Knuth 1984, allows combination computer code text prose one document. programming paradigm Literate Programming implemented QuartoRRStudioGitHub, platform facilitates creation variety output documents based source code. following components basic Quarto document contain? Front-matter sectionProse sectionBack-matter sectionCode block generate PDF document Quarto, can edit format attribute value front-matter section . TRUEFALSE code block options echo include can used hide code output, respectively. TRUEFALSE Quarto, code block, programming language code entered, bounded three underscores (_).","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-0.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"0. Writing with code","text":"concludes introduction literate programming using Quarto. covered basics much explore. preparation Lab 0, ensure completed following: Setup computing environment R RStudio quarto tinytex prepared following: Open RStudio understand basic interface Create, edit, render Quarto documents Use basic Markdown syntax format text first lab, provide guidance access lab materials. lab materials hosted GitHub. GitHub platform hosting code files. access lab materials, need create GitHub account, may sign free account wish1. , however, need verify Git installed computer, , install . start Git. Windows machine, likely need install Git. Download install Git. Go \"Assests\" section page click link latest .exe version. download executable file. Run executable file follow instructions install Git. Mac, first verify Git installed. Open Terminal application (can search using Spotlight). Terminal window, type git --version press enter. Git installed, see message says something like git version 2.43.0. see message, need install Git. Git installed, need clone lab repository. Cloning repository means download files repository computer. clone lab repository, follow steps: navigate lab repository GitHub identify blue button says \"Code\". Make sure dropdown menu selected \"Clone\" \"HTTPS\". Copy URL appears box. Open RStudio select \"New Project...\" \"File\" menu. Select \"Version Control\" \"Git\". Paste URL copied GitHub \"Repository URL\" box. Select location computer like save lab materials. Click \"Create Project\". process create folder computer lab materials. able open edit lab materials RStudio. mind, ready move Lab 0.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"1. Academic writing with Quarto","text":"implementation literate programming using course Quarto R. seen previously, Quarto provides ability combine prose code single document. powerful strategy creating reproducible documents can easily updated shared. common type writing academia research paper. recipe explore use Quarto include common elements found research papers. include: Numbered sections Table contents Cross-referencing tables figures -line citations references list Lab 1 opportunity practice concepts article summary includes features using Quarto.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"concepts-and-strategies","dir":"Articles","previous_headings":"","what":"Concepts and strategies","title":"1. Academic writing with Quarto","text":"many style components use Quarto, part addressed front-matter section part addressed prose section / code block sections. refresh memory, front-matter fenced three dashes (---) set document attributes. prose section write text document. code block section write code executed fenced three backticks (```) name code interpreter {r} (R us). mind look elements turn.","code":"--- title: \"My document title\" format: pdf ---  This is the prose section.  ```{r} #| label: example-code-block 1 + 1 ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"numbered-sections","dir":"Articles","previous_headings":"Concepts and strategies","what":"Numbered sections","title":"1. Academic writing with Quarto","text":"number sections Quarto, use number_sections key value yes. set front-matter section, nested value document type rendered. example, number sections PDF document, set number-sections key true front-matter section follows: Headers prose section numbered automatically. example, following markdown: render :  can also control depth numbering setting number-depth key front-matter section. example, number sections subsections, subsubsections, set number-depth key 2 follows: Now first second headers numbered formated third subsequent headers formatted. reason want turn numbering specific header, can add {.unnumbered} end header. example, following markdown: particularly useful academic writing want add reference, materials, section numbered end document. Warning Note header unnumbered, next header numbered unnumbered header exist. can unexpected results children unnumbered header.","code":"--- title: \"My document title\" format:   pdf:     number-sections: true --- # Section  ## Subsection  ### Subsubsection  #### Subsubsubsection  ##### Subsubsubsubsection --- title: \"My document title\" format:   pdf:     number-sections: true     number-depth: 2 --- # Section {.unnumbered}"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"table-of-contents","dir":"Articles","previous_headings":"Concepts and strategies","what":"Table of contents","title":"1. Academic writing with Quarto","text":"longer documents including table contents can useful way help readers navigate document. include table contents Quarto, use toc key value true. , front-matter section, nested format value, seen : Tip PDF Word document outputs, table contents automatically generated placed beginning document. HTML documents, table contents placed sidebar default. headers numbered, appeared numbered table contents. unnnumbered header, appear section number. section numbering, can also control depth table contents setting toc-depth key front-matter section. example, include sections subsections, subsubsections, set toc-depth key 2 follows: section numbering can avoid listing header table contents adding {.unlisted} end header.","code":"--- title: \"My document title\" format:   pdf:     toc: true --- --- title: \"My document title\" format:   pdf:     toc: true     toc-depth: 2 ---"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"cross-referencing-tables-and-figures","dir":"Articles","previous_headings":"Concepts and strategies","what":"Cross-referencing tables and figures","title":"1. Academic writing with Quarto","text":"Another key element academic writing using cross-references tables figures. allows us refer table figure number without manually update number add remove table figure. case, need add anything front-matter section. Instead, modify keys code block section code-generated table figure. cross-reference table figure, need add prefix label key's value. prefix, either tbl- fig-, indicates whether label table figure. Additionally, table figure captions can added tbl-cap fig-cap keys, respectively. look basic figure can cross-reference. following code block generate simple scatterplot. Figure 1: scatterplot Figure 1 see scatterplot. .... tables generated R, process similar figures. difference use tbl- prefix label value tbl-cap key instead fig-cap key caption. can also create tables using markdown syntax. case, format little different. Consider Table 1, example. Table 1:  simple table","code":"```{r} #| label: fig-scatterplot #| fig-cap: \"A scatterplot\"  plot(x = 1:10, y = 1:10) ```  In @fig-scatterplot we see a scatterplot. .... | Column 1 | Column 2 | Column 3 | |----------|----------|----------| | A        | B        | C        | | D        | E        | F        |  : A simple table {#tbl-table-1}"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"in-line-citations-and-references-list","dir":"Articles","previous_headings":"Concepts and strategies","what":"In-line citations and references list","title":"1. Academic writing with Quarto","text":"last element cover adding citations references list Quarto document. add citations need three things: bibliography file reference bibliography file front-matter section citation prose section contained bibliography file bibliography file plain text file contains citations want use document. file requires extension .bib formatted using BibTeX format. BibTeX reference syntax commonly used academia. take look sample file, bibliography.bib, contains single reference. file can see reference includes information type publication, title, author, year, journal, volume, number, pages, DOI. can find BibTeX formatted references almost everywhere can find scholarly work. example, Google Scholar, Web Science, Scopus provide BibTeX formatted references. Additionally, many journals provide BibTeX formatted references articles publish. Dive deeper Managing references can challenge begin amass large number . number tools can help manage references. example, Zotero free, open-source reference manager can help organize references generate BibTeX formatted references. Zotero also browser extension allows easily add references Zotero library browser. Furthermore, Zotero can connected RStudio facilitate incorporation BibTeX formatted references Quarto document. See RStudio documentation information. front-matter Quarto document, need add reference bibliography file. done using bibliography key. example, bibliography file called bibliography.bib located directory Quarto document, add following front-matter section: bibliography file reference bibliography file front-matter section, can now add citations document. , use @ symbol followed citation key prose section. example, cite tidyverse2019 reference bibliography.bib file, add @tidyverse2019 prose section follows: citation appear rendered document. citation Wickham et al. (2019). automatically, rendering document, references list added end document. reason citations document, good idea include header section # References end document. Tip number ways inline citations appear. example, parentheses, multiple citations, year, adding page number, etc.. information format citations, see Quarto documentation.","code":"@Article{tidyverse2019,   title = {Welcome to the {tidyverse}},   author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},   year = {2019},   journal = {Journal of Open Source Software},   volume = {4},   number = {43},   pages = {1686},   doi = {10.21105/joss.01686}, } --- title: \"My document title\" format: pdf bibliography: bibliography.bib --- This is a citation to @tidyverse2019."},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"1. Academic writing with Quarto","text":"Consider following front-matter sections, B. B Choose whether following statements true false. TRUEFALSE Section numbering included PDF output B. TRUEFALSE Section numbering applied first three levels headers PDF output B. TRUEFALSE table contents included PDF output B. TRUEFALSE table contents included PDF output B, include first two levels headers. Now respond following questions. @tbl-scatterplot@fig-scatterplot@scatterplot cross-reference figure label fig-scatterplot. front-matter key include path file contains BibTeX formatted references.","code":"--- title: \"My document title\" format:   pdf:     number-sections: true     number-depth: 3     toc: false --- --- title: \"My document title\" format:   pdf:     number-sections: true     toc: true     toc-depth: 2 ---"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-1.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"1. Academic writing with Quarto","text":"rounds introduction academic writing Quarto. Lab 1 opportunity practice concepts article summary includes features using Quarto. preparation Lab 1, ensure prepared following: PDF document Word document document numbered sections document table contents document path bibliography file Add inline citation prose section Quarto document Also, since article summary, prepared : research question data used methods used results/ findings study BibTeX formatted reference article","code":""},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-10.html","id":"concepts-and-stragegies","dir":"Articles","previous_headings":"","what":"Concepts and stragegies","title":"10. Testing generalizations","text":"Identify map hypothesis variables Inspect data distributions Interrogate data Interpret results","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"2. Reading, inspecting, and writing data","text":"Recipes guides process reading, inspecting, writing data using R packages functions Quarto environment. learn effectively combine code narrative create reproducible document can shared others. cover following topics: Loading packages R session Reading data R read_*() functions Inspecting data dplyr functions Writing data file write_*() functions Lab 2, able apply skills employ Quarto skills create well-documented reproducible document.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"quarto-documents-and-code-blocks","dir":"Articles","previous_headings":"Concepts and strategies","what":"Quarto documents and code blocks","title":"2. Reading, inspecting, and writing data","text":"Ask remember Lab 0 1, Quarto documents can combine prose code. prose written Markdown code written R1. code contained code blocks, opened three backticks (`), name programming language, r, curly braces {r} three backticks (`) close block. example, following minimal Quarto document contains R code block: Code blocks various options can added using key-value pairs prefixed #|. common key-value pairs use Recipe : label: unique name code block. used reference code block. echo: boolean value (true false) determines whether code displayed output document. include: boolean value (true false) determines whether output code displayed output document. message: boolean value (true false) determines whether messages code displayed output document.","code":"--- title: My Quarto Document format: pdf ---  # Goals  This script ...  ```{r} #| label: code-block-name  # R code goes here ```  As you can see in the code block, the ..."},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"setting-up-the-environment","dir":"Articles","previous_headings":"Concepts and strategies","what":"Setting up the environment","title":"2. Reading, inspecting, and writing data","text":"can read, inspect, write data, need load packages contain functions use. use readr package read data R write data disk dplyr package inspect transform (subset) data. ways load packages R session. common way use library() function. library() function loads package R session stops script package available current computing environment. example, following code block loads readr dplyr packages R session: code block assumes readr dplyr packages installed current computing environment. packages installed, code block stop display error message, : error can addressed installing missing package install.packages(\"readr\") re-running code block. ideal reproducibility, however, code block stop package installed. consider reproducible approach later course. Dive deeper interested learning safeguarding package loading reproducible way, see renv package. renv package project-oriented workflow create reproducible environment R projects. information, see renv documentation.","code":"```{r} #| label: load-packages  # Load packages library(readr) # for reading and writing data library(dplyr) # for inspecting and transforming data ``` Error in library(readr) : there is no package called ‘readr’"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"understanding-the-data","dir":"Articles","previous_headings":"Concepts and strategies","what":"Understanding the data","title":"2. Reading, inspecting, and writing data","text":"Now environment set , can read data R. , make sure understand data looking data documentation. dataset read R session based Brown Corpus (Francis Kuçera 1961). created data origin file contains data documentation Brown Corpus, can see Table 1. Table 1: Data origin file Brown Corpus. data origin file provides overview original data source. case, dataset read R subset Brown Corpus aggregate use passive voice. data developed authors corpora package (Evert 2023). exported data CSV file, read R. data dictionary describes dataset read appears Table 2. Table 2: Data dictionary file Brown Corpus. information, now position read inspect dataset.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"reading-data-into-r-with-readr","dir":"Articles","previous_headings":"Concepts and strategies","what":"Reading data into R with readr","title":"2. Reading, inspecting, and writing data","text":"now prepared Quarto document loading packages use reviewed data documentation understand data read R. now ready read data R. R provides number functions read data many types R. explore many types data datasets course. now, focus reading rectangular data R. Rectangular data data organized rows columns, spreadsheet. One common file formats rectangular data comma-separated values (CSV) file. CSV files text files lines represent rows commas separate columns data. example, sample CSV file snippet contains three rows three columns data: CSV file type delimited file, means data separated delimiter. case CSV file, delimiter comma. types delimited files use different delimiters, tab-separated values (TSV) files use tab character delimiter, even pipe (|) semicolon (;). readr package provides functions read rectangular data R. read_csv() function reads CSV files, read_tsv() function reads TSV files, read_delim() function reads types delimited files. use read_csv() function read brown_passives_curated.csv file R. use file = argument specify path file. Now, file \"path\" location file computer. can specify path two ways: Relative path: relative path path file relative current working directory. current working directory directory R session running. Absolute path: absolute path path file root directory computer. purpose, relative path better option portable. example, share code someone else, may different absolute path file. However, likely relative path file. say directory structure project follows: case, relative path reading-inspecting-writing.qmd brown_passives_curated.csv file ../data/derived/brown_passives_curated.csv. .. means \"go one directory\" rest path path file project/ directory. mind, can read brown_passives_curated.csv file R following code block: Running code chunk Quarto document read data R assign brown_passives_df variable. also show code used read data R. Furthermore, functions display messages output. example, read_csv() function display message various parsing options used read data R, seen : information can helpful interactive session, read_csv() tells us dimensions data data types column. output necessary, unnecessarily verbose reproducible document. can hide messages produced function using message = false key-value pair code block. example, following code block read data R assign brown_passives_df variable without displaying messages: messages displayed document output.","code":"\"word\",\"frequency\",\"part_of_speech\" \"the\",69971,\"article\" \"of\",36412,\"preposition\" \"and\",28853,\"conjunction\" project/ ├── data/ │   ├── original/ │   │   └── brown_passives_do.csv │   └── derived/ │       └── brown_passives_curated.csv └── scripts/     └── reading-inspecting-writing.qmd ```{r} #| label: read-dataset-brown-passives-curated  # Read the dataset brown_passives_df <-   read_csv(file = \"../data/derived/brown_passives_curated.csv\") ``` ## Rows: 15 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \",\" ## chr (2): cat, name ## dbl (3): passive, n_w, n_s ##  ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ```{r} #| label: read-dataset-brown-passives-curated #| message: false  # Read the dataset brown_passives_df <-   read_csv(file = \"../data/derived/brown_passives_curated.csv\") ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"inspecting-data-with-dplyr","dir":"Articles","previous_headings":"Concepts and strategies","what":"Inspecting data with dplyr","title":"2. Reading, inspecting, and writing data","text":"objective section demonstrate inspect transform (subset) data using dplyr package. use dplyr package inspect data read R previous section. Reading CSV file R create data frame object. Thus, assigned result brown_passives_df. df suffix common naming convention rectangular data frames. good practice use consistent naming convention objects code. makes easier understand code avoid errors. get overview data using glimpse() function dplyr package. glimpse() function displays dimensions data frame data types column. want , tabular-like view data, can simply print data frame console. worth mentioning, readr functions return tibbles, gain benefits tibbles read data R readr functions, one worry printing data frame console, document, print data. default, printing tibbles return first 10 rows columns, unless columns numerous display width-wise. dplyr also provides set slice_*() functions allow us display data tabular fashion, additional options. three slice_*() functions cover : slice_head(): Select first n rows data frame. slice_tail(): Select last n rows data frame. slice_sample(): Select random sample n rows data frame. example, following code block select first 5 rows data frame: can also select last 5 rows data frame slice_tail() function: Finally, can select random sample 5 rows data frame slice_sample() function: functions can helpful get sense data different ways. combination arrange() function, can also sort data frame column columns select first last rows. example, following code block sort data frame passive column ascending order select first 5 rows: want sort descending order, can surround column name desc(), arrange(desc(passive)). Now, previous code block want, readable. Enter pipe operator. pipe operator |> operator allows us chain output one function input another function. allows us write readable code. result code makes sense. can read code left right, top bottom, order functions executed. Dive deeper native R pipe |> introduced R 4.1.0. using earlier version R, can use magrittr package load pipe operator %>%. certain advantages using magrittr pipe operator, including ability use pipe operator pass arguments functions placeholders. information, see magrittr documentation. addition legible, using pipe function line allows us add comments line code. example, following code block previous code block, comments added line: good practice follow writing code. makes code readable easier understand others future self!","code":"# Preview glimpse(brown_passives_df) ## Rows: 15 ## Columns: 5 ## $ cat     <chr> \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"J\", \"K\", \"L\", \"M\", \"N… ## $ passive <dbl> 892, 543, 283, 351, 853, 1034, 1460, 837, 2423, 352, 265, 104,… ## $ n_w     <dbl> 101196, 61535, 40749, 39029, 82010, 110363, 173017, 69446, 181… ## $ n_s     <dbl> 3684, 2399, 1459, 1372, 3286, 4387, 6537, 2012, 6311, 3983, 36… ## $ name    <chr> \"press reportage\", \"press editorial\", \"press reviews\", \"religi… # Print the data frame brown_passives_df ## # A tibble: 15 × 5 ##    cat   passive    n_w   n_s name             ##    <chr>   <dbl>  <dbl> <dbl> <chr>            ##  1 A         892 101196  3684 press reportage  ##  2 B         543  61535  2399 press editorial  ##  3 C         283  40749  1459 press reviews    ##  4 D         351  39029  1372 religion         ##  5 E         853  82010  3286 skills / hobbies ##  6 F        1034 110363  4387 popular lore     ##  7 G        1460 173017  6537 belles lettres   ##  8 H         837  69446  2012 miscellaneous    ##  9 J        2423 181426  6311 learned          ## 10 K         352  68599  3983 general fiction  ## # ℹ 5 more rows # Select the first 5 rows slice_head(brown_passives_df, n = 5) ## # A tibble: 5 × 5 ##   cat   passive    n_w   n_s name             ##   <chr>   <dbl>  <dbl> <dbl> <chr>            ## 1 A         892 101196  3684 press reportage  ## 2 B         543  61535  2399 press editorial  ## 3 C         283  40749  1459 press reviews    ## 4 D         351  39029  1372 religion         ## 5 E         853  82010  3286 skills / hobbies # Select the last 5 rows slice_tail(brown_passives_df, n = 5) ## # A tibble: 5 × 5 ##   cat   passive   n_w   n_s name            ##   <chr>   <dbl> <dbl> <dbl> <chr>           ## 1 L         265 57624  3673 detective       ## 2 M         104 14433   873 science fiction ## 3 N         290 69909  4438 adventure       ## 4 P         290 70476  4187 romance         ## 5 R         146 21757   975 humour # Select a random sample of 5 rows slice_sample(brown_passives_df, n = 5) ## # A tibble: 5 × 5 ##   cat   passive    n_w   n_s name            ##   <chr>   <dbl>  <dbl> <dbl> <chr>           ## 1 L         265  57624  3673 detective       ## 2 F        1034 110363  4387 popular lore    ## 3 R         146  21757   975 humour          ## 4 H         837  69446  2012 miscellaneous   ## 5 M         104  14433   873 science fiction # Sort by the `passive` column and select the first 5 rows slice_head(arrange(brown_passives_df, passive), n = 5) ## # A tibble: 5 × 5 ##   cat   passive   n_w   n_s name            ##   <chr>   <dbl> <dbl> <dbl> <chr>           ## 1 M         104 14433   873 science fiction ## 2 R         146 21757   975 humour          ## 3 L         265 57624  3673 detective       ## 4 C         283 40749  1459 press reviews   ## 5 N         290 69909  4438 adventure # Sort by the `passive` column and select the first 5 rows brown_passives_df |>   arrange(passive) |>   slice_head(n = 5) ## # A tibble: 5 × 5 ##   cat   passive   n_w   n_s name            ##   <chr>   <dbl> <dbl> <dbl> <chr>           ## 1 M         104 14433   873 science fiction ## 2 R         146 21757   975 humour          ## 3 L         265 57624  3673 detective       ## 4 C         283 40749  1459 press reviews   ## 5 N         290 69909  4438 adventure # Sort by the passive column and select the first 5 rows brown_passives_df |> # Pass the data frame to the next function   arrange(passive) |> # Sort the data frame by the passive column   slice_head(n = 5) # Select the first 5 rows"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"subsetting-data-with-dplyr","dir":"Articles","previous_headings":"Concepts and strategies","what":"Subsetting data with dplyr","title":"2. Reading, inspecting, and writing data","text":"Now sense data, can subset data create variations original data frame. can subset data frame selecting columns / rows. R lesson \"Packages Functions\", saw base R provides bracket ([]) operator subset data frames. dplyr package provides functions subset data frames can readable easier use. first look selecting columns. select() function allows us select columns name. example, following code block select passive n_w columns data frame: Beyond selecting columns, can also reorder columns rename columns. example, following code block select passive n_w columns, rename n_w column num_words, reorder columns num_words first column: Dive deeper select() also provides number helper functions select columns. example, can use starts_with() function inside select() call select columns start certain string. can select columns vector type using (.character). information, see select() documentation use ?select command R console. selecting columns others, effectively dropped columns select. effective drop columns name, can use select() function - operator. example, following code block drop cat column data frame: now turn attention subsetting rows. filter() function allows us select rows logical condition. example, following code block select rows values passive column less < 1,000: can also use filter() function select rows character string. example, following code block select rows values name column equal religion: inequality operator != can used character strings well. include multiple values, can use %% operator. case can pass vector values filter() function. example, following code block select rows values name column equal religion learned: Dive deeper sophisticated subsetting, can use str_detect() function stringr package select rows values name column contain certain string. approach enhanced later course learn regular expressions.","code":"# Select the `passive` and `n_w` columns select(brown_passives_df, passive, n_w) ## # A tibble: 15 × 2 ##    passive    n_w ##      <dbl>  <dbl> ##  1     892 101196 ##  2     543  61535 ##  3     283  40749 ##  4     351  39029 ##  5     853  82010 ##  6    1034 110363 ##  7    1460 173017 ##  8     837  69446 ##  9    2423 181426 ## 10     352  68599 ## # ℹ 5 more rows # Select rename and reorder columns brown_passives_df |>   select(num_words = n_w, passive) ## # A tibble: 15 × 2 ##    num_words passive ##        <dbl>   <dbl> ##  1    101196     892 ##  2     61535     543 ##  3     40749     283 ##  4     39029     351 ##  5     82010     853 ##  6    110363    1034 ##  7    173017    1460 ##  8     69446     837 ##  9    181426    2423 ## 10     68599     352 ## # ℹ 5 more rows # Drop the `n_w` column brown_passives_df |>   select(-cat) ## # A tibble: 15 × 4 ##    passive    n_w   n_s name             ##      <dbl>  <dbl> <dbl> <chr>            ##  1     892 101196  3684 press reportage  ##  2     543  61535  2399 press editorial  ##  3     283  40749  1459 press reviews    ##  4     351  39029  1372 religion         ##  5     853  82010  3286 skills / hobbies ##  6    1034 110363  4387 popular lore     ##  7    1460 173017  6537 belles lettres   ##  8     837  69446  2012 miscellaneous    ##  9    2423 181426  6311 learned          ## 10     352  68599  3983 general fiction  ## # ℹ 5 more rows # Select rows where `passive` is less than 1,000 brown_passives_df |>   filter(passive < 1000) ## # A tibble: 12 × 5 ##    cat   passive    n_w   n_s name             ##    <chr>   <dbl>  <dbl> <dbl> <chr>            ##  1 A         892 101196  3684 press reportage  ##  2 B         543  61535  2399 press editorial  ##  3 C         283  40749  1459 press reviews    ##  4 D         351  39029  1372 religion         ##  5 E         853  82010  3286 skills / hobbies ##  6 H         837  69446  2012 miscellaneous    ##  7 K         352  68599  3983 general fiction  ##  8 L         265  57624  3673 detective        ##  9 M         104  14433   873 science fiction  ## 10 N         290  69909  4438 adventure        ## # ℹ 2 more rows # Select rows where `name` is equal to `religion` brown_passives_df |>   filter(name == \"religion\") ## # A tibble: 1 × 5 ##   cat   passive   n_w   n_s name     ##   <chr>   <dbl> <dbl> <dbl> <chr>    ## 1 D         351 39029  1372 religion # Select multiple values brown_passives_df |>   filter(name %in% c(\"religion\", \"learned\", \"detective\")) ## # A tibble: 3 × 5 ##   cat   passive    n_w   n_s name      ##   <chr>   <dbl>  <dbl> <dbl> <chr>     ## 1 D         351  39029  1372 religion  ## 2 J        2423 181426  6311 learned   ## 3 L         265  57624  3673 detective"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"writing-data-to-a-file-with-readr","dir":"Articles","previous_headings":"Concepts and strategies","what":"Writing data to a file with readr","title":"2. Reading, inspecting, and writing data","text":"Finally, can write data, including data frames, file write_*() functions readr package. write_*() functions include: write_csv(): Write data frame CSV file. write_tsv(): Write data frame TSV file. write_delim(): Write data frame delimited file specified delimiter (|, ;, etc). create distinct data frame one read R, subset brown_passives_df data frame columns rows create new data frame contains passive, n_w, name columns rows values passive column greater > 1,000 assign brown_passives_subset_df. Now following code block write brown_passives_subset_df data frame CSV file given specified file path: Given example directory structure saw earlier, new file appears data/derived/ directory. much learn reading, inspecting, writing data R. introduce functions techniques coming lessons. now, learned read, inspect, write data using R functions Quarto code blocks!","code":"# Subset the data frame brown_passives_subset_df <-   brown_passives_df |>   select(passive, n_w, name) |>   filter(passive > 1000) # Write the data frame to a CSV file write_csv(   x = brown_passives_subset_df,   file = \"../data/derived/brown_passives_subset.csv\" ) project/ ├── data/ │   ├── original/ │   │   └── brown_passives_do.csv │   └── derived/ │       ├── brown_passives_curated.csv │       ├── brown_passives_curated_dd.csv │       └── brown_passives_subset.csv └── scripts/     └── reading-inspecting-writing.qmd"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"2. Reading, inspecting, and writing data","text":"TRUEFALSE readr package provides functions read rectangular data R. echomessageinclude option code block determines whether code displayed output document. TRUEFALSE dplyr package provides functions create data dictionaries. read_csv()read_tsv()read_delim() used read tab-separated values (TSV) files. function dplyr used select columns name? select()filter()slice_head() TRUEFALSE R pipe operator |> allows us chain output one function input another function.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-2.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"2. Reading, inspecting, and writing data","text":"Lab 2 opportunity apply skills learned Recipe create Quarto document reads, inspects, writes data. addition knowledge skills developed Labs 0 1, complete Lab 2, need able : Create code blocks Quarto document Understand purpose label, echo, message, include options code block Load packages R session library() Understand read create file relative file paths Read data R read_csv() function Inspect data frames dplyr functions glimpse(), slice_head(), slice_tail(), slice_sample(), arrange(). Use |> pipe operator chain functions together. Subset data frames dplyr functions select() filter(). Write data frames file write_csv() function.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"3. Descriptive assessment of datasets","text":"Recipe explore appropriate methods summarizing variables datasets given number informational values variable(s). build understanding summarize data using statistics, tables, plots. cover following topics: Summary overviews datasets skimr Summary statistics dplyr Creating Quarto tables knitr Creating Quarto plots ggplot2 Lab 3, put skills practice provide descriptive assessment dataset includes statistics, tables, plots using Quarto R.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"concepts-and-strategies","dir":"Articles","previous_headings":"","what":"Concepts and strategies","title":"3. Descriptive assessment of datasets","text":"Recipe, use PassiveBrownFam dataset corpora package (Evert 2023). dataset contains information passive voice usage Brown family corpora. dataset contains 11 variables 2,449 observations. assigned dataset object brown_fam_df made minor modifications variable names improve readability dataset. can learn variables reading dataset documentation ?corpora::PassiveBrownFam.","code":"# Load packages library(dplyr)  # Read the dataset from the `corpora` package brown_fam_df <-   corpora::PassiveBrownFam |> # reference the dataset   as_tibble() # convert to a tibble  # Rename variables brown_fam_df <-   brown_fam_df |> # pass the original dataset   rename( # rename variables: new_name = old_name     lang_variety = lang,     num_words = n.words,     active_verbs = act,     passive_verbs = pass,     total_verbs = verbs,     percent_passive = p.pass   )  # Preview glimpse(brown_fam_df) ## Rows: 2,499 ## Columns: 11 ## $ id              <chr> \"brown_A01\", \"brown_A02\", \"brown_A03\", \"brown_A04\", \"b… ## $ corpus          <fct> Brown, Brown, Brown, Brown, Brown, Brown, Brown, Brown… ## $ section         <fct> A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, … ## $ genre           <fct> press reportage, press reportage, press reportage, pre… ## $ period          <fct> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, … ## $ lang_variety    <fct> AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE, AmE,… ## $ num_words       <int> 2080, 2116, 2051, 2095, 2111, 2102, 2099, 2069, 2058, … ## $ active_verbs    <int> 164, 154, 135, 128, 170, 166, 165, 163, 153, 169, 132,… ## $ passive_verbs   <int> 40, 25, 34, 25, 32, 21, 31, 19, 39, 23, 17, 10, 15, 26… ## $ total_verbs     <int> 204, 179, 169, 153, 202, 187, 196, 182, 192, 192, 149,… ## $ percent_passive <dbl> 19.61, 13.97, 20.12, 16.34, 15.84, 11.23, 15.82, 10.44…"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"statistical-overviews","dir":"Articles","previous_headings":"Concepts and strategies","what":"Statistical overviews","title":"3. Descriptive assessment of datasets","text":"Understanding data utmost importance , , analysis. get know data inspecting data origin, dictionary, structure, move summarizing data. statistical overview data good place start gives us sense variables variable types dataset. can use skimr package create statistical overview data, using convienent skim() function. create statistical overview brown_fam_df dataset.  output skim() function contains lot information essentially two parts: summary dataset summary variable dataset. summary variables, however, grouped variable type. Remember, variables data frame vector vector type. already learned different types vectors R, including character, numeric, logical. dataset, presented new type vector: factor. factor essentially character vector contains set discrete values, levels. Factors can ordered unordered can contain levels present data. Now, looking variable types, can see 1 character variable, 5 factor variables, 5 numeric variables. variable types assume different set summary statistics. example, can calculate mean numeric variable character variable. , can count number unique values character variable numeric variable. variables, skim() also provide number missing values percent non-missing values. Inspecting entire dataset good place start point often want focus set variables. can add yank() function extract statistical overview set variables variable types. extract statistical overview numeric variables brown_fam_df dataset.","code":"# Load packages library(skimr)  # Create a statistical overview of the `brown_fam_df` dataset skim(brown_fam_df) # ── Data Summary ──────────────────────── #                            Values       # Name                       brown_fam_df # Number of rows             2499         # Number of columns          11           # _______________________                 # Column type frequency:                  #   character                1            #   factor                   5            #   numeric                  5            # ________________________                # Group variables            None         #  # ── Variable type: character ──────────────────────────────────────────────────── #   skim_variable n_missing complete_rate min max empty n_unique whitespace # 1 id                    0             1   7   9     0     2499          0 #  # ── Variable type: factor ─────────────────────────────────────────────────────── #   skim_variable n_missing complete_rate ordered n_unique # 1 corpus                0             1 FALSE          5 # 2 section               0             1 FALSE         15 # 3 genre                 0             1 FALSE         15 # 4 period                0             1 FALSE          3 # 5 lang_variety          0             1 FALSE          2 #   top_counts                             # 1 BLO: 500, Bro: 500, LOB: 500, FLO: 500 # 2 J: 400, G: 381, F: 228, A: 220         # 3 lea: 400, bel: 381, pop: 228, pre: 220 # 4 196: 1000, 199: 999, 193: 500          # 5 BrE: 1500, AmE: 999                    #  # ── Variable type: numeric ────────────────────────────────────────────────────── #   skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50 # 1 num_words               0             1 2165.  97.8  1406     2127    2163   # 2 active_verbs            0             1  179.  56.6    39      139     170   # 3 passive_verbs           0             1   25.7 12.9     2       16      23   # 4 total_verbs             0             1  204.  49.1    66      170     196   # 5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1 #      p75   p100 hist  # 1 2200   4397   ▁▇▁▁▁ # 2  214    551   ▃▇▂▁▁ # 3   32     86   ▆▇▂▁▁ # 4  234    571   ▃▇▂▁▁ # 5   18.2   67.7 ▇▅▁▁▁ # Extract the statistical overview of the numeric variables brown_fam_df |>   skim() |>   yank(\"numeric\") ── Variable type: numeric ─────────────────────────────────────────────────────────────────────────   skim_variable   n_missing complete_rate   mean    sd       p0     p25    p50    p75   p100 hist 1 num_words               0             1 2165.  97.8  1406     2127    2163   2200   4397   ▁▇▁▁▁ 2 active_verbs            0             1  179.  56.6    39      139     170    214    551   ▃▇▂▁▁ 3 passive_verbs           0             1   25.7 12.9     2       16      23     32     86   ▆▇▂▁▁ 4 total_verbs             0             1  204.  49.1    66      170     196    234    571   ▃▇▂▁▁ 5 percent_passive         0             1   14.0  9.13    0.612    7.39   12.1   18.2   67.7 ▇▅▁▁▁"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"summary-statistics-of-particular-variables","dir":"Articles","previous_headings":"Concepts and strategies","what":"Summary statistics of particular variables","title":"3. Descriptive assessment of datasets","text":"summary statistics useful preliminary interactive use, oftent case want focus particular variable set variables potential relationships variables. can use dplyr package calculate summary statistics particular variable set variables. can use group_by() function group data particular variable variables. can use summarize() function calculate summary statistics grouped data. example, calculate mean median percent_passive variable brown_fam_df dataset grouped lang_variety variable. result 2x3 data frame includes mean median percent_passive variable two levels lang_variety variable. group_by() function can also used group multiple variables. example, calculate mean median percent_passive variable brown_fam_df dataset grouped lang_variety genre variables. numeric variables, percent_passive, number summary statistics can calculate. seen R functions mean median can also calculate standard deviation (sd()), variance (var()), minimum (min()), maximum (max()), interquartile range (IQR()), median absolute deviation (mad()), quantiles (quantile()). calculations make sense numeric variables character variables. character variables, factors, summary statistics limited. can calculate number observations (n()) / number unique values (n_distinct()). now summarize number observations n() grouped genre variable brown_fam_df dataset. Just , can add multiple grouping variables group_by(). add lang_variety grouping calculate number observations n() grouped genre lang_variety variables brown_fam_df dataset. Tip result calculating number observations character factor variable known frequency table. Grouping two categorical variables known cross-tabulation contingency table. Now, can also pipe results group_by() summarize() another function. can say sort, select, filter results. can also perform another summary function. important, however, remember result group_by() produces grouped data frame. Subsequent functions applied grouped data frame. can lead unexpected results original grouping relevant subsequent function. avoid , can use ungroup() function remove grouping relevant grouped summary statistics calculated. return calculating number observations n() grouped genre lang_variety variables brown_fam_df dataset. add another summary uses n variable calculate mean median number observations. use ungroup() function, mean median calculated genre collapsed across lang_variety. Therefore see mean median calculated number documents corpus 15 genres. use ungroup() function, mean median calculated genres. Note use ungroup() function summaries clear grouping calculating mean median. Now see mean median calculated across genres. leave section, look ways create frequency contingency tables character factor variables. shortcut calculate frequency table character factor variable use count() function dplyr package. calculate number observations grouped genre variable brown_fam_df dataset. can also add multiple grouping variables count() create contingency tables. add lang_variety grouping create cross-tabulation genre lang_variety variables brown_fam_df dataset. Note results count() grouped need use ungroup() function calculating subsequent summary statistics. Another way create frequency contingency tables use tabyl() function janitor package (Firke 2023). create frequency table genre variable brown_fam_df dataset. addition providing frequency counts, tabyl() function also provides percent observations level variable. , can add three grouping variables tabyl() well. add lang_variety grouping create contingency table genre lang_variety variables brown_fam_df dataset. results include percent observations level variable clear calculate percent observations level variable multiple grouping variables. must specify want calculate percent observations row column. janitor package includes variety adorn_*() functions add additional information results tabyl(), including percentages, frequency, totals. adorn_percentages(): add percentages results tabyl() adorn_pct_formatting(): format percentages include % sign adorn_ns(): add frequency results tabyl() adorn_rounding(): round results tabyl() adorn_totals(): add totals results tabyl() , return adding percentages results genre lang_variety cross-tabulation. calculate -column percentages observations give ue percent observations level genre variable level lang_varity variable. words, aiming assess degree distribution genre similar different across lang_variety. can see assigned results cross-tabulation object brown_genre_lang_ct. important note results tabyl() data frames, albeit special class tabyl. Therefore, can apply subsequent operations results tabyl() data frame. However, must pay attention variable types results tabyl(). see look like numeric values results tabyl() actually character values. Just something aware working results tabyl().","code":"# Mean and median of `percent_passive` grouped by `lang_variety` brown_fam_df |>   group_by(lang_variety) |>   summarize(     mean_percent_passive = mean(percent_passive),     median_percent_passive = median(percent_passive)   ) ## # A tibble: 2 × 3 ##   lang_variety mean_percent_passive median_percent_passive ##   <fct>                       <dbl>                  <dbl> ## 1 AmE                          12.9                   11.0 ## 2 BrE                          14.8                   13.3 # Mean and median of `percent_passive` grouped by `lang_variety` and `genre` brown_fam_df |>   group_by(lang_variety, genre) |>   summarize(     mean_percent_passive = mean(percent_passive),     median_percent_passive = median(percent_passive)   ) ## # A tibble: 30 × 4 ##    lang_variety genre            mean_percent_passive median_percent_passive ##    <fct>        <fct>                           <dbl>                  <dbl> ##  1 AmE          press reportage                 11.5                   11.0  ##  2 AmE          press editorial                 10.6                   10.1  ##  3 AmE          press reviews                    9.54                   9.77 ##  4 AmE          religion                        14.3                   14.3  ##  5 AmE          skills / hobbies                14.9                   13.9  ##  6 AmE          popular lore                    14.0                   12.7  ##  7 AmE          belles lettres                  12.0                   11.7  ##  8 AmE          miscellaneous                   23.5                   23.3  ##  9 AmE          learned                         21.3                   18.3  ## 10 AmE          general fiction                  6.22                   5.89 ## # ℹ 20 more rows # Frequency table for `genre` brown_fam_df |>   group_by(genre) |>   summarize(     n = n(),   ) ## # A tibble: 15 × 2 ##    genre                n ##    <fct>            <int> ##  1 press reportage    220 ##  2 press editorial    135 ##  3 press reviews       85 ##  4 religion            85 ##  5 skills / hobbies   186 ##  6 popular lore       228 ##  7 belles lettres     381 ##  8 miscellaneous      150 ##  9 learned            400 ## 10 general fiction    145 ## # ℹ 5 more rows # Cross-tabulation for `genre` and `lang_variety` brown_fam_df |>   group_by(genre, lang_variety) |>   summarize(     n = n(),   ) ## # A tibble: 30 × 3 ##    genre            lang_variety     n ##    <fct>            <fct>        <int> ##  1 press reportage  AmE             88 ##  2 press reportage  BrE            132 ##  3 press editorial  AmE             54 ##  4 press editorial  BrE             81 ##  5 press reviews    AmE             34 ##  6 press reviews    BrE             51 ##  7 religion         AmE             34 ##  8 religion         BrE             51 ##  9 skills / hobbies AmE             72 ## 10 skills / hobbies BrE            114 ## # ℹ 20 more rows # Mean and median of `n` grouped by `genre` brown_fam_df |>   group_by(genre, lang_variety) |>   summarize(     n = n(),   ) |>   summarize(     mean_n = mean(n),     median_n = median(n)   ) ## # A tibble: 15 × 3 ##    genre            mean_n median_n ##    <fct>             <dbl>    <dbl> ##  1 press reportage   110      110   ##  2 press editorial    67.5     67.5 ##  3 press reviews      42.5     42.5 ##  4 religion           42.5     42.5 ##  5 skills / hobbies   93       93   ##  6 popular lore      114      114   ##  7 belles lettres    190.     190.  ##  8 miscellaneous      75       75   ##  9 learned           200      200   ## 10 general fiction    72.5     72.5 ## # ℹ 5 more rows # Number of observations for each `genre` and `lang_variety` brown_fam_df |>   group_by(genre, lang_variety) |>   summarize(     n = n(),   ) |>   ungroup() |>   summarize(     mean_n = mean(n),     median_n = median(n)   ) ## # A tibble: 1 × 2 ##   mean_n median_n ##    <dbl>    <dbl> ## 1   83.3       72 # Frequency table for `genre` brown_fam_df |>   count(genre) ## # A tibble: 15 × 2 ##    genre                n ##    <fct>            <int> ##  1 press reportage    220 ##  2 press editorial    135 ##  3 press reviews       85 ##  4 religion            85 ##  5 skills / hobbies   186 ##  6 popular lore       228 ##  7 belles lettres     381 ##  8 miscellaneous      150 ##  9 learned            400 ## 10 general fiction    145 ## # ℹ 5 more rows # Cross-tabulation for `genre` and `lang_variety` brown_fam_df |>   count(genre, lang_variety) ## # A tibble: 30 × 3 ##    genre            lang_variety     n ##    <fct>            <fct>        <int> ##  1 press reportage  AmE             88 ##  2 press reportage  BrE            132 ##  3 press editorial  AmE             54 ##  4 press editorial  BrE             81 ##  5 press reviews    AmE             34 ##  6 press reviews    BrE             51 ##  7 religion         AmE             34 ##  8 religion         BrE             51 ##  9 skills / hobbies AmE             72 ## 10 skills / hobbies BrE            114 ## # ℹ 20 more rows # Frequency table for `genre`  # Load packages library(janitor)  brown_fam_df |>   tabyl(genre) ## # A tibble: 15 × 3 ##    genre                n percent ##    <fct>            <int>   <dbl> ##  1 press reportage    220  0.0880 ##  2 press editorial    135  0.0540 ##  3 press reviews       85  0.0340 ##  4 religion            85  0.0340 ##  5 skills / hobbies   186  0.0744 ##  6 popular lore       228  0.0912 ##  7 belles lettres     381  0.152  ##  8 miscellaneous      150  0.0600 ##  9 learned            400  0.160  ## 10 general fiction    145  0.0580 ## # ℹ 5 more rows # Cross-tabulation for `genre` and `lang_variety` brown_fam_df |>   tabyl(genre, lang_variety) ## # A tibble: 15 × 3 ##    genre              AmE   BrE ##    <fct>            <dbl> <dbl> ##  1 press reportage     88   132 ##  2 press editorial     54    81 ##  3 press reviews       34    51 ##  4 religion            34    51 ##  5 skills / hobbies    72   114 ##  6 popular lore        96   132 ##  7 belles lettres     150   231 ##  8 miscellaneous       60    90 ##  9 learned            160   240 ## 10 general fiction     58    87 ## # ℹ 5 more rows # Cross-tabulation for `genre` and `lang_variety` brown_genre_lang_ct <-   brown_fam_df |>   tabyl(genre, lang_variety) |> # genre x lang_variety   adorn_percentages(\"col\") |> # by-column percentages   adorn_pct_formatting() |> # format percentages   adorn_ns(\"front\") # add frequency (in front)  # View brown_genre_lang_ct ## # A tibble: 15 × 3 ##    genre            AmE         BrE         ##    <fct>            <chr>       <chr>       ##  1 press reportage  88  (8.8%)  132  (8.8%) ##  2 press editorial  54  (5.4%)  81  (5.4%)  ##  3 press reviews    34  (3.4%)  51  (3.4%)  ##  4 religion         34  (3.4%)  51  (3.4%)  ##  5 skills / hobbies 72  (7.2%)  114  (7.6%) ##  6 popular lore     96  (9.6%)  132  (8.8%) ##  7 belles lettres   150 (15.0%) 231 (15.4%) ##  8 miscellaneous    60  (6.0%)  90  (6.0%)  ##  9 learned          160 (16.0%) 240 (16.0%) ## 10 general fiction  58  (5.8%)  87  (5.8%)  ## # ℹ 5 more rows # Class of `brown_genre_lang_ct` class(brown_genre_lang_ct) ## [1] \"tabyl\"      \"data.frame\" # Variable types of `brown_genre_lang_ct` glimpse(brown_genre_lang_ct) ## Rows: 15 ## Columns: 3 ## $ genre <fct> press reportage, press editorial, press reviews, religion, skill… ## $ AmE   <chr> \"88  (8.8%)\", \"54  (5.4%)\", \"34  (3.4%)\", \"34  (3.4%)\", \"72  (7.… ## $ BrE   <chr> \"132  (8.8%)\", \"81  (5.4%)\", \"51  (3.4%)\", \"51  (3.4%)\", \"114  (…"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"creating-quarto-tables","dir":"Articles","previous_headings":"Concepts and strategies","what":"Creating Quarto tables","title":"3. Descriptive assessment of datasets","text":"Summarizing data useful understanding data part analysis also communicating data reports, manuscripts, presentations. One way communicate summary statistics tables. Quarto, can use knitr package (Xie 2023) combination code block options produce formatted tables can cross-reference prose sections. work brown_genre_lang_ct object created previous section. create table Quarto, use kable() function. kable() function takes data frame (matrix) argument. format argument derived Quarto document format ('html', 'pdf', etc.). add caption table enable cross-referencing, use code block options label tbl-cap. label option takes label prefixed tbl- create cross-reference table. tbl-cap option takes caption table, quotation marks. Now can cross-reference table @tbl-brown-genre-lang-ct syntax. following Quarto document produce following prose cross-reference formatted table output. see Table 1, distribution genre similar across lang_variety. Table 1: Cross-tabulation genre lang_variety Dive deeper kableExtra package (Zhu 2021) provides additional functionality formatting tables Quarto.","code":"# Load packages library(knitr)  # Create a table in Quarto kable(brown_genre_lang_ct) ```{r} #| label: tbl-brown-genre-lang-ct #| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"  # Create a table in Quarto kable(brown_genre_lang_ct) ``` As we see in @tbl-brown-genre-lang-ct, the distribution of `genre` is similar across `lang_variety`.  ```{r} #| label: tbl-brown-genre-lang-ct #| tbl-cap: \"Cross-tabulation of `genre` and `lang_variety`\"  # Print cross-tabulation kable(brown_genre_lang_ct) ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"creating-quarto-plots","dir":"Articles","previous_headings":"Concepts and strategies","what":"Creating Quarto plots","title":"3. Descriptive assessment of datasets","text":"tables useful communicating summary statistics numeric character variables, plots useful communicating relationships variables especially one variables numeric. Furthermore, complex relationships, plots can effective tables. Quarto, can use ggplot2 package (Wickham et al. 2023) combination code block options produce formatted plots can cross-reference prose sections. see action simple histogram percent_passive variable brown_fam_df dataset. Quarto document produce following prose cross-reference formatted plot output. see Figure 1, distribution percent_passive skewed right. Figure 1: Histogram percent_passive ggplot2 package implements 'Grammar Graphics' approach creating plots. approach based idea plots can broken components, layers, layer can manipulated independently. main components data, aesthetics, geometries. Data data frame contains variables plotted. Aesthetics variables mapped x-axis, y-axis (well color, shape, size, etc.). Geometries visual elements used represent data, points, lines, bars, etc.. discussed R lesson \"Visual Summaries\", aes() function used map variables aesthetics can added ggplot() function geom_*() function depending whether aesthetic mapped geometries specific geometry, respectively. Take look following stages earlier plot tabs .","code":"As we see in @fig-brown-fam-percent-passive-hist, the distribution of `percent_passive` is skewed to the right.  ```{r} #| label: fig-brown-fam-percent-passive-hist #| fig-cap: \"Histogram of `percent_passive`\"  # Create a histogram in Quarto ggplot(brown_fam_df) +   geom_histogram(aes(x = percent_passive)) ```"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"stages","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Stages","title":"3. Descriptive assessment of datasets","text":"Data Aesthetics Geometries data layer produce plot foundation plot.  aesthetics layer produce plot maps variables aesthetics used plot.  geometries layer produces plot connecting data aesthetics layers particular way specified geometries, case histogram.","code":"# Data layer ggplot(brown_fam_df) # Aesthetics layer ggplot(brown_fam_df, aes(x = percent_passive)) # Geometries layer ggplot(brown_fam_df, aes(x = percent_passive)) +   geom_histogram()"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"choosing-the-right-plot","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Choosing the right plot","title":"3. Descriptive assessment of datasets","text":"Just tables, type summary choose communicate plot depends type variables working relationships variables. included examples plots can used communicate different types variables relationships.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"single-numeric-variable","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Single numeric variable","title":"3. Descriptive assessment of datasets","text":"Histogram Density plot","code":"# Histogram ggplot(brown_fam_df) +   geom_histogram(aes(x = percent_passive)) # Density plot ggplot(brown_fam_df) +   geom_density(aes(x = percent_passive))"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"numeric-and-categorical-variables","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Numeric and categorical variables","title":"3. Descriptive assessment of datasets","text":"Density plot Boxplot Violin plot","code":"# Density plot ggplot(brown_fam_df) +   geom_density(     aes(       x = percent_passive,       fill = lang_variety     ),     alpha = 0.5 # adds transparency   ) # Boxplot ggplot(brown_fam_df) +   geom_boxplot(     aes(       x = lang_variety,       y = percent_passive     )   ) # Violin plot ggplot(brown_fam_df) +   geom_violin(     aes(       x = lang_variety,       y = percent_passive     )   )"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"two-numeric-variables","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Two numeric variables","title":"3. Descriptive assessment of datasets","text":"Scatterplot Scatterplot regression line","code":"# Scatterplot ggplot(brown_fam_df) +   geom_point(     aes(       x = active_verbs,       y = passive_verbs     )   ) # Scatterplot with regression line ggplot(   brown_fam_df,   aes(     x = active_verbs,     y = passive_verbs   ) ) +   geom_point() +   geom_smooth(method = \"lm\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"other-variable-combinations","dir":"Articles","previous_headings":"Concepts and strategies > Creating Quarto plots","what":"Other variable combinations","title":"3. Descriptive assessment of datasets","text":"examples, looked common variable combinations one two variable plots. sophisticated plots can used variable combinations using ggplot2. now, leave another time.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"3. Descriptive assessment of datasets","text":"factor character vector augmented include information discrete values, levels, vector. TRUEFALSE difference frequency table contingency table? frequency table cross-tabulation two categorical variables.contingency table cross-tabulation two categorical variables. skimrdplyrggplot2knitr package used create formatted tables R. add geometry layer, geom_histogram(), ggplot object |> operator used. TRUEFALSE visualize relationship two numeric variables, histogramdensity plotboxplotviolin plotscatterplot often used. aes() function added ggplot() function, aesthetic mapped geometries. TRUEFALSE","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-3.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"3. Descriptive assessment of datasets","text":"beginning Lab 3, learners comfortable skills knowledge developed previous recipes labs. lab, chance use skills introduced Recipe provide descriptive assessment dataset includes statistics, tables, plots using Quarto R. additional skills knowledge need complete Lab 3 include: Summarizing data skimr Summarizing data dplyr Creating Quarto tables knitr Creating Quarto plots ggplot2","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"4. Scaffolding a research project","text":"recipe, learn scaffold research project use tools resources available us manage research projects. build understanding computing environment structure reproducible projects introduce new features Git GitHub. cover following topics: Understanding components reproducible project Connecting computing environment reproducible project management Understanding workflow project structure Using Git GitHub manage project Lab 4, apply learned recipe scaffold research project forking, cloning, editing, commiting, pushing repository GitHub.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"understanding-components-of-a-reproducible-project","dir":"Articles","previous_headings":"Concepts and strategies","what":"Understanding components of a reproducible project","title":"4. Scaffolding a research project","text":"Reproducible projects composed two main components: computing environment project structure. computing environment hardware, operating system, software use work project structure organization files folders make project. can see Figure 1, components subcomponents nested within . Figure 1: Components reproducible project lesson computing environment, learned importance understanding computing environment find information computing environment inspecting R session, see . learned, computing environment used create project different computing environment used reproduce project, risk project reproducible. tackle components Figure 1 , however, instead focus project structure. Later , experience, address components.","code":"─ Session info ──────────────────────────────────────────────────────  setting  value  version  R version 4.3.2 (2023-10-31)  os       macOS Ventura 13.6.1  system   x86_64, darwin22.6.0  ui       unknown  language (EN)  collate  en_US.UTF-8  ctype    en_US.UTF-8  tz       America/New_York  date     2023-11-26  pandoc   3.1.9 @ /usr/local/bin/pandoc  ─ Packages ──────────────────────────────────────────────────────  package     * version date (UTC) lib source  cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.2)  jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.2)  rlang         1.1.2   2023-11-04 [1] CRAN (R 4.3.2)   [1] /Users/francojc/R/Library  [2] /usr/local/Cellar/r/4.3.2/lib/R/library"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"approaching-reproducible-project-management","dir":"Articles","previous_headings":"Concepts and strategies","what":"Approaching reproducible project management","title":"4. Scaffolding a research project","text":"project structure organization files folders make project. minimal project structure includes separation input, process, output research, documentation project, reproduce project, project files . project structure important helps us organize work helps others understand . concensus best project structure . However, principles covered Chapter 4 can guide us developing project structure organizes work makes work understandable others. principles met, can add additional structure project meet project-specific needs. create reproducible project structure, need create directory set files meet principles minimal reproducible framework. project, back directories files / share others number ways. Although approach already good step right direction, error prone likely lead inconsistencies across projects. better approach develop, adopt, project structure template can used projects, use version control track changes project, upload project remote repository backed (including version history) can shared others efficiently. later approach one use subsequent lessons, recipes, labs course.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"leveraging-git-and-github-for-reproducible-project-management","dir":"Articles","previous_headings":"Concepts and strategies","what":"Leveraging Git and GitHub for reproducible project management","title":"4. Scaffolding a research project","text":"point, somewhat familiar Git Github. likely used Git copy download repository GitHub, say example labs course. However, going behind scenes likely still bit mystery. section, demystify Git GitHub, bit, learn use together manage project different scenarios. Tip Verify working version Git computing environment make sure GitHub account. can refer Guide 2 information . rewind bit review Git Github work together. Git version control system allows us track changes project files. command line tool, much like R, installed software. Also like R, can interact Git graphical user interface (GUI), RStudio. Directory file tracking Git can added project time. tracked project called repository, repo short. used computing environment, repo called local repository. many benefits using Git track changes local repository, including ability revert previous versions files, create edit parallel copies files selectively integrate , much . real power Git realized combination GitHub. Github cloud-based remote repository allows us store git-tracked projects. web-based platform requires account use. signed , can connect Git Github create remote repositories upload local repositories . can also download remote repositories computing environment. many, many features Github offers, now focus key features help us manage projects. Figure 2, provide schematic look relationship Github Git three common scenarios. Figure 2: Key features GitHub Scenario : Clone remote repository scenario, locate remote repository Github someone made publically available. clone (copy download) repository computing environment. repository cloned locally, can edit files see fit. essence, just downloading group files folders computing environment Github. scenario using course download lab repositories. Steps: Locate remote repository Github someone made publically available copy clone URL. Open RStudio create new project version control. Paste clone URL repository URL field. Choose project parent directory (project saved). (optional) Rename project directory. Scenario B: Fork clone remote repository scenario differs two respects. First, first fork (copy ) remote repository Github account cloning computing environment. Second, commit (log edits) changes git tracking system push (sync ) changes remote repository. case, just downloading group files folders. setting link person's remote repository remote repository. use link, want , can pull (sync ) changes made person's remote repository remote repository. can useful want keep remote repository date person's remote repository. Furthermore, link allows us propose changes person's remote repository pull request --request person pull changes remote repository. can useful want collaborate person project. Pull pull request advanced features address point. second difference using Git track changes local repository push changes remote repository. key feature allows us keep track changes project files folders revert previous versions needed. also allows us share project others collaborate . Steps: Locate remote repository Github someone made publically available click fork button. Still Github, choose new account owner forked repository. forked repository, copy clone URL. Open RStudio create new project version control. Paste clone URL repository URL field. Choose project parent directory (project saved). (optional) Rename project directory. Make changes project files folders. Commit changes git tracking system. Push changes remote repository. Scenario C: Create/ Join clone remote repository scenario similar B, instead forking remote repository, create new remote repository Github clone computing environment. commit push changes remote repository. case, creating new remote repository using Git track changes local repository push changes remote repository. scenario common work projects want collaborate others project. latter case, create remote repository invite others collaborate . Everyone permissions remote repository can clone computing environment, make changes, push changes remote repository. allows everyone work project keep track changes project files folders. working multiple people project, can imagine working project locally working project locally, might make changes files folders. push changes remote repository, risk changes conflict . Git Github features help us manage conflicts (pull, fetch, merge, etc.), address point either. Steps: Create new remote repository Github accept invitation collaborate remote repository. Copy clone URL. Open RStudio create new project version control. Paste clone URL repository URL field. Choose project parent directory (project saved). (optional) Rename project directory. Make changes project files folders. Commit changes git tracking system. Push changes remote repository.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"4. Scaffolding a research project","text":"recipe, reviewed components reproducible research projects: computing environment project structure. computing environment hardware, operating system, software use work project structure organization files folders make project. Furthermore, learned Git Github can used together manage project different scenarios.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"4. Scaffolding a research project","text":"Select component computing environment following related : Windows 10 hardwareoperating systemsoftware R version 4.3.1 hardwareoperating systemsoftware dplyr_1.1.4 hardwareoperating systemsoftware Select Git name following actions: Copy download remote repository computing environment cloneforkcommitpush Log edits git tracking system cloneforkcommitpush Sync changes remote repository cloneforkcommitpush","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-4.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"4. Scaffolding a research project","text":"lab 4, apply learned recipe scaffold research project forking cloning research project template repository Github. edit project files folders, commit changes git tracking system, push changes remote repository Github. beginning Lab 4, make sure comfortable following: Cloning remote repository computing environment Creating editing files folders, particular Quarto documents. additional knowledge skills need complete lab covered recipe include: Understanding components reproducible project Understanding importance project structure reproducible project management Forking, cloning, editing, commiting, pushing repository","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"5. Collecting and documenting data","text":"point, now strong undertanding foundations programming R data science workflow. Previous lessons, recipes, labs focused developing skills chapters aimed provide conceptual framework understanding steps data science workflow. now turn applying conceptual knowledge technical skills accomplish tasks data science workflow. recipe, focus acquiring data text analysis project. cover following topics: Finding data sources Data collection strategies Data documentation Along way put practice foundational R skills also continue work newly introduced skills control statements custom functions. Lab 5, apply learned far acquire data text analysis project.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"finding-data-sources","dir":"Articles","previous_headings":"Concepts and strategies","what":"Finding data sources","title":"5. Collecting and documenting data","text":"find data sources, best research question mind. help narrow search data sources. However, finding data sources can also good way generate research questions. either case, takes sleuthing find data sources work research question. addition, data source , also need consider permissions licensing data source. best consider early process avoid surprises later. Finally, also need consider data format used analysis. can case data source seems ideal, data format conducive analysis like . Tip Consult Identifying data data sources guide ideas find data sources. recipe, consider hypothetical reseach aimed exploring potential similarities differences lexical, syntactic, / stylistic features American English literature mid 19th century. Dive deeper interested understanding literary analysis perspective text analysis, highly recommend Matthew Jockers' book Text Analysis R Students Literature (Jockers 2014). book great resource understanding apply text analysis literary analysis. Project Gutenberg great source data research question. Project Gutenberg volunteer effort digitize archive cultural works. great majority works Project Gutenberg database public domain United States. means works can freely used shared. Furthermore, gutenbergr package provides API accessing Project Gutenberg database. means can use R access Project Gutenberg database download text metadata works interested . gutenbergr package also provides number data frames can help us identify works interested .","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"data-collection-strategy","dir":"Articles","previous_headings":"Concepts and strategies","what":"Data collection strategy","title":"5. Collecting and documenting data","text":"now turn data collection strategy. number data collection strategies can used acquire data text analysis project. chapter, covered manual programmatic downloads APIs. use R package provide API accessing data source. Dive deeper interested learning another data collection strategy, web scraping, suggest look Web scraping R guide. load dplyr, readr gutenbergr packages prepare data collection process. main workhorse gutenbergr package gutenberg_download(). required argument id(s) used Project Gutenberg index works database. function download text work(s) return data frame gutenberg id text work(s). find gutenberg ids? manual method go Project Gutenberg website search work interested . example, say interested work \"Tale Two Cities\" Charles Dickens. can search work Project Gutenberg website click link work. url work : https://www.gutenberg.org/ebooks/98. gutenberg id number end url, case 98. work individual works, just download text Project Gutenberg website? works Project Gutenberg perfectly fine. can share text others license works Project Gutenberg public domain. However, interested downloading multiple works? number works increases, time takes manually download work increases. Furthermore, gutenbergr package provides number additional attributes can downloaded organized along side text. Finally, results gutenberg_download() function returned data frame can easily manipulated analyzed R. data acquisition plan, want collect works number authors. best leverge gutenbergr package download works interested . need know gutenberg ids works interested . Convienently, gutenbergr package also includes number data frames contain meta data works Project Gutenberg database. data frames include meta data works Project Gutenberg database (gutenberg_metadata), authors (gutenberg_authors), subjects (gutenberg_subjects). take look structure data frames. overvew, can see 72,569 works Project Gutenberg database. can also see 23,980 authors 231,741 subjects. dicussed, work Project Gutenberg database gutenberg id. gutenberg_id appears gutenberg_metadata also gutenberg_subjects data frame. common attribute means work particular gutenberg id can linked subject(s) associated work. Another important attribute gutenberg_author_id links work author(s) work. Yes, author name gutenberg_metadata data frame, gutenberg_author_id can used link work gutenberg_authors data frame contains additional information authors. Tip gutenbergr package periodically updated. check see data frame last updated run: now describe attributes useful data acquisition plan. gutenberg_subjects data frame, subject_type subject. subject_type type subject classification system used classify work. tabulate column, see two types subject classification systems used: Library Congress Classification (lcc) Library Congress Subject Headings (lcsh). subject column contains subject code work. lsch subject code descriptive character string lcc subject code id character string combination letters (numbers) Library Congress uses classify works. data acquistion plan, use lcc subject classification system select works Library Congress Classification English Literature (PR) American Literature (PS). gutenberg_authors data frame, birthdate deathdate attributes. attributes useful filtering authors lived mid 19th century. overview gutenbergr package data frames contains, can now begin develop data acquisition plan. Select authors lived mid 19th century gutenberg_authors data frame. Select works Library Congress Classification English Literature (PR) American Literature (PS) gutenberg_subjects data frame. Select works gutenberg_metadata associated authors subjects selected steps 1 2. Download text metadata works selected step 3 using gutenberg_download() function. Write data disk appropriate format.","code":"library(dplyr) # data manipulation library(readr) # data import/ export library(gutenbergr) # Project Gutenberg API glimpse(gutenberg_metadata) ## Rows: 72,569 ## Columns: 8 ## $ gutenberg_id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,… ## $ title               <chr> \"The Declaration of Independence of the United Sta… ## $ author              <chr> \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo… ## $ gutenberg_author_id <int> 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 8, … ## $ language            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e… ## $ gutenberg_bookshelf <chr> \"Politics/American Revolutionary War/United States… ## $ rights              <chr> \"Public domain in the USA.\", \"Public domain in the… ## $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR… glimpse(gutenberg_authors) ## Rows: 23,980 ## Columns: 7 ## $ gutenberg_author_id <int> 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… ## $ author              <chr> \"United States\", \"Lincoln, Abraham\", \"Henry, Patri… ## $ alias               <chr> \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",… ## $ birthdate           <int> NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, NA, 18… ## $ deathdate           <int> NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, NA, 18… ## $ wikipedia           <chr> \"https://en.wikipedia.org/wiki/United_States\", \"ht… ## $ aliases             <chr> \"U.S.A.\", \"United States President (1861-1865)/Lin… glimpse(gutenberg_subjects) ## Rows: 231,741 ## Columns: 3 ## $ gutenberg_id <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, … ## $ subject_type <chr> \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc… ## $ subject      <chr> \"United States -- History -- Revolution, 1775-1783 -- Sou… attr(gutenberg_metadata, \"date_updated\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"data-collection","dir":"Articles","previous_headings":"Concepts and strategies","what":"Data collection","title":"5. Collecting and documenting data","text":"take steps turn. First, need select authors lived mid 19th century gutenberg_authors data frame. use filter() function. pass gutenberg_authors data frame filter() function use birthdate column select authors born 1800 died 1880 --year chosen mid 19th century generally considered period 1830 1870. assign result variable name authors. ! now data frame authors lived mid 19th century, 787 authors total. span subjects languages, final number authors working . next step select works Library Congress Classification English Literature (PR) American Literature (PS) gutenberg_subjects data frame. use filter() function . pass gutenberg_subjects data frame filter() function use subject_type subject columns select works associated Library Congress Classification English Literature (PR) American Literature (PS). assign result variable name subjects. Now data frame subjects interested . inspect data frame see many works subject. next step subset gutenberg_metadata data frame select works authors subjects selected previous steps. , use filter() . pass gutenberg_metadata data frame filter() function use gutenberg_author_id gutenberg_id columns select works associated authors subjects selected previous steps. assign result variable name works. Filtering gutenberg_metadata data frame authors subjects selected previous steps, now data frame 1,014 works. final number works working can now download text metadata works using gutenberg_download() function. things note gutenberg_download() function. First, vectorized, , can take single value multiple values argument gutenberg_id. good passing vector gutenberg ids function. small fraction works Project Gutenberg public domain therefore downloaded, documented rights column. Furthermore, works text available, seen has_text column. Finally, gutenberg_download() function returns data frame gutenberg id text work(s) --can also select additional attributes returned passing character vector attribute names argument meta_fields. column names gutenberg_metadata data frame contains available attributes. mind, quick test download works. select first 5 works works data frame fit criteria download text metadata works using gutenberg_download() function. assign result variable name works_sample. inspect works_sample data frame. First, output can see meta data attributes returned. Second, can see text column contains values line text (delimited carriage return) 5 works downloaded, even blank lines. make sure correct number works, can use count() function count number works gutenberg_id. Yes, 5 works can see many lines works. now run code entire works data frame write data disk like : accomplish primary goal data acquisition plan. However, key functionality missing like make code reproducible-friendly. First, checking see data already exists disk. already run code script, likely want run . Second, may want use code different parameters, example, may want retrieve different subject codes, different time periods, languages. three additional features can accomplished writing custom function. take look code written far see can turn custom function.","code":"authors <-   gutenberg_authors |>   filter(     birthdate > 1800,     deathdate < 1880   ) subjects <-   gutenberg_subjects |>   filter(     subject_type == \"lcc\",     subject %in% c(\"PR\", \"PS\")   ) subjects |>   count(subject) ## # A tibble: 2 × 2 ##   subject     n ##   <chr>   <int> ## 1 PR       9926 ## 2 PS      10953 works <-   gutenberg_metadata |>   filter(     gutenberg_author_id %in% authors$gutenberg_author_id,     gutenberg_id %in% subjects$gutenberg_id   )  works ## # A tibble: 1,014 × 8 ##    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf ##           <int> <chr>    <chr>                <int> <chr>    <chr>               ##  1           33 The Sca… Hawth…                  28 en       \"Harvard Classics/… ##  2           46 A Chris… Dicke…                  37 en       \"Children's Litera… ##  3           71 On the … Thore…                  54 en       \"\"                  ##  4           77 The Hou… Hawth…                  28 en       \"Best Books Ever L… ##  5           98 A Tale … Dicke…                  37 en       \"Historical Fictio… ##  6          205 Walden,… Thore…                  54 en       \"\"                  ##  7          258 Poems b… Gordo…                 145 en       \"\"                  ##  8          271 Black B… Sewel…                 154 en       \"Best Books Ever L… ##  9          292 Beauty … Taylo…                 167 en       \"\"                  ## 10          394 Cranford Gaske…                 220 en       \"\"                  ## # ℹ 1,004 more rows ## # ℹ 2 more variables: rights <chr>, has_text <lgl> works_sample <-   works |>   filter(     rights == \"Public domain in the USA.\",     has_text == TRUE   ) |>   slice_head(n = 5) |>   gutenberg_download(     meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")   )  works_sample ## # A tibble: 34,385 × 6 ##    gutenberg_id text        title author gutenberg_author_id gutenberg_bookshelf ##           <int> <chr>       <chr> <chr>                <int> <chr>               ##  1           33 \"The Scarl… The … Hawth…                  28 Harvard Classics/M… ##  2           33 \"\"          The … Hawth…                  28 Harvard Classics/M… ##  3           33 \"by Nathan… The … Hawth…                  28 Harvard Classics/M… ##  4           33 \"\"          The … Hawth…                  28 Harvard Classics/M… ##  5           33 \"\"          The … Hawth…                  28 Harvard Classics/M… ##  6           33 \"Contents\"  The … Hawth…                  28 Harvard Classics/M… ##  7           33 \"\"          The … Hawth…                  28 Harvard Classics/M… ##  8           33 \" THE CUST… The … Hawth…                  28 Harvard Classics/M… ##  9           33 \" THE SCAR… The … Hawth…                  28 Harvard Classics/M… ## 10           33 \" I. THE P… The … Hawth…                  28 Harvard Classics/M… ## # ℹ 34,375 more rows works_sample |>   count(gutenberg_id) ## # A tibble: 4 × 2 ##   gutenberg_id     n ##          <int> <int> ## 1           33  8212 ## 2          258 11050 ## 3          271  5997 ## 4          292  9126 works |>   filter(     rights == \"Public domain in the USA.\",     has_text == TRUE   ) |>   gutenberg_download(     meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")   ) |>   write_csv(file = \"data/original/gutenberg/works.csv\") # Get authors within years authors <-   gutenberg_authors |>   filter(     birthdate > 1800,     deathdate < 1880   ) # Get LCC subjects subjects <-   gutenberg_subjects |>   filter(     subject_type == \"lcc\",     subject %in% c(\"PR\", \"PS\")   ) # Get works based on authors and subjects works <-   gutenberg_metadata |>   filter(     gutenberg_author_id %in% authors$gutenberg_author_id,     gutenberg_id %in% subjects$gutenberg_id   ) # Download works works |>   filter(     rights == \"Public domain in the USA.\",     has_text == TRUE   ) |>   gutenberg_download(     meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")   ) |>   write_csv(file = \"data/original/gutenberg/works.csv\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"build-the-custom-function","dir":"Articles","previous_headings":"Concepts and strategies > Data collection","what":"Build the custom function","title":"5. Collecting and documenting data","text":"Function name Function arguments Function code: comments Function code: packages Function code: data check Function code: authors Function code: subject Function code: works Function code: download Function code: write start create function creating name calling function() function. name function get_gutenberg_works(). Now need think arguments like pass function can used customize data acquisition process. First, want check see data already exists disk. need pass path data file function. name argument target_file. Next, want pass subject code works associated . name argument lcc_subject. Finally, want pass birth year death year authors associated . name arguments birth_year death_year. now turn code. like start creating comments describe steps inside function adding code. packages want make sure installed loaded. use pacman package . use p_load() function install load packages. pass character vector package names p_load() function. need create code check data exists. use statement . data exist, print message console data already exists stop function. data exist, create directory structure continue data acquisition process. use fs package (Hester, Wickham, Csárdi 2023) code load library top function. now add code get authors within years. now use birth_year death_year arguments filter gutenberg_authors data frame. Using lcc_subject argument, now filter gutenberg_subjects data frame. use authors subjects data frames filter gutenberg_metadata data frame . now use works data frame download text metadata works using gutenberg_download() function assign results. Finally, write results data frame disk using write_csv() function target_file argument.","code":"get_gutenberg_works <- function() {  } get_gutenberg_works <- function(target_file) {  } get_gutenberg_works <- function(target_file, lcc_subject) {  } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {  } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages    # Check to see if the data already exists    # Get authors within years    # Get LCC subjects    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   library(dplyr)   library(gutenbergr)   library(readr)    # Check to see if the data already exists    # Get authors within years    # Get LCC subjects    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   library(dplyr)   library(gutenbergr)   library(readr)   library(fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years    # Get LCC subjects    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   library(dplyr)   library(gutenbergr)   library(readr)   library(fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   library(dplyr)   library(gutenbergr)   library(readr)   library(fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects   subjects <-     gutenberg_subjects |>     filter(       subject_type == \"lcc\",       subject %in% lcc_subject     )    # Get works based on authors and subjects    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   library(dplyr)   library(gutenbergr)   library(readr)   library(fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects   subjects <-     gutenberg_subjects |>     filter(       subject_type == \"lcc\",       subject %in% lcc_subject     )    # Get works based on authors and subjects   works <-     gutenberg_metadata |>     filter(       gutenberg_author_id %in% authors$gutenberg_author_id,       gutenberg_id %in% subjects$gutenberg_id     )    # Download works    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   library(dplyr)   library(gutenbergr)   library(readr)   library(fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects   subjects <-     gutenberg_subjects |>     filter(       subject_type == \"lcc\",       subject %in% lcc_subject     )    # Get works based on authors and subjects   works <-     gutenberg_metadata |>     filter(       gutenberg_author_id %in% authors$gutenberg_author_id,       gutenberg_id %in% subjects$gutenberg_id     )    # Download works   results <-     works |>     filter(       rights == \"Public domain in the USA.\",       has_text == TRUE     ) |>     gutenberg_download(       meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")     )    # Write works to disk } get_gutenberg_works <- function(target_file, lcc_subject, birth_year, death_year) {   # Load packages   library(dplyr)   library(gutenbergr)   library(readr)   library(fs)    # Check to see if the data already exists   if (file_exists(target_file)) {     message(\"Data already exists \\n\")     return()   } else {     target_dir <- dirname(target_file)     dir_create(path = target_dir, recurse = TRUE)   }    # Get authors within years   authors <-     gutenberg_authors |>     filter(       birthdate > birth_year,       deathdate < death_year     )    # Get LCC subjects   subjects <-     gutenberg_subjects |>     filter(       subject_type == \"lcc\",       subject %in% lcc_subject     )    # Get works based on authors and subjects   works <-     gutenberg_metadata |>     filter(       gutenberg_author_id %in% authors$gutenberg_author_id,       gutenberg_id %in% subjects$gutenberg_id     )    # Download works   results <-     works |>     filter(       rights == \"Public domain in the USA.\",       has_text == TRUE     ) |>     gutenberg_download(       meta_fields = c(\"title\", \"author\", \"gutenberg_author_id\", \"gutenberg_bookshelf\")     )    # Write works to disk   write_csv(results, file = target_file) }"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"using-the-custom-function","dir":"Articles","previous_headings":"Concepts and strategies > Data collection","what":"Using the custom function","title":"5. Collecting and documenting data","text":"now function, get_gutenberg_works(), can use acquire works Project Gutenberg given LCC code authors lived given time period. now flexible function can use acquire data. can add function script use , can add separate script source script want use . Another option add function package. great option plan use function multiple projects share others. Since already created package book, qtalrkit, added function, additional functionality, package. modified function create directory structure data file already exist. also create file name data file based arguments passed function.","code":"# Source function source(\"get_gutenberg_works.R\")  # Get works for PR and PS for authors born between 1800 and 1880 get_gutenberg_works(   target_file = \"data/original/gutenberg/works.csv\",   lcc_subject = c(\"PR\", \"PS\"),   birth_year = 1800,   death_year = 1880 ) # Load package library(qtalrkit)  # Get works for fiction for authors born between 1870 and 1920 get_gutenberg_works(   target_dir = \"data/original/gutenberg/\",   lcc_subject = \"PZ\",   birth_year = 1870,   death_year = 1920 )"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"data-documentation","dir":"Articles","previous_headings":"Concepts and strategies","what":"Data documentation","title":"5. Collecting and documenting data","text":"Finding data sources collecting data important steps acquisition process. However, also important document data collection process. important , others, can reproduce data collection process. data acquisition, documentation includes code, code comments, prose process file used acquire data also data origin file. data origin file text file describes data source data collection process. qtalrkit package includes function, create_data_origin(), can used scaffold data origin file. simply takes file path creates data origin file CSV format. edit file ensure contains information needed document data. Make sure file near data file easy find.","code":"attribute,description Resource name,The name of the resource. Data source,\"URL, DOI, etc.\" Data sampling frame,\"Language, language variety, modality, genre, etc.\" Data collection date(s),The dates the data was collected. Data format,\".txt, .csv, .xml, .html, etc.\" Data schema,\"Relationships between data elements: files, folders, etc.\" License,\"CC BY, CC BY-SA, etc.\" Attribution,Citation information. data   ├── analysis/   ├── derived/   └── original/       ├── works_do.csv       └── gutenberg/           ├── works_pr.csv           └── works_ps.csv"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"5. Collecting and documenting data","text":"recipe, covered acquiring data text analysis project. used `gutenbergr (Johnston Robinson 2023) acquire works Project Gutenberg. exploring resources available, established acquisition plan. used R implement plan. make code reproducible-friendly, wrote custom function acquire data. Finally, discussed importance documenting data collection process introduced data origin file.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"5. Collecting and documenting data","text":"chapter recipe, strategies acquiring data discussed. following discussed strategy acquiring data? Direct downloadProgrammatic downloadAPIsWeb scraping recipe, used gutenbergr package acquire works Project Gutenberg. name function used acquire actual text? gutenberg_metatagutenberg_get()gutenberg_search()gutenberg_download() TrueFalse custom function really necessary writting R package. writing custom function, first step? Write codeWrite commentsLoad packagesCreate function arguments mean say function 'vectorized' R? function returns vectorThe function can take vector argumentThe function can take vector operates element vector Tidyverse package allows us apply non-vectorized functions vectors? dplyrstringrreadrpurrr","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-5.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"5. Collecting and documenting data","text":"beginning Lab 5, make sure comfortable following: Reading subsetting data R Writing data R project structure reproducible projects additional skills covered lab : Identifying data sources Acquiring data manual programmatic downloads APIs Creating data acquisition plan Documenting data collection process Writing custom function Documenting data source data origin file choice data source acquire data . start lab, consider data source like use, strategy use acquire data, data acquire. also consider information need document data collection process. Consult Identifying data data sources guide ideas find data sources.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"6. Organizing and documenting data","text":"acquiring data, next step process organize data tabular curated dataset. curated dataset tidy dataset reflects data without major modifications. dataset serves general starting point data transformation. Just aspects research process, important document process resulting dataset. recipe, focus curating data semi-structured format. process, cover following topics: Reading parsing semi-structured data Creating custom function iterating collection files Combining results single dataset Documenting data curation process resulting dataset process, make use readr, dplyr, stringr, purrr packages, employ regular expressions parse semi-structured data, use qtalrkit package document dataset. load packages now. Lab 6, apply learn recipe curate document acquired data.","code":"# Load packages library(readr) library(dplyr) library(stringr) library(purrr) library(fs) library(qtalrkit)"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"assessing-the-data","dir":"Articles","previous_headings":"Concepts and strategies","what":"Assessing the data","title":"6. Organizing and documenting data","text":"Acquired data can variety formats. range unstructured data running text structured data tabular data. Semi-structured data somewhere . structure, well defined structured data requires work organize tidy dataset. semi-structured example work Switchboard Dialog Act Corpus (SWDA) (University Colorado Boulder 2008) extends Switchboard Corpus speech act annotation. Tip like download decompress data , can running following code: starting point, assume acquired SWDA corpus decompressed project's data/original/swda/ directory, seen . first step inspect data directory file structure (course documentation files). README file contains basic information resource, doc/ directory contains detailed information dialog annotations, following directories prefixed sw... contain individual conversation files. Taking closer look first conversation file directory, sw00utt/ can see contains files .utt extension. take look inside conversation file (sw_0001_4325.utt) see structured internally. can opening file text editor using read_lines() function readr package. things take note . First see conversation files meta-data header offset conversation text line = characters. Second header contains meta-information various types. Third, conversation text interleaved annotation scheme. information may readily understandable, various pieces meta-data header, get better understanding information encoded take look README file. file get birds eye view going . short, data includes 1155 telephone conversations two people annotated 42 'DAMSL' dialog act labels. README file refers us doc/manual.august1.html file information scheme. point open doc/manual.august1.html file browser investigation. find 'DAMSL' stands 'Discourse Annotation Markup System Labeling' first characters line conversation text correspond one combination labels utterance. first utterances : utterance also labeled speaker ('' 'B'), speaker turn ('1', '2', '3', etc.), utterance within turn ('utt1', 'utt2', etc.). annotation provided withing utterance, enough get us started conversations. Now turn meta-data header. see information creation file: 'FILENAME', 'TOPIC', 'DATE', etc. doc/manual.august1.html file much say information returned LDC Documentation found information Online Documentation section. poking around documentation discovered meta-data speaker corpus found caller_tab.csv file. tabular file contain column names, caller_doc.txt . inspecting files manually comparing information conversation file noticed 'FILENAME' information contained three pieces useful information delimited underscores _. first information document id (4325), second third correspond speaker number: first speaker (1632) second speaker B (1519). sum, 1155 conversation files. file two parts, header text section, separated line = characters. header section contains 'FILENAME' line document id, ids speaker speaker B. text section annotated DAMSL tags beginning line, followed speaker, turn number, utterance number, utterance text. knowledge hand, set create tidy dataset column structure Table 1. Table 1: Idealized curated dataset","code":"qtalrkit::get_compressed_data(   url = \"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz\",   target_dir = \"data/original/swda/\" ) data/ ├── analysis/ ├── derived/ └── original/     └── swda/         ├── README         ├── doc/         ├── sw00utt/         ├── sw01utt/         ├── sw02utt/         ├── sw03utt/         ├── sw04utt/         ├── sw05utt/         ├── sw06utt/         ├── sw07utt/         ├── sw08utt/         ├── sw09utt/         ├── sw10utt/         ├── sw11utt/         ├── sw12utt/         └── sw13utt/ ├── sw00utt │   ├── sw_0001_4325.utt │   ├── sw_0002_4330.utt │   ├── sw_0003_4103.utt │   ├── sw_0004_4327.utt │   ├── sw_0005_4646.utt # *x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x* #  #  # FILENAME: 4325_1632_1519 # TOPIC#:       323 # DATE:     920323 # TRANSCRIBER:  glp # UTT_CODER:    tc # DIFFICULTY:   1 # TOPICALITY:   3 # NATURALNESS:  2 # ECHO_FROM_B:  1 # ECHO_FROM_A:  4 # STATIC_ON_A:  1 # STATIC_ON_B:  1 # BACKGROUND_A: 1 # BACKGROUND_B: 2 # REMARKS:        None. #  # ========================================================================= #    #  # o          A.1 utt1: Okay.  / # qw          A.1 utt2: {D So, }    #  # qy^d          B.2 utt1: [ [ I guess, +    #  # +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /   #  # +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /   #  # qy          A.5 utt1: Does it say something? /   #  # sd          B.6 utt1: I think it usually does.  / o = \"Other\" qw = \"Wh-Question\" qy^d = \"Declarative Yes-No-Question\" + = \"Segment (multi-utterance)\" *x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*   FILENAME:   4325_1632_1519 TOPIC#:     323 DATE:       920323 TRANSCRIBER:    glp"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"tidy-the-data","dir":"Articles","previous_headings":"Concepts and strategies","what":"Tidy the data","title":"6. Organizing and documenting data","text":"many ways approach task tidying data general, semi-structured data particular. recipe, take step--step approach parsing semi-structured data one file apply process files corpus using custom function. begin reading one conversation files R character vector using read_lines() function readr package. isolate vector element contains document speaker ids, use str_subset() stringr package. function takes two arguments, string pattern, returns vector element matches pattern. case looking pattern matches three groups digits separated underscores. test pattern, can use str_view() function. use regular expression character class \\\\d digits + operator match 1 contiguous digits. separate three groups \\\\d+ underscores _. result \\\\d+_\\\\d+_\\\\d+. can see pattern matches line looking . Now can use pattern str_subset() return vector element contains pattern. Tip Regular Expressions powerful pattern matching syntax. used extensively text manipulation see . develop regular expressions, helpful tool allows interactively test pattern matching. stringr package handy function str_view() allows interactive pattern matching. good website practice Regular Expressions RegEx101. can also install regexplain package (Aden-Buie 2021) R get access useful RStudio Addin. next step extract three digit sequences correspond doc_id, speaker_a_id, speaker_b_id. First extract pattern identified str_extract() can break single character vector multiple parts based underscore _. str_split() function takes string pattern use split character vector. return list character vectors. list special object type R. unordered collection objects whose lengths can differ (contrast data frame collection objects whose lengths --hence tabular format). case list length 1, whose sole element character vector length 3 --one element per segment returned split. desired result cases pass multiple character vectors str_split() function want results conflated single character vector blurring distinction individual character vectors. case, however, want extract three elements character vector assign meaningful variable names. use unlist() function convert list single character vector. assign result speaker_info_chr. speaker_info_chr now character vector length three. subset elements assign meaningful variable names can conveniently use later tidying process. next step isolate text section extracting rest document. noted previously, sequence = separates header section text section. need index point character vector doc_chr line occurs subset doc_chr point end character vector. first find point = sequence occurs. use str_view() test pattern matches contiguous sequence =. file see one element matches element's index 31. Now important keep mind working single file swda/ data. Since plan use code apply files, need cautious create pattern may matched multiple times another document corpus. =+ pattern match =, ==, ===, etc. implausible believe might = character line one files. update regular expression avoid potential scenario matching sequences three =. case make use curly bracket operators {}. get result file, safeguard bit unlikely find multiple matches ===, ====, etc. extract just index match, can use str_which() function pattern. return index vector element matches pattern. However, consider . actually using index subset vector, need increment index 1 get next vector element. assign result text_start_index. index end text simply length doc_chr vector. can use length() function get index. now bookends, speak, text section. extract text subset doc_chr vector indices. text extra whitespace lines blank lines well. cleaning moving forward organize data. get rid whitespace use str_trim() function default remove leading trailing whitespace line. remove blank lines use str_subset() subset text vector. .+ pattern match elements blank. assign result text overwriting original text vector. first step towards tidy dataset now combine doc_id element text data frame, leaving aside speaker ids. use tibble() function pass variables named arguments. data now data frame, time parse text column extract damsl tags, speaker, speaker turn, utterance number, utterance text separate columns. make extensive use regular expressions. aim find consistent pattern distinguishes piece information text given row. best way learn regular expressions use . end included link interactive regular expression practice website regex101. Open site copy text 'TEST STRING' field. Figure 1: RegEx101 Now manually type following regular expressions 'REGULAR EXPRESSION' field one--one (separate line). Notice matched type finished typing. can find exactly component parts expression toggling top right icon window hovering mouse relevant parts expression. can now see, regular expressions match damsl tags, speaker speaker turn, utterance number, utterance text. apply expressions data extract information separate columns make use mutate() str_extract() functions. mutate() take data frame create new columns values match extract row data frame str_extract(). Tip Notice str_extract() different str_extract_all(). work mutate() row evaluated turn, therefore need make one match per row. chained steps code , dropping original text column select(-text), overwriting swda_df results. Warning One twist notice regular expressions R require double backslashes (\\\\) programming environments use single backslash (\\). couple things left columns extracted text move finishing tidy dataset. First, need separate speaker_turn column speaker turn_num columns second need remove unwanted characters damsl_tag, utterance_num, utterance_text columns. separate values column two columns use separate_wider_delim() function. takes column separate, delimiter use separate values, character vector names new columns create. remove unwanted leading trailing whitespace apply str_trim() function. removing characters matching character(s) replace empty string (\"\") str_replace() function. , chained functions together overwritten data results. round tidy dataset single conversation file connect speaker_a_id speaker_b_id speaker B current dataset adding new column speaker_id. case_when() function exactly : allows us map rows speaker value \"\" speaker_a_id rows value \"B\" speaker_b_id. now tidy dataset set create. dataset includes one conversation file! want apply code 1,155 conversation files swda/ corpus. approach create custom function groups code done single file iteratively send file corpus function combine results one data frame. custom function extra code print progress message file runs. sanity check run extract_swda_data() function conversation file just working make sure works expected. Looks good! now time create vector paths conversation files. ls_dif() function fs package interfaces OS file system return paths files specified directory. also add pattern match conversation files (regexp = \\\\.utt$) accidentally include files corpus. recurse set TRUE means get full path file. pass conversation file vector paths conversation files iteratively extract_swda_data() function use map_dfr(). apply function conversation file return data frame combine results single data frame. now see 223, 606 observations (individual utterances dataset). structure data frame matches idealized dataset Table 1. also good idea inspect data frame ensure data expected. One check missing values. can use skim() function skimr package get quick summary data frame. Another spot check data frame see values expected. working fairly large dataset, can use slice_sample() function dplyr package randomly sample subset rows data frame.","code":"# Read a single file as character vector doc_chr <-   read_lines(file = \"../data/original/swda/sw00utt/sw_0001_4325.utt\") # Test out a pattern doc_chr |>   str_view(pattern = \"\\\\d+_\\\\d+_\\\\d+\") ## [15] │ FILENAME:{\\t}<4325_1632_1519> # Isolate the vector element that contains the document and speaker ids str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") ## [1] \"FILENAME:\\t4325_1632_1519\" str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |> # isolate vector element   str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |> # extract the pattern   str_split(\"_\") # split the character vector by underscore ## [[1]] ## [1] \"4325\" \"1632\" \"1519\" speaker_info_chr <-   str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |>   str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |>   str_split(\"_\") |>   unlist() # convert the list to a character vector  # Preview speaker_info_chr ## [1] \"4325\" \"1632\" \"1519\" doc_id <- speaker_info_chr[1] # extract by index speaker_a_id <- speaker_info_chr[2] # extract by index speaker_b_id <- speaker_info_chr[3] # extract by index str_view(doc_chr, \"=+\") ## [31] │ <=========================================================================> str_view(doc_chr, \"={3,}\") ## [31] │ <=========================================================================> # Find where text starts text_start_index <- str_which(doc_chr, \"={3,}\") + 1 # Find where text ends text_end_index <- length(doc_chr) # Extract text between indices text <- doc_chr[text_start_index:text_end_index]  # Preview head(text) ## [1] \"  \"                                        ## [2] \"\"                                          ## [3] \"o          A.1 utt1: Okay.  /\"             ## [4] \"qw          A.1 utt2: {D So, }   \"         ## [5] \"\"                                          ## [6] \"qy^d          B.2 utt1: [ [ I guess, +   \" # Remove leading and trailing whitespace text <- str_trim(text)  # Preview head(text) ## [1] \"\"                                       ## [2] \"\"                                       ## [3] \"o          A.1 utt1: Okay.  /\"          ## [4] \"qw          A.1 utt2: {D So, }\"         ## [5] \"\"                                       ## [6] \"qy^d          B.2 utt1: [ [ I guess, +\" # Remove blank lines text <- str_subset(text, \".+\")  # Preview head(text) ## [1] \"o          A.1 utt1: Okay.  /\"                                                                   ## [2] \"qw          A.1 utt2: {D So, }\"                                                                  ## [3] \"qy^d          B.2 utt1: [ [ I guess, +\"                                                          ## [4] \"+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\" ## [5] \"+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\"                         ## [6] \"qy          A.5 utt1: Does it say something? /\" # Combine info and text into a data frame swda_df <- tibble(doc_id, text)  # Preview slice_head(swda_df, n = 5) ## # A tibble: 5 × 2 ##   doc_id text                                                                    ##   <chr>  <chr>                                                                   ## 1 4325   o          A.1 utt1: Okay.  /                                           ## 2 4325   qw          A.1 utt2: {D So, }                                          ## 3 4325   qy^d          B.2 utt1: [ [ I guess, +                                  ## 4 4325   +          A.3 utt1: What kind of experience [ do you, + do you ] have… ## 5 4325   +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. / o          A.1 utt1: Okay.  / qw          A.1 utt2: {D So, } qy^d          B.2 utt1: [ [ I guess, + +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? / +          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. / qy          A.5 utt1: Does it say something? / sd          B.6 utt1: I think it usually does.  / ad          B.6 utt2: You might try, {F uh, }  / h          B.6 utt3: I don't know,  / ad          B.6 utt4: hold it down a little longer,  / ^.+?\\s [AB]\\.\\d+ utt\\d+ :.+$ # Extract column information from `text` swda_df <-   swda_df |> # current dataset   mutate(damsl_tag = str_extract(text, \"^.+?\\\\s\")) |> # damsl tags   mutate(speaker_turn = str_extract(text, \"[AB]\\\\.\\\\d+\")) |> # speaker_turn pairs   mutate(utterance_num = str_extract(text, \"utt\\\\d+\")) |> # utterance number   mutate(utterance_text = str_extract(text, \":.+$\")) |> # utterance text   select(-text) # drop the `text` column  # Preview glimpse(swda_df) ## Rows: 159 ## Columns: 5 ## $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",… ## $ damsl_tag      <chr> \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", … ## $ speaker_turn   <chr> \"A.1\", \"A.1\", \"B.2\", \"A.3\", \"B.4\", \"A.5\", \"B.6\", \"B.6\",… ## $ utterance_num  <chr> \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",… ## $ utterance_text <chr> \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What… # Separate speaker_turn into distinct columns swda_df <-   swda_df |>   separate_wider_delim(     cols = speaker_turn,     delim = \".\",     names = c(\"speaker\", \"turn_num\")   )  # Preview glimpse(swda_df) ## Rows: 159 ## Columns: 6 ## $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",… ## $ damsl_tag      <chr> \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", … ## $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", … ## $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", … ## $ utterance_num  <chr> \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",… ## $ utterance_text <chr> \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What… # Clean up column information swda_df <-   swda_df |> # current dataset   mutate(damsl_tag = str_trim(damsl_tag)) |> # remove leading/ trailing whitespace   mutate(utterance_num = str_replace(utterance_num, \"utt\", \"\")) |> # remove 'utt'   mutate(utterance_text = str_replace(utterance_text, \":\\\\s\", \"\")) |> # remove ': '   mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace  # Preview glimpse(swda_df) ## Rows: 159 ## Columns: 6 ## $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",… ## $ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad… ## $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", … ## $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", … ## $ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", … ## $ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of… # Link speaker with speaker_id swda_df <-   swda_df |> # current dataset   mutate(speaker_id = case_when( # create speaker_id     speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A     speaker == \"B\" ~ speaker_b_id, # speaker_b_id value when B     TRUE ~ NA_character_ # NA otherwise   ))  # Preview glimpse(swda_df) ## Rows: 159 ## Columns: 7 ## $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",… ## $ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad… ## $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", … ## $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", … ## $ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", … ## $ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of… ## $ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",… # [ ] add to `qtalrkit` package, note the convention of `extract_` prefix for curation functions. In combination with `get_compressed_data()` this corpus can be curated with few steps.  extract_swda_data <- function(file) {   # Progress message   file_basename <- basename(file) # file name   message(\"Processing \", file_basename, \"\\n\")    # Read `file` by lines   doc_chr <- read_lines(file)    # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`   speaker_info_chr <-     str_subset(doc_chr, \"\\\\d+_\\\\d+_\\\\d+\") |>     str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |>     str_split(\"_\") |>     unlist()    doc_id <- speaker_info_chr[1]   speaker_a_id <- speaker_info_chr[2]   speaker_b_id <- speaker_info_chr[3]    # Extract `text`   text_start_index <- str_which(doc_chr, \"={3,}\") + 1   text_end_index <- length(doc_chr)    text <-     doc_chr[text_start_index:text_end_index] |>     str_trim() |>     str_subset(\".+\")    swda_df <- tibble(doc_id, text) # tidy format `doc_id` and `text`    # Extract column information from `text`   swda_df <-     swda_df |> # current dataset     mutate(damsl_tag = str_extract(text, \"^.+?\\\\s\")) |> # damsl tags     mutate(speaker_turn = str_extract(text, \"[AB]\\\\.\\\\d+\")) |> # speaker_turn pairs     mutate(utterance_num = str_extract(text, \"utt\\\\d+\")) |> # utterance number     mutate(utterance_text = str_extract(text, \":.+$\")) |> # utterance text     select(-text) # drop the `text` column    # Separate speaker_turn into distinct columns   swda_df <-     swda_df |> # current dataset     separate_wider_delim(       cols = speaker_turn,       delim = \".\",       names = c(\"speaker\", \"turn_num\")     )    # Clean up column information   swda_df <-     swda_df |> # current dataset     mutate(damsl_tag = str_trim(damsl_tag)) |> # remove leading/ trailing whitespace     mutate(utterance_num = str_replace(utterance_num, \"utt\", \"\")) |> # remove 'utt'     mutate(utterance_text = str_replace(utterance_text, \":\\\\s\", \"\")) |> # remove ': '     mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace    # Link speaker with speaker_id   swda_df <-     swda_df |> # current dataset     mutate(speaker_id = case_when( # create speaker_id       speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A       speaker == \"B\" ~ speaker_b_id # speaker_b_id value when B     ))    message(\"Processed \", file_basename, \"\\n\")   return(swda_df) } # Process a single file (test) extract_swda_data(   file = \"../data/original/swda/sw00utt/sw_0001_4325.utt\" ) |>   glimpse() ## Rows: 159 ## Columns: 7 ## $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",… ## $ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad… ## $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", … ## $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", … ## $ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", … ## $ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of… ## $ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",… # List all conversation files swda_files_chr <-   dir_ls(     path = \"../data/original/swda/\", # source directory     recurse = TRUE, # traverse all sub-directories     type = \"file\", # only return files     regexp = \"\\\\.utt$\"   ) # only return files ending in .utt  head(swda_files_chr) # preview file paths ../data/original/swda/sw00utt/sw_0001_4325.utt ../data/original/swda/sw00utt/sw_0002_4330.utt ../data/original/swda/sw00utt/sw_0003_4103.utt ../data/original/swda/sw00utt/sw_0004_4327.utt ../data/original/swda/sw00utt/sw_0005_4646.utt ../data/original/swda/sw00utt/sw_0006_4108.utt # Process all conversation files swda_df <-   swda_files_chr |> # pass file names   map_dfr(extract_swda_data) # read and tidy iteratively  # Preview glimpse(swda_df) ## Rows: 223,606 ## Columns: 7 ## $ doc_id         <chr> \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",… ## $ damsl_tag      <chr> \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad… ## $ speaker        <chr> \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", … ## $ turn_num       <chr> \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", … ## $ utterance_num  <chr> \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", … ## $ utterance_text <chr> \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of… ## $ speaker_id     <chr> \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"documentation","dir":"Articles","previous_headings":"Concepts and strategies","what":"Documentation","title":"6. Organizing and documenting data","text":"now tidy dataset, need document data curation process resulting dataset. script used curate data cleaned well documented prose code comments. need write dataset disk create data dictionary. make sure add curated dataset derived/ directory data dictionary close dataset. directory structure now looks like : data dictionary file contain information dataset variables values. file can created manually edited text editor spreadsheet software. alternatively, scaffolding CSV file can generated create_data_dictionary() function qtalrkit package.","code":"# Write to disk dir_create(path = \"../data/derived/swda/\") # create swda subdirectory  write_csv(swda_df,   file = \"../data/derived/swda/swda_curated.csv\" ) data/ ├── analysis/ ├── derived/ │   └── swda/ │       └── swda_curated.csv └── original/     └── swda/         ├── README         ├── doc/         ├── sw00utt/         ├── sw01utt/         ├── sw02utt/         ├── sw03utt/         ├── sw04utt/         ├── sw05utt/         ├── sw06utt/         ├── sw07utt/         ├── sw08utt/         ├── sw09utt/         ├── sw10utt/         ├── sw11utt/         ├── sw12utt/         └── sw13utt/ # Create data dictionary create_data_dictionary(   data = swda,   file_path = \"../data/derived/swda/swda_data_dictionary.csv\" )"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"6. Organizing and documenting data","text":"recipe, learned read parse semi-structured data, create custom function iterate collection files, combine results single dataset, document data curation process resulting dataset. skills used recipe include regular expressions, readr, dplyr, stringr, purrr packages, qtalrkit package documenting dataset.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"6. Organizing and documenting data","text":"first thing done data curation process know packages going useexplore data documentation understand resourceread data Rparse data tidy dataset. read_lines() function readr package read file R character vectordata framelistmatrix. TRUEFALSE separate_wider_delim() function tidyr package separate column two columns based delimiter (e.g. -, ., etc.). following functions stringr package return vector elements contain match pattern? str_subset()str_extract()str_replace()str_trim() map_dfr() function purrr package apply function element vector return listnested data framedata frame rows combineddata frame columns combined. data dictionary document describes data curation processdata analysis processdataset variables valuesdata visualization process.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-6.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"6. Organizing and documenting data","text":"beginning Lab 6, review ensure familiar following: Vector, data frame, list data structures Subsetting indexing vectors, data frames, lists Basic regular expressions character classes, quantifiers, anchors Reading, writing, manipulating files Creating employing custom functions lab, practice skills expand use readr, dplyr, stringr, purrr packages curate document dataset. choice data curate. start lab, consider data source like use, idealized structure curated dataset take, strategies likely employ curate dataset. also consider information need document data curation process.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-7.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"7. Transforming and documenting data","text":"curated dataset reflects tidy version original data. data relatively project-neutral. , project-specific changes often made bring data line research goals. may include modifying unit observation / adding additional attributes data. process may generate one new datasets used analysis. recipe, explore practical example transforming data. include operations : Text normalization tokenization Creating new variables splitting, merging, recoding existing variables Augmenting data additional variables sources resources Along way, employ variety tools techniques accomplish tasks. load packages need recipe. Lab 7, apply learned recipe new dataset.","code":"# Load packages library(readr) library(dplyr) library(stringr) library(tidyr) library(tidytext) library(qtalrkit)"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-7.html","id":"orientation","dir":"Articles","previous_headings":"Concepts and strategies","what":"Orientation","title":"7. Transforming and documenting data","text":"Curated datasets often project-neutral. , necessarily designed answer specific research question. Rather, designed flexible enough used variety projects. good thing, also means likely need transform data bring line research goals. can include normalizing text, modifying unit observation / adding additional attributes data. recipe, explore practical example transforming data. start curated dataset transform reflect specific research goal. dataset use MASC dataset (Ide et al. 2008). dataset contains collection words variety genres modalities American English. Tip Acquire tidy data MASC dataset curated version original data. data relatively project-neutral. like acquire original data curate use recipe, can running following code: starting point, assume curated dataset available data/derived/masc/ directory, seen . first step inspect data dictionary file. file contains information variables dataset. also good idea review data origin file, contains information original data source. Looking data dictionary, Table 1. Table 1: Data dictionary MASC dataset read data take glimpse . may also want summary overview dataset skimr package. give us sense data types number missing values. summary, dataset contains 591,097 observations 10 variables. unit observation word. variable names somewhat opaque, data dictionary provides context help us understand data. Now want consider plan use data analysis. assume want use data explore lexical variation MASC dataset across modalities genres. want transform data reflect goal. Table 2, see idealized version dataset like . Table 2:  Idealized version MASC dataset note, recipe derive single transformed dataset. projects, may want generate various datasets different units observations. depends research question research aim adopting.","code":"# Acquire the original data qtalrkit::get_compressed_data(   url = \"..\",   target_dir = \"data/original/masc/\" )  # Curate the data  # ... write a function and add it to the package data/ ├── analysis/ ├── derived/ │   ├── masc_curated_dd.csv │   ├── masc/ │   │   ├── masc_curated.csv ├── original/ │   ├── masc_do.csv │   ├── masc/ │   │   ├── ... # Read the data masc_curated <- read_csv(\"data/derived/masc/masc_curated.csv\")  # Preview glimpse(masc_curated) ## Rows: 591,097 ## Columns: 10 ## $ file   <chr> \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\"… ## $ ref    <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… ## $ base   <chr> \"december\", \"1998\", \"your\", \"contribution\", \"to\", \"goodwill\", \"… ## $ msd    <chr> \"NNP\", \"CD\", \"PRP$\", \"NN\", \"TO\", \"NNP\", \"MD\", \"VB\", \"JJR\", \"IN\"… ## $ string <chr> \"December\", \"1998\", \"Your\", \"contribution\", \"to\", \"Goodwill\", \"… ## $ title  <chr> \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\", \"110CYL067\"… ## $ source <chr> \"ICIC Corpus of Philanthropic Fundraising Discourse\", \"ICIC Cor… ## $ date   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ class  <chr> \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", \"WR LT\", … ## $ domain <chr> \"philanthropic fundraising discourse\", \"philanthropic fundraisi… ── Data Summary ───────────────────────                            Values Name                       masc_curated Number of rows             591097 Number of columns          10 _______________________ Column type frequency:   character                9   numeric                  1 ________________________ Group variables            None  ── Variable type: character ───────────   skim_variable n_missing complete_rate min max empty n_unique whitespace 1 file                  0         1       3  40     0      392          0 2 base                  4         1.00    1  99     0    28010          0 3 msd                   0         1       1   8     0       60          0 4 string               25         1.00    1  99     0    39474          0 5 title                 0         1       3 203     0      373          0 6 source             5732         0.990   3 139     0      348          0 7 date              94002         0.841   4  17     0       62          0 8 class                 0         1       5   5     0       18          0 9 domain            18165         0.969   4  35     0       21          0  ── Variable type: numeric ─────────────   skim_variable n_missing complete_rate  mean    sd p0 p25  p50  p75  p100 hist 1 ref                   0             1 3854. 4633.  0 549 2033 5455 24519 ▇▂▁▁▁"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-7.html","id":"transforming-data","dir":"Articles","previous_headings":"Concepts and strategies","what":"Transforming data","title":"7. Transforming and documenting data","text":"get curated dataset idealized dataset, need perform number transformations. transformations relatively straightforward, others require work. start easy ones. drop variables use time rename variables make intuitive. use select() function drop rename variables. good start structure. Next, split mod_gen variable two variables: modality genre. variable mod_gen contains two pieces information: modality genre (e.g., WR LT). information appears separated space. can make sure case tabulating values. count() function count number occurrences value variable, side effect summarize values variable can see unexpected values. Looks good, values separated space. can use separate_wider_delim() function tidyr package split variable two variables. use delim argument specify delimiter names argument specify names new variables. Create document id variable. Now variables want, can turn attention values variables. start doc_id variable. may good variable use document id. take look values, however, can see values informative. use distinct() function show unique values variable. also chain slice_sample() function randomly select sample values. give us sense values variable. can run code various times get different sample values. Since doc_id variable informative, replace variable's values numeric values. end, want digit unique document want words document grouped together. need group data doc_id generate new number group. can achieve passing data grouped doc_id (group_by()) mutate() function using cur_group_id() function generate number group. check, can apply count() function. 392 unique documents dataset. also can see word lengths vary quite bit. something need keep mind move forward analysis. Check values pos variable. pos variable contains part--speech tags word. PENN Treebank tagset used. take look values get familiar , also see unexpected values. use slice_sample() function randomly select sample values. give us sense values variable. running code times, can see many values expected. , however, unexpected values. particular, punctuation symbols tagged nouns. can get better appreciation unexpected values filtering data show non alpha-numeric values (^\\\\W+$) term column tabulating values term pos. can see sample PENN tagset documentation, punctuation tagged punctuation . example, period tagged . comma tagged ,. edit data reflect . look code, discuss . case_when() function allows us specify series conditions values. first condition term variable contains non alpha-numeric characters. , want replace value pos variable first character term variable, str_sub(term, start = 1, end = 1). condition met, want keep original value pos variable, TRUE ~ pos. can see code worked filtering data show non alpha-numeric values (^\\\\W+$) term column tabulating values term pos. completeness, also recode lemma values values well lemma can times multiple punctuation marks (e.g. !!!!!, ---, etc.) terms. Check values modality variable. modality variable contains modality tags document. take look values. tabulate values count(). see values SP WR, stand spoken written, respectively. make bit transparent, can recode values Spoken Written. use case_when() function . Check values genre variable. look values genre variable. genre labels definitely cryptic. data dictionary list labels verbose descriptions. However, looking original data's README, can find file (resource-headers.xml) lists genre labels. Now can use case_when() function. time see genre equal one genre labels , replace value verbose description. process transformation afterwards, good idea tabulate / visualize dataset. provides us opportunity get know dataset better also may help us identify inconsistencies like address transformation, least aware move towards analysis.    satisfied structure values dataset, can save file. use write_csv() function readr package . structure data/ directory project now look like :","code":"# Drop and rename variables masc_df <-   masc_curated |>   select(     doc_id = file,     term_num = ref,     term = string,     lemma = base,     pos = msd,     mod_gen = class   )  masc_df ## # A tibble: 591,097 × 6 ##    doc_id    term_num term         lemma        pos   mod_gen ##    <chr>        <dbl> <chr>        <chr>        <chr> <chr>   ##  1 110CYL067        0 December     december     NNP   WR LT   ##  2 110CYL067        1 1998         1998         CD    WR LT   ##  3 110CYL067        2 Your         your         PRP$  WR LT   ##  4 110CYL067        3 contribution contribution NN    WR LT   ##  5 110CYL067        4 to           to           TO    WR LT   ##  6 110CYL067        5 Goodwill     goodwill     NNP   WR LT   ##  7 110CYL067        6 will         will         MD    WR LT   ##  8 110CYL067        7 mean         mean         VB    WR LT   ##  9 110CYL067        8 more         more         JJR   WR LT   ## 10 110CYL067        9 than         than         IN    WR LT   ## # ℹ 591,087 more rows # Tabulate mod_gen masc_df |>   count(mod_gen) |>   arrange(-n) |>   print(n = Inf) ## # A tibble: 18 × 2 ##    mod_gen     n ##    <chr>   <int> ##  1 SP TR   71630 ##  2 WR EM   62036 ##  3 WR FC   38608 ##  4 WR ES   34938 ##  5 WR FT   34373 ##  6 WR BL   33278 ##  7 WR JO   33042 ##  8 WR JK   32420 ##  9 WR NP   31225 ## 10 SP MS   29879 ## 11 WR NF   29531 ## 12 WR TW   28128 ## 13 WR GV   27848 ## 14 WR TG   27624 ## 15 WR LT   26468 ## 16 SP FF   23871 ## 17 WR TC   19419 ## 18 SP TP    6779 # Split mod_gen into modality and genre masc_df <-   masc_df |>   separate_wider_delim(     cols = mod_gen,     delim = \" \",     names = c(\"modality\", \"genre\")   )  masc_df ## # A tibble: 591,097 × 7 ##    doc_id    term_num term         lemma        pos   modality genre ##    <chr>        <dbl> <chr>        <chr>        <chr> <chr>    <chr> ##  1 110CYL067        0 December     december     NNP   WR       LT    ##  2 110CYL067        1 1998         1998         CD    WR       LT    ##  3 110CYL067        2 Your         your         PRP$  WR       LT    ##  4 110CYL067        3 contribution contribution NN    WR       LT    ##  5 110CYL067        4 to           to           TO    WR       LT    ##  6 110CYL067        5 Goodwill     goodwill     NNP   WR       LT    ##  7 110CYL067        6 will         will         MD    WR       LT    ##  8 110CYL067        7 mean         mean         VB    WR       LT    ##  9 110CYL067        8 more         more         JJR   WR       LT    ## 10 110CYL067        9 than         than         IN    WR       LT    ## # ℹ 591,087 more rows # Preview doc_id masc_df |>   distinct(doc_id) |>   slice_sample(n = 10) ## # A tibble: 10 × 1 ##    doc_id              ##    <chr>               ##  1 JurassicParkIV-INT  ##  2 111367              ##  3 NYTnewswire6        ##  4 sw2014-ms98-a-trans ##  5 52713               ##  6 new_clients         ##  7 cable_spool_fort    ##  8 jokes10             ##  9 wsj_2465            ## 10 wsj_0158 # Recode doc_id masc_df <-   masc_df |>   group_by(doc_id) |>   mutate(doc_id = cur_group_id()) |>   ungroup()  masc_df ## # A tibble: 591,097 × 7 ##    doc_id term_num term         lemma        pos   modality genre ##     <int>    <dbl> <chr>        <chr>        <chr> <chr>    <chr> ##  1      1        0 December     december     NNP   WR       LT    ##  2      1        1 1998         1998         CD    WR       LT    ##  3      1        2 Your         your         PRP$  WR       LT    ##  4      1        3 contribution contribution NN    WR       LT    ##  5      1        4 to           to           TO    WR       LT    ##  6      1        5 Goodwill     goodwill     NNP   WR       LT    ##  7      1        6 will         will         MD    WR       LT    ##  8      1        7 mean         mean         VB    WR       LT    ##  9      1        8 more         more         JJR   WR       LT    ## 10      1        9 than         than         IN    WR       LT    ## # ℹ 591,087 more rows # Check masc_df |>   count(doc_id) |>   arrange(-n) |>   print(n = Inf) ## # A tibble: 392 × 2 ##     doc_id     n ##      <int> <int> ##   1    158 24520 ##   2    300 22261 ##   3    112 18459 ##   4    113 17986 ##   5    215 17302 ##   6    312 14752 ##   7    311 13376 ##   8    200 13138 ##   9    217 11753 ##  10    186 10665 ##  11    140 10064 ##  12    304  9417 ##  13    154  9277 ##  14     98  8959 ##  15    152  8862 ##  16    238  7617 ##  17    240  7580 ##  18    155  7361 ##  19     79  7217 ##  20     80  7032 ##  21    230  6931 ##  22    233  6887 ##  23     77  6850 ##  24     76  6839 ##  25    156  6664 ##  26     78  6435 ##  27    162  6251 ##  28    203  5665 ##  29    137  5654 ##  30    163  5330 ##  31    151  5155 ##  32    231  5059 ##  33    211  4656 ##  34    243  4291 ##  35     83  4269 ##  36    216  4263 ##  37     92  4148 ##  38     81  3874 ##  39    242  3785 ##  40    219  3752 ##  41    235  3745 ##  42    201  3713 ##  43    236  3514 ##  44    303  3481 ##  45    183  3353 ##  46    153  3340 ##  47    251  3295 ##  48    171  3198 ##  49    172  3151 ##  50    252  3072 ##  51    170  3054 ##  52    169  3038 ##  53    253  2848 ##  54    259  2841 ##  55     84  2768 ##  56    166  2730 ##  57    167  2691 ##  58    199  2562 ##  59     85  2557 ##  60     82  2470 ##  61    283  2387 ##  62    184  2349 ##  63    250  2290 ##  64    198  2246 ##  65    310  2242 ##  66    254  2151 ##  67    205  2147 ##  68    181  2099 ##  69    187  1978 ##  70    229  1938 ##  71    195  1804 ##  72    260  1734 ##  73     75  1697 ##  74    191  1696 ##  75    245  1694 ##  76    255  1682 ##  77    257  1613 ##  78    248  1594 ##  79    244  1539 ##  80    220  1468 ##  81    213  1451 ##  82    308  1443 ##  83    234  1441 ##  84    274  1433 ##  85    247  1426 ##  86    196  1414 ##  87    179  1371 ##  88    256  1364 ##  89    209  1354 ##  90    176  1336 ##  91    307  1326 ##  92    258  1275 ##  93    249  1271 ##  94    223  1269 ##  95    225  1240 ##  96    241  1203 ##  97    226  1171 ##  98    227  1144 ##  99    228  1131 ## 100    178  1103 ## 101    150  1102 ## 102    214  1102 ## 103    266  1092 ## 104    192  1063 ## 105    232  1030 ## 106    149  1023 ## 107    246  1015 ## 108    363  1013 ## 109    141  1004 ## 110    197   989 ## 111    177   982 ## 112    309   974 ## 113    164   941 ## 114    190   936 ## 115    353   932 ## 116    271   931 ## 117    286   925 ## 118    189   923 ## 119    193   904 ## 120    143   901 ## 121    139   898 ## 122    175   888 ## 123    298   885 ## 124    332   879 ## 125    391   869 ## 126    147   858 ## 127    297   851 ## 128     26   842 ## 129     86   842 ## 130    261   831 ## 131    212   815 ## 132     54   810 ## 133     12   809 ## 134    173   801 ## 135    146   797 ## 136     21   796 ## 137    221   794 ## 138    224   794 ## 139    306   794 ## 140     42   790 ## 141    145   783 ## 142    165   766 ## 143    126   760 ## 144    148   758 ## 145    138   754 ## 146     66   749 ## 147    355   748 ## 148    299   741 ## 149    392   741 ## 150     71   724 ## 151     30   707 ## 152    239   701 ## 153    185   692 ## 154    204   688 ## 155    174   652 ## 156      3   649 ## 157    180   646 ## 158    202   620 ## 159    390   620 ## 160    136   616 ## 161    121   612 ## 162    279   603 ## 163      1   596 ## 164     51   593 ## 165    305   575 ## 166    144   564 ## 167      9   533 ## 168    123   533 ## 169    124   533 ## 170      2   522 ## 171    119   510 ## 172    295   509 ## 173    280   506 ## 174    291   506 ## 175     11   495 ## 176    282   490 ## 177    194   484 ## 178    268   482 ## 179      5   481 ## 180     60   460 ## 181    301   459 ## 182     63   458 ## 183      7   454 ## 184    222   451 ## 185     20   446 ## 186    168   444 ## 187    288   441 ## 188    237   440 ## 189    354   438 ## 190     64   435 ## 191     93   435 ## 192    102   433 ## 193    125   425 ## 194     36   423 ## 195    114   420 ## 196    275   420 ## 197     58   417 ## 198    352   415 ## 199    374   412 ## 200    350   410 ## 201     62   407 ## 202    120   405 ## 203     70   403 ## 204    206   403 ## 205    294   402 ## 206    278   399 ## 207    210   395 ## 208    263   393 ## 209    292   390 ## 210     14   389 ## 211    105   380 ## 212     61   374 ## 213     10   364 ## 214    128   364 ## 215     89   362 ## 216    387   360 ## 217    342   359 ## 218    111   358 ## 219    129   358 ## 220     13   348 ## 221     34   348 ## 222     33   347 ## 223     38   344 ## 224    117   340 ## 225     91   337 ## 226     59   333 ## 227     16   332 ## 228    132   330 ## 229     65   323 ## 230    290   322 ## 231    218   314 ## 232     57   313 ## 233    389   313 ## 234     35   308 ## 235     37   307 ## 236     40   307 ## 237    281   306 ## 238     95   305 ## 239    296   302 ## 240     22   300 ## 241    108   299 ## 242     50   291 ## 243    388   291 ## 244     73   283 ## 245     68   281 ## 246    358   281 ## 247    207   279 ## 248    338   279 ## 249     46   275 ## 250     69   274 ## 251     90   268 ## 252    131   265 ## 253     67   262 ## 254     72   260 ## 255    130   260 ## 256    265   260 ## 257    135   259 ## 258     88   258 ## 259     94   257 ## 260    277   256 ## 261    115   253 ## 262    101   252 ## 263    357   250 ## 264    107   245 ## 265    262   245 ## 266    317   245 ## 267     96   243 ## 268     41   240 ## 269     45   238 ## 270    157   237 ## 271    293   237 ## 272    142   236 ## 273    349   230 ## 274    335   228 ## 275      4   224 ## 276    371   224 ## 277     29   223 ## 278    134   220 ## 279     39   218 ## 280    161   217 ## 281    375   217 ## 282    365   214 ## 283     43   213 ## 284    369   212 ## 285    315   211 ## 286    384   211 ## 287    104   210 ## 288    319   210 ## 289     49   206 ## 290    366   205 ## 291     87   203 ## 292     48   202 ## 293    346   202 ## 294    122   201 ## 295    351   201 ## 296    313   199 ## 297    343   198 ## 298    318   196 ## 299    285   195 ## 300    333   195 ## 301    364   194 ## 302    378   194 ## 303     15   193 ## 304    264   190 ## 305    373   190 ## 306    382   189 ## 307     55   187 ## 308    386   187 ## 309     74   185 ## 310    330   185 ## 311    109   184 ## 312     17   183 ## 313     53   177 ## 314    368   177 ## 315    287   176 ## 316     23   175 ## 317     56   175 ## 318    284   175 ## 319    289   175 ## 320    383   173 ## 321    380   172 ## 322    381   172 ## 323     19   169 ## 324    370   168 ## 325      6   167 ## 326    269   166 ## 327    345   163 ## 328    326   162 ## 329     18   160 ## 330    276   160 ## 331    116   159 ## 332    302   158 ## 333     99   157 ## 334    272   157 ## 335    103   155 ## 336    385   154 ## 337    372   153 ## 338    270   151 ## 339    379   150 ## 340     52   149 ## 341    182   149 ## 342    334   146 ## 343    376   146 ## 344     31   143 ## 345    377   143 ## 346    133   140 ## 347    361   138 ## 348    273   136 ## 349    325   135 ## 350    328   135 ## 351    127   134 ## 352    322   134 ## 353    336   134 ## 354     24   133 ## 355     47   132 ## 356    316   131 ## 357    327   131 ## 358     27   127 ## 359    340   127 ## 360    337   124 ## 361    320   122 ## 362    348   121 ## 363    331   120 ## 364    323   118 ## 365    362   116 ## 366    208   114 ## 367     32   113 ## 368     25   111 ## 369    339   111 ## 370    188   110 ## 371    367   110 ## 372    324   108 ## 373      8   106 ## 374    347   105 ## 375     44   104 ## 376    344   103 ## 377    360    99 ## 378    267    98 ## 379    341    97 ## 380    359    97 ## 381    160    95 ## 382     28    94 ## 383    159    93 ## 384    106    90 ## 385    118    90 ## 386    321    89 ## 387    329    86 ## 388    110    80 ## 389    100    71 ## 390    314    63 ## 391     97    52 ## 392    356    45 # Preview pos masc_df |>   slice_sample(n = 10) ## # A tibble: 10 × 7 ##    doc_id term_num term          lemma         pos   modality genre ##     <int>    <dbl> <chr>         <chr>         <chr> <chr>    <chr> ##  1    303     2511 proliferation proliferation NN    WR       TC    ##  2     76     5245 And           and           CC    WR       FT    ##  3    300    17170 DAVY          davy          NNP   SP       MS    ##  4     80     5341 ”             ”             NN    WR       FT    ##  5    171      900 .             .             .     WR       TG    ##  6    166     2588 out           out           RP    WR       BL    ##  7     67       58 organization  organization  NN    WR       LT    ##  8    216     2944 include       include       VB    WR       TG    ##  9    234     1304 donation      donation      NN    WR       LT    ## 10    231     3539 say           say           VB    WR       NF # Filter and tabulate masc_df |>   filter(str_detect(term, \"^\\\\W+$\")) |>   count(term, pos) |>   arrange(-n) |>   print(n = 20) ## # A tibble: 152 × 3 ##    term  pos       n ##    <chr> <chr> <int> ##  1 \",\"   ,     27112 ##  2 \".\"   .     26256 ##  3 \"\\\"\"  ''     5495 ##  4 \":\"   :      4938 ##  5 \"?\"   .      3002 ##  6 \")\"   )      2447 ##  7 \"(\"   (      2363 ##  8 \"-\"   :      1778 ##  9 \"!\"   .      1747 ## 10 \"/\"   NN     1494 ## 11 \"’\"   NN     1319 ## 12 \"-\"   -      1213 ## 13 \"”\"   NN     1076 ## 14 \"“\"   NN     1061 ## 15 \"]\"   NN     1003 ## 16 \"[\"   NN     1001 ## 17 \";\"   :       991 ## 18 \"--\"  :       772 ## 19 \">\"   NN      752 ## 20 \"...\" ...     716 ## # ℹ 132 more rows # Recode masc_df <-   masc_df |>   mutate(pos = case_when(     str_detect(term, \"^\\\\W+$\") ~ str_sub(term, start = 1, end = 1),     TRUE ~ pos   ))  # Check masc_df |>   filter(str_detect(term, \"^\\\\W+$\")) |> # preview   count(term, pos) |>   arrange(-n) |>   print(n = 20) ## # A tibble: 127 × 3 ##    term  pos       n ##    <chr> <chr> <int> ##  1 \",\"   \",\"   27113 ##  2 \".\"   \".\"   26257 ##  3 \"\\\"\"  \"\\\"\"   5502 ##  4 \":\"   \":\"    4939 ##  5 \"?\"   \"?\"    3002 ##  6 \"-\"   \"-\"    2994 ##  7 \")\"   \")\"    2447 ##  8 \"(\"   \"(\"    2363 ##  9 \"!\"   \"!\"    1747 ## 10 \"/\"   \"/\"    1495 ## 11 \"’\"   \"’\"    1325 ## 12 \"”\"   \"”\"    1092 ## 13 \"“\"   \"“\"    1078 ## 14 \"]\"   \"]\"    1003 ## 15 \"[\"   \"[\"    1001 ## 16 \";\"   \";\"     993 ## 17 \"--\"  \"-\"     772 ## 18 \">\"   \">\"     753 ## 19 \"...\" \".\"     747 ## 20 \"'\"   \"'\"     741 ## # ℹ 107 more rows # Recode masc_df <-   masc_df |>   mutate(lemma = case_when(     str_detect(term, \"^\\\\W+$\") ~ str_sub(term, start = 1, end = 1),     TRUE ~ lemma   ))  # Check masc_df |>   filter(str_detect(term, \"^\\\\W+$\")) |> # preview   count(term, lemma) |>   arrange(-n) |>   print(n = 20) ## # A tibble: 127 × 3 ##    term  lemma     n ##    <chr> <chr> <int> ##  1 \",\"   \",\"   27113 ##  2 \".\"   \".\"   26257 ##  3 \"\\\"\"  \"\\\"\"   5502 ##  4 \":\"   \":\"    4939 ##  5 \"?\"   \"?\"    3002 ##  6 \"-\"   \"-\"    2994 ##  7 \")\"   \")\"    2447 ##  8 \"(\"   \"(\"    2363 ##  9 \"!\"   \"!\"    1747 ## 10 \"/\"   \"/\"    1495 ## 11 \"’\"   \"’\"    1325 ## 12 \"”\"   \"”\"    1092 ## 13 \"“\"   \"“\"    1078 ## 14 \"]\"   \"]\"    1003 ## 15 \"[\"   \"[\"    1001 ## 16 \";\"   \";\"     993 ## 17 \"--\"  \"-\"     772 ## 18 \">\"   \">\"     753 ## 19 \"...\" \".\"     747 ## 20 \"'\"   \"'\"     741 ## # ℹ 107 more rows # Tabulate modality masc_df |>   count(modality) ## # A tibble: 2 × 2 ##   modality      n ##   <chr>     <int> ## 1 SP       132159 ## 2 WR       458938 # Recode modality masc_df <-   masc_df |>   mutate(     modality = case_when(       modality == \"SP\" ~ \"Spoken\",       modality == \"WR\" ~ \"Written\"     )   )  masc_df ## # A tibble: 591,097 × 7 ##    doc_id term_num term         lemma        pos   modality genre ##     <int>    <dbl> <chr>        <chr>        <chr> <chr>    <chr> ##  1      1        0 December     december     NNP   Written  LT    ##  2      1        1 1998         1998         CD    Written  LT    ##  3      1        2 Your         your         PRP$  Written  LT    ##  4      1        3 contribution contribution NN    Written  LT    ##  5      1        4 to           to           TO    Written  LT    ##  6      1        5 Goodwill     goodwill     NNP   Written  LT    ##  7      1        6 will         will         MD    Written  LT    ##  8      1        7 mean         mean         VB    Written  LT    ##  9      1        8 more         more         JJR   Written  LT    ## 10      1        9 than         than         IN    Written  LT    ## # ℹ 591,087 more rows # Tabulate genre masc_df |>   count(genre) |>   print(n = Inf) ## # A tibble: 18 × 2 ##    genre     n ##    <chr> <int> ##  1 BL    33278 ##  2 EM    62036 ##  3 ES    34938 ##  4 FC    38608 ##  5 FF    23871 ##  6 FT    34373 ##  7 GV    27848 ##  8 JK    32420 ##  9 JO    33042 ## 10 LT    26468 ## 11 MS    29879 ## 12 NF    29531 ## 13 NP    31225 ## 14 TC    19419 ## 15 TG    27624 ## 16 TP     6779 ## 17 TR    71630 ## 18 TW    28128 1. 'BL' for blog 2. 'NP' is newspaper 3. 'EM' is email 4. 'ES' is essay 5. 'FT' is fictlets 6. 'FC' is fiction 7. 'GV' is government 8. 'JK' is jokes 9. 'JO' is journal 10. 'LT' is letters 11. 'MS' is movie script 12. 'NF' is non-fiction 13. 'FF' is face-to-face 14. 'TC' is technical 15. 'TG' is travel guide 16. 'TP' is telephone 17. 'TR' is transcript 18. 'TW' is twitter # Recode genre masc_df <-   masc_df |>   mutate(     genre = case_when(       genre == \"BL\" ~ \"Blog\",       genre == \"NP\" ~ \"Newspaper\",       genre == \"EM\" ~ \"Email\",       genre == \"ES\" ~ \"Essay\",       genre == \"FT\" ~ \"Fictlets\",       genre == \"FC\" ~ \"Fiction\",       genre == \"GV\" ~ \"Government\",       genre == \"JK\" ~ \"Jokes\",       genre == \"JO\" ~ \"Journal\",       genre == \"LT\" ~ \"Letters\",       genre == \"MS\" ~ \"Movie script\",       genre == \"NF\" ~ \"Non-fiction\",       genre == \"FF\" ~ \"Face-to-face\",       genre == \"TC\" ~ \"Technical\",       genre == \"TG\" ~ \"Travel guide\",       genre == \"TP\" ~ \"Telephone\",       genre == \"TR\" ~ \"Transcript\",       genre == \"TW\" ~ \"Twitter\"     )   )  masc_df ## # A tibble: 591,097 × 7 ##    doc_id term_num term         lemma        pos   modality genre   ##     <int>    <dbl> <chr>        <chr>        <chr> <chr>    <chr>   ##  1      1        0 December     december     NNP   Written  Letters ##  2      1        1 1998         1998         CD    Written  Letters ##  3      1        2 Your         your         PRP$  Written  Letters ##  4      1        3 contribution contribution NN    Written  Letters ##  5      1        4 to           to           TO    Written  Letters ##  6      1        5 Goodwill     goodwill     NNP   Written  Letters ##  7      1        6 will         will         MD    Written  Letters ##  8      1        7 mean         mean         VB    Written  Letters ##  9      1        8 more         more         JJR   Written  Letters ## 10      1        9 than         than         IN    Written  Letters ## # ℹ 591,087 more rows # How many documents are in each modality? masc_df |>   distinct(doc_id, modality) |>   count(modality) |>   arrange(-n) ## # A tibble: 2 × 2 ##   modality     n ##   <chr>    <int> ## 1 Written    371 ## 2 Spoken      21 # How many documents are in each genre? masc_df |>   distinct(doc_id, genre) |>   count(genre) |>   arrange(-n) ## # A tibble: 18 × 2 ##    genre            n ##    <chr>        <int> ##  1 Email          174 ##  2 Newspaper       54 ##  3 Letters         49 ##  4 Blog            21 ##  5 Jokes           16 ##  6 Journal         12 ##  7 Essay            8 ##  8 Fiction          7 ##  9 Travel guide     7 ## 10 Face-to-face     6 ## # ℹ 8 more rows # What is the averge length of documents (in words)? masc_df |>   group_by(doc_id) |>   summarize(n = n()) |>   summarize(     mean = mean(n),     median = median(n),     min = min(n),     max = max(n)   ) ## # A tibble: 1 × 4 ##    mean median   min   max ##   <dbl>  <dbl> <int> <int> ## 1 1508.   418.    45 24520 masc_df |>   group_by(doc_id) |>   summarize(n = n()) |>   ggplot(aes(x = n)) +   geom_density() # What is the distribution of the length of documents by modality? masc_df |>   group_by(doc_id, modality) |>   summarize(n = n()) |>   ggplot(aes(x = n, fill = modality)) +   geom_density(alpha = 0.5) # What is the distribution of the length of documents by genre? masc_df |>   group_by(doc_id, modality, genre) |>   summarize(n = n()) |>   ggplot(aes(x = genre, y = n)) +   geom_boxplot() +   facet_wrap(~ modality, scales = \"free_x\") +   theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Save the data write_csv(masc_df, \"data/derived/masc/masc_transformed.csv\") data/ ├── analysis/ ├── derived/ │   ├── masc_curated_dd.csv │   ├── masc/ │   │   ├── masc_curated.csv │   │   ├── masc_transformed.csv ├── original/"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-7.html","id":"documenting-data","dir":"Articles","previous_headings":"Concepts and strategies","what":"Documenting data","title":"7. Transforming and documenting data","text":"last step document process resulting dataset(s). particular case derived one transformed dataset. documentation steps curation step. organize document process file (often .qmd file) create data dictionary transformed datasets. create_data_dictionary() function can come handy scaffolding data dictionary file.","code":"# Create a data dictionary create_data_dictionary(   data = masc_df,   file_path = \"data/derived/masc/masc_transformed_dd.csv\" )"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-7.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"7. Transforming and documenting data","text":"recipe, looked example transforming curated dataset. recipe included operations : Text normalization Variable recoding Splitting variables projects, transformation steps inevitably differ, strategies commonly necessary almost project. Just steps data preparation process, important document transformation steps. help others understand process resulting dataset(s).","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-7.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"7. Transforming and documenting data","text":"function use remove duplicate rows dataset? group_by()mutate()distinct()filter() TRUEFALSE str_c() function stringr package used separate strings rather combine . TRUEFALSE count() function dplyr package used tabulate values variable. want recode age learners categories \"child\", \"teen\", \"adult\" based age, function use? mutate()case_when()unite()separate_wider_delim() normalize text removing leading trailing whitespace, use () function stringr package. normalize text converting characters lowercase, use () function stringr package.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-7.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"7. Transforming and documenting data","text":"preparation Lab 7, review ensure comfortable following: Vector, data frame, list data structures Subsetting filtering data structures without regular expressions Reshaping datasets rows columns lab, practice skills expand knowledge data preparation transforming documenting data Tidyverse packages dplyr, tidyr, stringr. choice dataset transform. start lab, consider dataset like use, idealized structure transformed dataset take, strategies likely employ transform dataset. also consider information need document data transformation process.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"8. Employing exploratory methods","text":"Exploratory analysis wide-ranging term encompasses many different methods. recipe, focus methods commonly used analysis textual data. include frequency distributional analysis, clustering, word embedding models. approach exploratory analysis highly iterative. process linear, rather cycle steps Table 1. cycle repeated research question(s) addressed. Table 1:  exploratory analysis workflow model explore iteratively using output one method inform next ultimately address research question. reason, subsequent sections recipe grouped research question rather approach step method. get started loading packages likely use. Lab 8, try hand exploratory analysis using another dataset.","code":"library(dplyr)      # for data manipulation library(stringr)    # for string manipulation library(tidyr)      # for data tidying library(tidytext)   # for text analysis library(ggplot2)    # for data visualization"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"orientation","dir":"Articles","previous_headings":"Concepts and strategies","what":"Orientation","title":"8. Employing exploratory methods","text":"use SOTU corpus demonstrate different methods. select subset corpus (post-1945) explore question: language SOTU changed time? include methods frequency distributional analysis, dimensionality reduction, word embedding models. look first rows data get sense . can see dataset contains president, year, party, address SOTU address. Now view statistical overview summary data using skim() function. can see dataset contains 73 rows --corresponding number SOTU addresses. Looking missing values, can see missing values variables. Taking closer look variable, president variable character vector 12 unique presidents. two unique parties 73 unique addresses. year variable numeric minimum value 1947 maximum value 2020, therefore addresses include 73 years.","code":"sotu_df # # A tibble: 73 × 4 #    president   year party      address                                           #    <chr>      <dbl> <chr>      <chr>                                             #  1 Truman      1947 Democratic \"Mr. President, Mr. Speaker, Members of the Cong… #  2 Truman      1948 Democratic \"Mr. President, Mr. Speaker, and Members of the … #  3 Truman      1949 Democratic \"Mr. President, Mr. Speaker, Members of the Cong… #  4 Truman      1950 Democratic \"Mr. President, Mr. Speaker, Members of the Cong… #  5 Truman      1951 Democratic \"Mr. President, Mr. Speaker, Members of the Cong… #  6 Truman      1952 Democratic \"Mr. President, Mr. Speaker, Members of the Cong… #  7 Eisenhower  1953 Republican \"Mr. President, Mr. Speaker, Members of the Eigh… #  8 Eisenhower  1954 Republican \"Mr. President, Mr. Speaker, Members of the Eigh… #  9 Eisenhower  1955 Republican \"Mr. President, Mr. Speaker, Members of the Cong… # 10 Eisenhower  1956 Republican \"My Fellow Citizens: This morning I sent to the … # # ℹ 63 more rows skimr::skim(sotu_df) # ── Data Summary ──────────────────────── #                            Values  # Name                       sotu_df # Number of rows             73      # Number of columns          4       # _______________________            # Column type frequency:             #   character                3       #   numeric                  1       # ________________________           # Group variables            None    #  # ── Variable type: character ──────────────────────────────────────────────────── #   skim_variable n_missing complete_rate  min   max empty n_unique whitespace # 1 president             0             1    4    10     0       12          0 # 2 party                 0             1   10    10     0        2          0 # 3 address               0             1 6160 51947     0       73          0 #  # ── Variable type: numeric ────────────────────────────────────────────────────── #   skim_variable n_missing complete_rate  mean   sd   p0  p25  p50  p75 p100 # 1 year                  0             1 1984. 21.6 1947 1965 1984 2002 2020 #   hist  # 1 ▇▇▇▇▇"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"identify","dir":"Articles","previous_headings":"Concepts and strategies","what":"Identify","title":"8. Employing exploratory methods","text":"general sense data, can now move first step exploratory analysis workflow: identifying variables interest. linguistic variables might interest address question? Words might obvious variable, might also interested linguistic variables parts speech, syntactic structures, combination . start words. look words, might interested frequency words, distribution words, meaning words. might also interested relationship words. example, might interested co-occurrence words similarity words. , , possible approaches might consider. Another consideration want comparisons across time, across presidents, across parties, etc.. research question, already identified want compare across time focus. However, mean \"time\" clear. mean across years, across decades, across presidencies, etc.? need make decision want define time, can fold exploratory analysis, see . posit following sub-questions: frequent words across time periods? distribution words change across time periods? meaning words change across time periods? use sub-questions guide exploratory analysis.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"inspect","dir":"Articles","previous_headings":"Concepts and strategies","what":"Inspect","title":"8. Employing exploratory methods","text":"next step inspect data. transform data necessary prepare analysis diagnostic checks make sure data ready analysis. Since working words, tokenize addresses variable extract words maintain tidy dataset. use unnest_tokens() function tidytext package . apply function assign result new variable called sotu_words_df. continue looking whether relationship number words years. can using count() function year variable. group count number observations (words) per year n. can visualize line plot x-axis year y-axis number words n. add plot variable called p can add layers . Figure 1: Number words per year Tip count() function wrapper summarize() function. convenient way count number observations dataset. difference count() grouping added removed automatically. cases, can mimic behavior operations inside summarize() mutate() function using .argument. example: can see Figure 1 number words per year varies, sometimes quite bit. get sense relationship number words year, can add linear trend line plot. can adding geom_smooth() function plot. set method argument \"lm\" use linear model.  plot shows positive relationship number words year --number words increases time. line great fit furthermore angle line horizontal vertical. suggests relationship strong one. can confirm calculating correlation number words year. can using cor.test() function year n variables inside summarize() function. , even though working inspect data, already finding. number words increases time, despite fact relationship strong one. Now, turn attention frequency individual words. start looking frequent words entire corpus. can grouping word variable summarizing number times word occurs. arrange data descending order number times word occurs. usual suspects top list. surprising, likely case corpora, sizeable subcorpora --case time periods. address . use stopwords list just eliminate many common words, might bit agressive likely lose words want keep. Considering nuanced approach, use \\(tf\\)-\\(idf\\) transformation attenuate effect words common across time periods, flip side, promote effect words distinctive time period. order , need calculate \\(tf\\)-\\(idf\\) word time period. keep things simple, calculate \\(tf\\)-\\(idf\\) word decade. creating new variable called decade year rounded nearest decade. can group decade variable count number times word occurs. calculate \\(tf\\)-\\(idf\\) word decade using bind_tf_idf() function tidytext package. arrange data descending order \\(tf\\)-\\(idf\\) value. OK. Even preview shows getting interesting list words. look top 10 words decade. group decade slice top 10 words \\(tf\\)-\\(idf\\) value slice_max(). use reorder_within() function tidytext package reorder words within facet \\(tf\\)-\\(idf\\) value. visualize bar chart word x-axis height bar \\(tf\\)-\\(idf\\) value. also facet plot decade. flipped coordinates words y-axis bars horizontal. personal preference, find easier read words way.  Scanning top words decades can see words signal contemporary issues. hint picking changes language SOTU time. Dive deeper plot makes use reorder_within() scale_x_reordered() functions tidytext package. functions allow us reorder words within facet \\(tf\\)-\\(idf\\) value. nice way visualize distinctive words decade. advanced functions. interested learning , can read help documentation ?tidytext::reorder_within eye, however, 1940s 2020s seem jump way. take closer look 1940s 2020s original dataset. Well, explains . 3 addresses 1940s 1 address 2020s. enough data get good sense language SOTU decades. remove decades original dataset representative language SOTU. Another consideration catches eye looking top words decade words like \"communist\" \"communists\" counted separately. fine, want count word? can lemmatizing words --reducing words root form. can using lemmatize_words() function textstem package. consider example: Dive deeper default, lemmatize_words() function uses lookup table English lemmatize words. simple approach works well many cases. However, perfect. example, lemmatize words lookup table. want lemmatize words lookup table, want lemmatize words another language, can create add lookup table. can read help documentation ?textstem::lemmatize_words(). resource lemma lookup tables can found https://github.com/michmech/lemmatization-lists. considerations mind, update sotu_df dataset remove 1940s 2020s, tokenize lemmatize words, add decade variable. inspection process go . continue inspect data make changes dataset often case process analysis often run issues require us go back make changes dataset. move next step exploratory analysis workflow.","code":"# Tokenize the words by year (with numbers removed) sotu_words_df <-   sotu_df |>   unnest_tokens(word, address, strip_numeric = TRUE) # Get the number of words per year -------------------------------- p <-   sotu_words_df |>   count(year) |>   ggplot(aes(x = year, y = n)) +   geom_line()  # View p # the output of df |>   count(var1)  # is equivalent to df |>   group_by(var1) |>   summarize(n = n()) |>   ungroup() df |>   summarize(n = n(), .by = var1) p + geom_smooth(method = \"lm\") # Get the correlation between the number of words and the year ---- sotu_words_df |>   count(year) |>   summarize(cor = cor.test(year, n)$estimate) ## # A tibble: 1 × 1 ##     cor ##   <dbl> ## 1 0.342 # Get the most frequent words ------------------------------------  sotu_words_df |>   count(word, sort = TRUE) |>   slice_head(n = 10) ## # A tibble: 10 × 2 ##    word      n ##    <chr> <int> ##  1 the   21565 ##  2 and   14433 ##  3 to    13679 ##  4 of    13087 ##  5 in     8116 ##  6 we     7327 ##  7 a      7137 ##  8 our    6822 ##  9 that   5391 ## 10 for    4513 # Get the tf-idf of words by decade ------------------------------  sotu_words_tfidf_df <-   sotu_words_df |>   mutate(decade = floor(year / 10) * 10) |>   count(decade, word) |>   bind_tf_idf(word, decade, n) |>   arrange(decade, desc(tf_idf))  # Preview sotu_words_tfidf_df ## # A tibble: 40,073 × 6 ##    decade word               n       tf   idf   tf_idf ##     <dbl> <chr>          <int>    <dbl> <dbl>    <dbl> ##  1   1940 boycotts           5 0.000344 2.20  0.000756 ##  2   1940 jurisdictional     7 0.000482 1.50  0.000725 ##  3   1940 interpretation     4 0.000275 2.20  0.000605 ##  4   1940 unjustified        4 0.000275 2.20  0.000605 ##  5   1940 insecurity         5 0.000344 1.50  0.000518 ##  6   1940 arbitration        3 0.000207 2.20  0.000454 ##  7   1940 output             6 0.000413 1.10  0.000454 ##  8   1940 unjustifiable      3 0.000207 2.20  0.000454 ##  9   1940 management        25 0.00172  0.251 0.000433 ## 10   1940 rental             4 0.000275 1.50  0.000414 ## # ℹ 40,063 more rows # Visualize the top 10 words by decade ---------------------------------  sotu_words_tfidf_df |>   group_by(decade) |>   slice_max(n = 10, tf_idf) |>   ungroup() |>   mutate(decade = as.character(decade)) |>   mutate(word = reorder_within(word, desc(tf_idf), decade)) |>   ggplot(aes(word, tf_idf, fill = decade)) +   geom_col() +   scale_x_reordered() +   facet_wrap(~decade, scales = \"free\") +   theme(legend.position = \"none\") +   coord_flip() sotu_df |>   filter(year < 1950 | year >= 2020) ## # A tibble: 4 × 4 ##   president  year party      address                                             ##   <chr>     <dbl> <chr>      <chr>                                               ## 1 Truman     1947 Democratic \"Mr. President, Mr. Speaker, Members of the Congre… ## 2 Truman     1948 Democratic \"Mr. President, Mr. Speaker, and Members of the 80… ## 3 Truman     1949 Democratic \"Mr. President, Mr. Speaker, Members of the Congre… ## 4 Trump      2020 Republican \"Madam Speaker, Mr. Vice President, Members of Con… # Lemmatize the words -------------------------------------------- words_chr <- c(\"freedom\", \"free\", \"frees\", \"freeing\", \"freed\") textstem::lemmatize_words(words_chr) ## [1] \"freedom\" \"free\"    \"free\"    \"free\"    \"free\" # Update the dataset ---------------------------------------------- sotu_terms_df <-   sotu_df |>   filter(year >= 1950 & year < 2020) |> # Remove the 1940s and 2020s   unnest_tokens(word, address, strip_numeric = TRUE) |> # Tokenize the words   mutate(lemma = textstem::lemmatize_words(word)) |> # Lemmatize the words   mutate(decade = floor(year / 10) * 10) |> # Add a decade variable   select(president, decade, year, party, word, lemma) #  # Preview sotu_terms_df ## # A tibble: 368,586 × 6 ##    president decade  year party      word      lemma     ##    <chr>      <dbl> <dbl> <chr>      <chr>     <chr>     ##  1 Truman      1950  1950 Democratic mr        mr        ##  2 Truman      1950  1950 Democratic president president ##  3 Truman      1950  1950 Democratic mr        mr        ##  4 Truman      1950  1950 Democratic speaker   speaker   ##  5 Truman      1950  1950 Democratic members   member    ##  6 Truman      1950  1950 Democratic of        of        ##  7 Truman      1950  1950 Democratic the       the       ##  8 Truman      1950  1950 Democratic congress  congress  ##  9 Truman      1950  1950 Democratic a         a         ## 10 Truman      1950  1950 Democratic year      year      ## # ℹ 368,576 more rows"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"interrogate","dir":"Articles","previous_headings":"Concepts and strategies","what":"Interrogate","title":"8. Employing exploratory methods","text":"Now data tidy format, can move next step exploratory analysis workflow: interrogating data. submit selected variables descriptive unsupervised learning methods provide quantitative measures evaluate.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"frequency","dir":"Articles","previous_headings":"Concepts and strategies > Interrogate","what":"Frequency","title":"8. Employing exploratory methods","text":"frequent words across time periods? already made progress question inspection phase, now can updated dataset. Now can visualize top 10 lemmas decade, , lemmas instead words seven full decades.","code":"# Get the most frequent lemmas by decade --------------------------  sotu_lemmas_tfidf_df <-   sotu_terms_df |>   count(decade, lemma) |> # Count the lemmas by decade   bind_tf_idf(lemma, decade, n) |> # Calculate the tf-idf   arrange(decade, desc(tf_idf))  # Preview sotu_lemmas_tfidf_df ## # A tibble: 26,435 × 6 ##    decade lemma            n       tf   idf   tf_idf ##     <dbl> <chr>        <int>    <dbl> <dbl>    <dbl> ##  1   1950 armament        16 0.000321 1.95  0.000625 ##  2   1950 imperialism      9 0.000181 1.95  0.000351 ##  3   1950 shall          107 0.00215  0.154 0.000331 ##  4   1950 disarmament     13 0.000261 1.25  0.000327 ##  5   1950 mineral          8 0.000160 1.95  0.000312 ##  6   1950 mobilization    12 0.000241 1.25  0.000302 ##  7   1950 survivor        12 0.000241 1.25  0.000302 ##  8   1950 expenditure     42 0.000842 0.336 0.000283 ##  9   1950 adequate        25 0.000501 0.560 0.000281 ## 10   1950 constantly      11 0.000221 1.25  0.000276 ## # ℹ 26,425 more rows # Visualize the top 10 lemmas by decade --------------------------- sotu_lemmas_tfidf_df |>   group_by(decade) |>   slice_max(n = 10, tf_idf) |>   ungroup() |>   mutate(decade = as.character(decade)) |>   mutate(lemma = reorder_within(lemma, desc(tf_idf), decade)) |>   ggplot(aes(lemma, tf_idf, fill = decade)) +   geom_col() +   scale_x_reordered() +   facet_wrap(~decade, scales = \"free\") +   theme(legend.position = \"none\") +   coord_flip()"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"distribution","dir":"Articles","previous_headings":"Concepts and strategies > Interrogate","what":"Distribution","title":"8. Employing exploratory methods","text":"distribution words change across time periods? OK. Now focus word frequency distributions time. return sotu_terms_df. want get sense word distributions change time. need calculate frequency words year, first . need go back sotu_terms_df dataset group year word count number times word occurs. Since lose lemma variable process, add back \\(tf\\)-\\(idf\\) transformation using mutate() function textstem::lemmatize_words() function. format, can visualize distinctiveness words time. need filter data lemmas interested first. just start random poltical-oriented words. Figure 2: Distinctiveness political words time can see Figure 2 distinctiveness lemmas varies time. Now, plot used small span value geom_smooth() function get sense fine-grained changes time. However, may fine-grained. can adjust increasing span value. try span value 0.5. Figure 3: Distinctiveness political words time Figure 3 seems picking general word usage trends time. Another thing note way plotted data used facet_wrap() function create separate plot word used scales = \"free_y\" allowing y-axis vary plot. means comparing y-axis values across plots thus can say anything differing magnitudes visual inspection. address , can remove scales = \"free_y\" argument use default fix x- y-axis scales across plots. Figure 4: Distinctiveness political words time Figure 4, can clearly see magnitude words \"crime\" \"terror\" much higher words happend select. Furthermore, words interesting patterns. particular, \"terror\" two peaks around 1980 another around turn century. \"Crime\" also two distinctive peaks 1970s one 1990s. terms selected somewhat arbitrary. can identify words changed drastically years data ? can creating term-document matrix calculating standard deviation \\(tf\\)-\\(idf\\) values word. give us sense words changed time. calculate standard deviation \\(tf\\)-\\(idf\\) values word, can use apply() function iterate row matrix calculate standard deviation values row. can think apply() function cousin map() function. apply() function iterates rows columns matrix data frame applies function row column. choose whether function applied rows columns MARGIN argument. can set MARGIN = 1 apply function rows MARGIN = 2 apply function columns. Now can choose top words changed time. another selection words based standard deviation \\(tf\\)-\\(idf\\) values. Figure 5: Distinctiveness words time can see words selected based standard deviation can either increase, decrease, fluctuate time.","code":"sotu_terms_df ## # A tibble: 368,586 × 6 ##    president decade  year party      word      lemma     ##    <chr>      <dbl> <dbl> <chr>      <chr>     <chr>     ##  1 Truman      1950  1950 Democratic mr        mr        ##  2 Truman      1950  1950 Democratic president president ##  3 Truman      1950  1950 Democratic mr        mr        ##  4 Truman      1950  1950 Democratic speaker   speaker   ##  5 Truman      1950  1950 Democratic members   member    ##  6 Truman      1950  1950 Democratic of        of        ##  7 Truman      1950  1950 Democratic the       the       ##  8 Truman      1950  1950 Democratic congress  congress  ##  9 Truman      1950  1950 Democratic a         a         ## 10 Truman      1950  1950 Democratic year      year      ## # ℹ 368,576 more rows sotu_terms_tfidf_df <-   sotu_terms_df |>   count(year, word) |> # Count the words by year   bind_tf_idf(word, year, n) |> # Calculate the tf-idf   mutate(lemma = textstem::lemmatize_words(word)) |> # Lemmatize the words   arrange(year, desc(tf_idf))  # Preview sotu_terms_tfidf_df ## # A tibble: 97,213 × 7 ##     year word               n       tf   idf  tf_idf lemma          ##    <dbl> <chr>          <int>    <dbl> <dbl>   <dbl> <chr>          ##  1  1950 enjoyment          3 0.000585 3.54  0.00207 enjoyment      ##  2  1950 rent               3 0.000585 2.85  0.00167 rend           ##  3  1950 widespread         3 0.000585 2.85  0.00167 widespread     ##  4  1950 underdeveloped     2 0.000390 4.23  0.00165 underdeveloped ##  5  1950 ideals             7 0.00136  1.14  0.00156 ideal          ##  6  1950 transmit           3 0.000585 2.62  0.00153 transmit       ##  7  1950 peoples            8 0.00156  0.976 0.00152 people         ##  8  1950 democratic        12 0.00234  0.623 0.00146 democratic     ##  9  1950 expenditures       7 0.00136  1.06  0.00144 expenditure    ## 10  1950 businessmen        3 0.000585 2.44  0.00143 businessman    ## # ℹ 97,203 more rows plot_terms <- c(\"crime\", \"law\", \"free\", \"terror\", \"family\", \"government\")  sotu_terms_tfidf_df |>   filter(lemma %in% plot_terms) |>   ggplot(aes(year, tf_idf)) +   geom_smooth(se = FALSE, span = 0.25) +   facet_wrap(~lemma, scales = \"free_y\") sotu_terms_tfidf_df |>   filter(lemma %in% plot_terms) |>   ggplot(aes(year, tf_idf)) +   geom_smooth(se = FALSE, span = 0.5) +   facet_wrap(~lemma, scales = \"free_y\") sotu_terms_tfidf_df |>   filter(lemma %in% plot_terms) |>   ggplot(aes(year, tf_idf)) +   geom_smooth(se = FALSE, span = 0.5) +   facet_wrap(~lemma) # Create TDM with words-year and tf-idf values sotu_word_tfidf_mat <-   sotu_terms_tfidf_df |>   cast_tdm(word, year, tf_idf) |>   as.matrix()  # Preview dim(sotu_word_tfidf_mat) ## [1] 13109    69 sotu_word_tfidf_mat[1:5, 1:5] ##                 Docs ## Terms               1950    1951     1952 1953 1954 ##   enjoyment      0.00207 0.00000 0.000661    0    0 ##   rent           0.00167 0.00000 0.000000    0    0 ##   widespread     0.00167 0.00000 0.000000    0    0 ##   underdeveloped 0.00165 0.00000 0.000000    0    0 ##   ideals         0.00156 0.00171 0.000853    0    0 # Calculate the standard deviation of the tf-idf values for each word sotu_words_sd <-   apply(sotu_word_tfidf_mat, MARGIN = 1, FUN = sd, na.rm = TRUE)  # Preview seed words sotu_words_sd |>   sort(decreasing = TRUE) |>   head(100) ##      vietnam      hussein       saddam         salt         iraq        shall  ##     0.001188     0.001138     0.001121     0.001067     0.000976     0.000887  ##          oil        iraqi       that's   inspectors        qaida       terror  ##     0.000798     0.000786     0.000733     0.000715     0.000702     0.000662  ##   terrorists         it's        crude       soviet    terrorist     activity  ##     0.000651     0.000647     0.000635     0.000633     0.000632     0.000630  ##     covenant arrangements      session           al  disarmament       steven  ##     0.000611     0.000609     0.000596     0.000596     0.000589     0.000588  ##      wartime      barrels         isil    mentioned        ought        elvin  ##     0.000573     0.000569     0.000567     0.000559     0.000558     0.000556  ##       iraqis         ryan        we're         kids        let's          gun  ##     0.000551     0.000539     0.000539     0.000535     0.000533     0.000532  ##      rebekah           cj        camps      empower         cory        we've  ##     0.000531     0.000527     0.000524     0.000523     0.000522     0.000518  ##          92d  afghanistan    childcare        100th        music         gulf  ##     0.000516     0.000514     0.000513     0.000507     0.000500     0.000494  ##         21st   extremists   foundation      picture     beguiled  contemplate  ##     0.000491     0.000489     0.000487     0.000486     0.000473     0.000473  ##    enumerate    hurriedly   hysterical  ingredients        smile       smiles  ##     0.000473     0.000473     0.000473     0.000473     0.000473     0.000473  ##     wagehour     josefina        shi'a        alice     alliance    seventies  ##     0.000473     0.000471     0.000468     0.000468     0.000467     0.000464  ##       herman       joshua      matthew        julie         11th       border  ##     0.000463     0.000463     0.000463     0.000462     0.000461     0.000459  ##      persian    recommend    communist        mayor    nicaragua       planes  ##     0.000456     0.000453     0.000447     0.000443     0.000442     0.000442  ##       trevor        corey       kenton      preston        seong     property  ##     0.000440     0.000439     0.000439     0.000439     0.000439     0.000439  ##      regimes       disarm      mention    peacetime   localities    objective  ##     0.000437     0.000436     0.000436     0.000433     0.000429     0.000428  ##          i'm       surtax        banks       pounds       foster          she  ##     0.000424     0.000423     0.000423     0.000422     0.000421     0.000418  ##         isis  sandinistas  discharging         gold  ##     0.000417     0.000416     0.000416     0.000415 plot_terms <- c(\"equality\", \"right\", \"community\", \"child\", \"woman\", \"man\")  sotu_terms_tfidf_df |>   filter(lemma %in% plot_terms) |>   ggplot(aes(year, tf_idf)) +   geom_smooth(se = FALSE, span = 0.5) +   geom_smooth(method = \"lm\") +   facet_wrap(~lemma)"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"meaning","dir":"Articles","previous_headings":"Concepts and strategies > Interrogate","what":"Meaning","title":"8. Employing exploratory methods","text":"meaning words change across time periods? approach need turn word embeddings. Word embeddings show capture distributional semantics --meaning words based distribution corpus (Hamilton, Leskovec, Jurafsky 2016). Since research question aimed change time performing diachronic analysis. means need create word embeddings time period, identify common vocabulary across time periods, align word embeddings common space can compare . load packages need. first start creating sub-corpora decade. write disk can use necessary. Note data datasets generated process analysis often stored analysis/ folder inside main data folder keep separate data acquired derived project. use str_c() function summarize words address single string decade. use write_lines() function write string text file. pwalk() function purrr package convenient way iterate multiple arguments (decade address case) function without returning value console. p pwalk() stands \"parallel\" indicates function iterate arguments parallel. Now written function read text files, train word embeddings , write word embeddings disk, load back VectorSpaceModel objects list. Tip note training word embeddings. two main approaches training word embeddings: Continuous Bag Words (CBOW) Skip-gram. CBOW better common words larger datasets. Skip-gram better less common words smaller datasets. Given varying sizes sub-corpora, Skip-gram might suitable may capture nuances less frequent words. Furthermore, number dimensions hyperparameter needs tuned. default 50, chosen 100 attempting capture nuanced changes language SOTU. can now apply create_embeddings() custom function decade sub-corpora text files ../analysis/sotu/. word embedding model matrix words rows dimensions columns element sotu_vec_mods list. elements name decade. can extract element list using pluck() function purrr package. Tip Since models list, using purrr functions quite bit. quick summary purrr functions using. map() iterates list applies function element list. returns list. walk() iterates list applies function element list. return value. parallel version, pmap() pwalk(), iterate multiple lists parallel, version iterates named lists, imap() iwalk(). p pmap() pwalk() stands \"parallel\" indicates function iterate arguments parallel. imap() iwalk() stands \"indexed\" indicates function iterate arguments parallel return index list element. embeddings can explored number ways. example, can get words closest given word vector space decade. PsychWordVec package function, most_similar(), us. can use map() function iterate model list get words similar vectors. use str_c() function summarize words single string decade. previous example shows closest words \"freedom\" decade synchronic manner. can inspect synchronic changes draw conclusions . However, interested diachronic changes. , need align word embeddings common space. Now identify common words across decades subset word embeddings common vocabulary. map() function iterates model list returns rownames rownames() (words) model. reduce() function iterates list words returns intersection words across models intersect(). 534 words common vocabulary. Now can subset word embeddings common vocabulary. use map() function iterate model list subset model common vocabulary using common_vocab variable part bracket notation subset. now models list vocabulary number dimensions, 100. Now can align models common space. reason models need aligned word embeddings trained different corpora. means words represented different spaces. use Orthogonal Procrustes solution align models common coordinate space. PsychWordVec package (Bao 2023) function, orth_procrustes(), us. process aligning models, models converted plain matrices need convert back embed matrix objects. model decade aligned vocabulary space, can now use models compare words across time. number ways can compare words across time. first approach, consider semantic displacement words time vector space. calculating cosine difference word embeddings word decade. cosine difference, can visualize change time. now list cosine difference decade compared first decade (\"1950s\"). can visualize line plot x-axis decade y-axis cosine difference. add linear trend line plot get sense overall trend. write function get differences word return data frame decade difference. make easier visualize differences multiple words. create function performs us can plot multiple words time.  can focus particular word nearest words word decade. use map() function iterate model list get closest words word. use str_c() function summarize words single string decade. Another approach visualize vector space words occupy time. collapse word embeddings decade single matrix. append decade word lose decade information. extract first two principal components matrix, can visualize data two dimensions. plot data scatter plot x-axis first principal component y-axis second principal component. label points words. PCA word embeddings, yes! aligned models results much sensible interesting. individual words grouped closer together across time, general, exceptions.  kinds visualizations can useful exploring data drawing conclusions.","code":"library(fs) # for file system functions library(PsychWordVec) # for working with word embeddings library(purrr) # for iterating over lists # Create sub-corpora for each decade and write to disk ------------ sotu_terms_df |>   summarize(address = str_c(lemma, collapse = \" \"), .by = decade) |>   select(decade, address) |>   pwalk(\\(decade, address) {     file_name <- str_c(\"../data/analysis/sotu/\", decade, \"s.txt\")     write_lines(address, file_name)   }) create_embeddings <- function(dir_path, dims = 100) {   # Get the text file paths   txt_files <- dir_ls(dir_path, regexp = \"\\\\.txt$\")   # Train the word embeddings   models <-     txt_files |>     map(\\(file) {       train_wordvec(         text = file,         dims = 100,         normalize = TRUE       )     })   # Modify the list names   names(models) <-     names(models) |>     basename() |>     str_remove(\"\\\\.txt\")    # Convert to embed matrices   models <- map(models, as_embed)    return(models) } sotu_vec_mods <- create_embeddings(\"../analysis/sotu/\") # Extract an element from a list sotu_vec_mods |> pluck(\"1950s\") ##                         dim1 ...     dim100 ##    1: the             0.0882 ... <100 dims> ##    2: of              0.1310 ... <100 dims> ##    3: and             0.1200 ... <100 dims> ##    4: be              0.0959 ... <100 dims> ##    5: to              0.0891 ... <100 dims> ## -----                                       ## 1164: appear          0.1262 ... <100 dims> ## 1165: allegiance      0.1210 ... <100 dims> ## 1166: accumulate      0.1244 ... <100 dims> ## 1167: accomplishment  0.1116 ... <100 dims> ## 1168: accept          0.1155 ... <100 dims> # Get the closest words sotu_vec_mods |>   map(\\(mod) {     most_similar(mod, \"freedom\", verbose = FALSE) # get words similar to \"freedom\"   }) |>   map(\\(res) {     str_c(res$word, collapse = \", \") # summarize the words into a single string   }) ## $`1950s` ## [1] \"all, free, man, win, fight, us, no, if, preserve, let\" ##  ## $`1960s` ## [1] \"cuba, communist, face, independent, ambition, close, future, how, southeast, goal\" ##  ## $`1970s` ## [1] \"africa, democracy, demonstrate, root, historic, shape, element, side, conflict, depend\" ##  ## $`1980s` ## [1] \"fighter, nation, world, defend, great, abroad, political, peace, safe, together\" ##  ## $`1990s` ## [1] \"of, peace, perfect, begin, our, abroad, define, democracy, peaceful, saddam\" ##  ## $`2000s` ## [1] \"world, friend, democracy, afghanistan, terror, free, force, defeat, cause, peace\" ##  ## $`2010s` ## [1] \"please, stand, great, always, destiny, country, light, flag, report, celebrate\" # Extract the common vocabulary ----------------------------------- common_vocab <-   sotu_vec_mods |>   map(rownames) |>   reduce(intersect)  length(common_vocab) ## [1] 534 head(common_vocab) ## [1] \"the\" \"of\"  \"and\" \"be\"  \"to\"  \"in\" # Subset the models to the common vocabulary ---------------------- sotu_vec_common_mods <-   sotu_vec_mods |>   map(\\(mod) {     mod[common_vocab, ] # subset each model to the common vocabulary   })  sotu_vec_common_mods |> pluck(\"1950s\") ##                    dim1 ...     dim100 ##   1: the         0.0882 ... <100 dims> ##   2: of          0.1310 ... <100 dims> ##   3: and         0.1200 ... <100 dims> ##   4: be          0.0959 ... <100 dims> ##   5: to          0.0891 ... <100 dims> ## ----                                   ## 530: happen      0.1076 ... <100 dims> ## 531: everything  0.1122 ... <100 dims> ## 532: different   0.1056 ... <100 dims> ## 533: debate      0.1243 ... <100 dims> ## 534: city        0.1163 ... <100 dims> sotu_vec_common_mods |> pluck(\"2010s\") ##                    dim1 ...     dim100 ##   1: the         0.1350 ... <100 dims> ##   2: of          0.1209 ... <100 dims> ##   3: and         0.1140 ... <100 dims> ##   4: be          0.1021 ... <100 dims> ##   5: to          0.1539 ... <100 dims> ## ----                                   ## 530: happen      0.1301 ... <100 dims> ## 531: everything  0.1127 ... <100 dims> ## 532: different   0.1295 ... <100 dims> ## 533: debate      0.1310 ... <100 dims> ## 534: city        0.1272 ... <100 dims> # Align the models to a common space sotu_aligned_mods <-   sotu_vec_common_mods |>   map(\\(mod) {     orth_procrustes(sotu_vec_common_mods[[1]], mod) # align to the first model   }) |>   map(\\(mod) {     emb <- as_embed(mod) # convert to a embed matrix object     emb   }) # Calculate the cosine difference between the models -------------- word <- \"freedom\"  word_vectors <-   sotu_aligned_mods |>   map(\\(mod) {     mod[word, ]   })  differences <-   word_vectors |>   map(\\(vec) {     cos_dist(vec, word_vectors[[1]])   })  differences ## $`1950s` ## [1] -2.22e-16 ##  ## $`1960s` ## [1] 0.0221 ##  ## $`1970s` ## [1] 0.0399 ##  ## $`1980s` ## [1] 0.0321 ##  ## $`1990s` ## [1] 0.0226 ##  ## $`2000s` ## [1] 0.0112 ##  ## $`2010s` ## [1] 0.0221 # Function to get the cosine difference over time  get_cosine_diff <- function(word, models) {   word_vectors <- map(models, \\(mod) {     mod[word, ]   })    differences <- map(word_vectors, \\(vec) {     cos_dist(vec, word_vectors[[1]])   })    tibble(word, decade = basename(names(differences)), difference = unlist(differences)) }  get_cosine_diff(word = \"freedom\", models = sotu_aligned_mods) ## # A tibble: 7 × 3 ##   word    decade difference ##   <chr>   <chr>       <dbl> ## 1 freedom 1950s   -2.22e-16 ## 2 freedom 1960s    2.21e- 2 ## 3 freedom 1970s    3.99e- 2 ## 4 freedom 1980s    3.21e- 2 ## 5 freedom 1990s    2.26e- 2 ## 6 freedom 2000s    1.12e- 2 ## 7 freedom 2010s    2.21e- 2 plot_words <-   c(\"freedom\", \"nation\", \"country\", \"america\")  plot_words |>   map(get_cosine_diff, models = sotu_aligned_mods) |>   bind_rows() |>   arrange(decade) |>   ggplot(aes(decade, difference, group = word, color = word)) +   geom_smooth(se = FALSE, span = 1) +   labs(title = word) # Function to get the closest words to a word --------------------- word <- \"america\"  sotu_vec_common_mods |>   map(\\(mod) {     most_similar(mod, word, verbose = FALSE)   }) |>   map(\\(mod) {     str_c(mod$word, collapse = \", \")   }) |>   enframe(name = \"decade\", value = \"words\") |>   unnest(words) ## # A tibble: 7 × 2 ##   decade words                                                                   ##   <chr>  <chr>                                                                   ## 1 1950s  everything, issue, happen, one, reason, word, society, achieve, debate… ## 2 1960s  difference, historic, beyond, forward, progress, real, value, decision… ## 3 1970s  not, mean, life, matter, special, concern, meet, seek, chance, achieve  ## 4 1980s  look, us, courage, see, here, always, home, nation, free, ever          ## 5 1990s  leadership, freedom, our, report, spirit, begin, stand, here, moment, … ## 6 2000s  fear, lead, country, liberty, friend, military, determine, show, stren… ## 7 2010s  then, country, word, task, fact, hope, kind, americas, never, american # Visualize the vector space of words over time ------------------- sotu_joined_mods <-   sotu_aligned_mods |>   imap(\\(mod, index) {     rownames(mod) <- str_c(rownames(mod), \"_\", index)     mod   }) |>   reduce(rbind) |>   as_embed() sotu_joined_pca <-   sotu_joined_mods |>   scale() |>   prcomp()  sotu_pca_df <-   as_tibble(sotu_joined_pca$x[, 1:2]) |>   mutate(word = names(sotu_joined_pca$x[, 1]))  sotu_pca_df |>   filter(str_detect(word, \"^(nation|country|america)_\")) |>   ggplot(aes(x = PC1, y = PC2, label = word)) +   geom_point() +   ggrepel::geom_text_repel()"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"8. Employing exploratory methods","text":"recipe, explored State Union addresses 1950 2019. used number tools techniques explore data draw conclusions. used tidytext package tokenize lemmatize words addresses. used word2vec package train word embeddings decade. used PsychWordVec package align word embeddings common space. used wordVectors package explore word embeddings. used ggplot2 package visualize data. strategies, others, can used explore questions questions depth. Exploratory analysis creativity curiosity can shine.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"8. Employing exploratory methods","text":"text analysis,  used transform effect words common across time periods promote effect words distinctive time period. correct method use adding linear trend line plot ggplot2? geom_smooth(method = 'lm')geom_line()geom_bar()geom_point() torf(TRUE) working lists, walk() like map() return value. TRUEFALSE creating word embeddings, CBOW model better suited less common words smaller datasets compared Skip-gram. package used align word embedding models common coordinate space? PsychWordVectidytextword2vecggplot2 process reducing number features dataset retaining much information possible known  reduction.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-8.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"8. Employing exploratory methods","text":"preparation Lab 8, review ensure familiar following concepts: Tokenizing text Generating frequency dispersion measures Creating Term-Document Matrices Using purrr package iterate lists Visualizations ggplot2 lab, opportunity apply concepts materials chapter new dataset. consider dataset questions want ask data. also consider tools techniques use explore data draw conclusions. asked submit code brief report findings.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"9. Building predictive models","text":"recipe cover process building predictive model classify text one three Spanish dialects: Argentinian, Mexican, Spanish. take step--step approach includes data preparation, model training evaluation, result interpretation. see practical examples apply tidymodels framework build evaluate predictive model. Including: identify variables interest inspect data interrogate data iterate model improve performance interpret results model workflow building predictive model shown Table 1. Note Step 6 includes optional step iterate model improve performance. optional always necessary iterate model. However, often case first model build best model. good prepared iterate model. Table 1:  predictive modeling workflow get started loading key packages use recipe. Tip Note loading tidymodels package load 22 packages commonly used modeling workflows. can see list packages running tidymodels::tidyverse_packages(). Lab 9, apply tidymodels framework build explore predictive model based another dataset.","code":"library(tidymodels) # for modeling library(textrecipes) # for text preprocessing library(dplyr) # for data manipulation library(tidyr) # for data manipulation library(stringr) # for string manipulation library(tidytext) # for text manipulation library(ggplot2) # for visualization library(janitor) # for tabyl()  tidymodels_prefer() # avoid function name conflicts"},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"orientation","dir":"Articles","previous_headings":"Concepts and strategies","what":"Orientation","title":"9. Building predictive models","text":"use ACTIV-ES corpus build predictive model can classify text one three dialects Spanish: Argentinian, Mexican, Spanish. frame supervised learning problem, set texts labeled dialect Spanish written . contrast classification task chapter, binary, multiclass classification task, trying classify document one three classes. preview structure ACTIVES dataset. dataset contains 430 documents, labeled variety Spanish written text document. document ID also included, able use index documents. variety vector factor. outcome variable predictive model, good predictive models require classification variables factors. get sense distribution variety variable. can see dataset somewhat balanced, Peninsular Spanish comprising larger portion texts.","code":"aes_df # # A tibble: 430 × 3 #    doc_id variety   text                                                         #     <dbl> <fct>     <chr>                                                        #  1 199500 Argentina No está , señora . Aquí tampoco . No aparece , señora . ¿ D… #  2 184782 Argentina ALGUIEN AL TELÉFONO . LA ANGUSTIA . Ah , no , no , no mi hi… #  3  47823 Argentina Habrá que cumplir su última voluntad , ¿ el medallón ? Lo v… #  4 282622 Argentina Sucedió en Hualfin Esta es la historia de tres generaciones… #  5  62433 Argentina 10 secuestros en 10 días ! Y no hay el menor índice . Bueno… #  6  70250 Argentina Y preguntada que fue sí reconocen el cadáver exhumado ... y… #  7  71897 Argentina ¡ Jeremías ! ¡ Jeremías ! ¡ No dejés parir a tu mujer ! Sei… #  8 333883 Argentina Usted . Usted que frecuenta el éxito como una costumbre más… #  9 333954 Argentina Miles de campanas nos traen , a través de los siglos , el t… # 10 175243 Argentina Y ? Enseguida viene , fue al baño . Bueno , pero la mesa la… # # ℹ 420 more rows aes_df |>   tabyl(variety) |>   adorn_pct_formatting(digits = 1) ## # A tibble: 3 × 3 ##   variety       n percent ##   <fct>     <dbl> <chr>   ## 1 Argentina   128 29.8%   ## 2 Mexico      119 27.7%   ## 3 Spain       183 42.6%"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"analysis","dir":"Articles","previous_headings":"Concepts and strategies","what":"Analysis","title":"9. Building predictive models","text":"point can start approach building predictive model can distinguish Spanish varieties using text. first start applying steps 1 2 workflow. apply steps 3-5 iteratively build, evaluate, improve model, necessary, applying test data assess interpret results. go ahead perform steps 1 2. split data training testing sets, use rsample package. initial_split() function, sets splits use variety stratification variable ensure training testing sets representative distribution outcome variable. process random, set seed reproducibility. Finally, call training() testing() functions extract training testing sets. set base recipe formally identifies relationship predictor outcome variables. recipe() function recipes package used create recipe. now steps 1 2 workflow completed. identified variables interest split data training testing sets. One thing set cross-validation folds. Every time fit model training data, want evaluate model's performance training data. However, want way biased --testing model data trained ! reason, use cross-validation split training data multiple training validation sets represent different splits training data. use vfold_cv() function rsample package set cross-validation folds. use 10 folds, common number folds use. also use strata argument ensure folds representative distribution outcome variable. common steps completed, can now apply reapply steps 3-5 workflow build, evaluate, improve model.","code":"# Set the seed for reproducibility set.seed(1234)  # Split the data into training and testing sets aes_split <-   initial_split(     data = aes_df,     prop = 0.8,     strata = variety   )  aes_train <- training(aes_split) # extract the training set aes_test <- testing(aes_split) # extract the testing set # Set the base recipe aes_base_rec <-   recipe(     formula = variety ~ text,     data = aes_train   ) # Set seed for reproducibility set.seed(1234)  # Set up the cross-validation folds cv_folds <-   vfold_cv(     data = aes_train,     v = 10,     strata = variety   )"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"approach-1","dir":"Articles","previous_headings":"Concepts and strategies > Analysis","what":"Approach 1","title":"9. Building predictive models","text":"first approach, start simple using words features apply logistic regression model. completely naive, however, familiar undo influence frequent words. address , apply term frequency-inverse document frequency (TF-IDF) transformation text order downweight influence frequent words promote words indicative class. Furthermore, know want use regularized regression model avoid overfitting particular words. get started, use textrecipes package add steps aes_base_rec recipe preprocess text. use step_tokenize() function tokenize text. tokenization process likely result large number terms, informative add computational overhead. want restrict number terms step_tokenfilter() function. However, clear many terms restrict tokens . now, start 1,000 tokens, likely want revisit later. also use step_tfidf() function apply TF-IDF transformation text setting smooth_idf FALSE. implement recipe preview text preprocessing steps apply prep() bake() functions. now recipe tokenize text, restrict tokens common 1,000 tokens, create TF-IDF matrix. bad idea inspect features point make sure preprocessing steps applied correctly gauge feature selection looks like comes time interpret model, sense model . TF-IDF going main feature model, visualize top 20 terms class. , use dplyr package get median TF-IDF score word class, convert data wide long format using pivot_longer() function, use ggplot2 package visualize data. Figure 1: Top 20 terms class Figure 1, see words indicative language variety. familiar Spanish, can probably detect variety-specific terms. example, \"vos\" pronoun used Argentinian Spanish \"os\" pronoun used Peninsular Spanish. also overlap varieties, \"tienes\" \"te\". Another point note difference magnitude TF-IDF scores Argentinian varieties. suggests Argentinian variety distinct varieties varieties . Among distinctive terms verbal forms specific Argentinian Spanish, \"tenés\" \"sos\". Now create model specification. use multinom_reg() function parsnip package create multinomial logistic regression model, multiple classes prediction task. use \"glmnet\" engine, allow us apply regularization model, arbitrarily set 0.01. use set_engine() function set engine set_mode() function set mode \"classification\". combine recipe model specification, use workflows package. use workflow() function pass add_recipe(aes_rec) add_model(aes_spec) arguments add recipe model specification workflow. can now use cross-validation folds set earlier. use fit_resamples() function fit model training data using cross-validation folds. can now evaluate model's performance training data. use collect_metrics() function collect metrics cross-validation folds. can see model mean accuracy 80.4% ROC-AUC 92.3%. pretty good first pass. get sense good (bad) , compare baseline model. baseline model simplest model can use compare performance model . common baseline model model always predicts frequent class. case, Peninsular Spanish, accounts 42.6% data. clear model much better baseline model accuracy score 42.6%. can visualize correct incorrect predictions using confusion matrix. use conf_mat_resampled() function yardstick package create confusion matrix autoplot() function ggfortify package visualize . Figure 2: Confusion matrix model Approach 1 left-downward diagonal confusion matrix represents average number documents correctly predicted aggregated model across cross-validation folds. cells represent average number documents incorrectly predicted class-class combination. can read using row label identify predicted class column label identify actual class. , example, model predicted Mexico \\(n\\) times actual class Argentina. .","code":"# Add preprocessing steps to the recipe aes_rec <-   aes_base_rec |>   step_tokenize(text) |>   step_tokenfilter(text, max_tokens = 1000) |>   step_tfidf(text, smooth_idf = FALSE)  # Preview the recipe aes_rec aes_bake <-   aes_rec |>   prep() |>   bake(new_data = NULL)  # Preview dim(aes_bake) ## [1]  343 1001 aes_bake[1:5, 1:5] ## # A tibble: 5 × 5 ##   variety   tfidf_text_1 tfidf_text_10 tfidf_text_15 tfidf_text_2 ##   <fct>            <dbl>         <dbl>         <dbl>        <dbl> ## 1 Argentina     0.000206     0.000599       0.000402      0       ## 2 Argentina     0            0              0             0       ## 3 Argentina     0            0.0000887      0             0       ## 4 Argentina     0.00437      0              0.00142       0.00381 ## 5 Argentina     0            0.00155        0             0.00124 # Sum the term frequencies by class class_freq_wide <-   aes_bake |>   group_by(variety) |>   summarize(     across(       starts_with(\"tfidf_\"),       median     )   ) |>   ungroup()  # Convert the data from wide to long format class_freq_long <-   class_freq_wide |>   pivot_longer(     cols = starts_with(\"tfidf_\"),     names_to = \"term\",     values_to = \"tfidf\"   ) |>   mutate(term = str_remove(term, \"tfidf_text_\"))  # Visualize the top 20 terms by class class_freq_long |>   slice_max(n = 20, order_by = tfidf, by = variety) |>   mutate(term = reorder_within(term, tfidf, variety)) |>   ggplot(aes(x = term, y = tfidf)) +   geom_col() +   scale_x_reordered() +   facet_wrap(~variety, scales = \"free_y\") +   coord_flip() # Create a model specification aes_spec <-   multinom_reg(     penalty = 0.01,     mixture = 1   ) |>   set_engine(\"glmnet\") |>   set_mode(\"classification\") # Create a workflow aes_wf <-   workflow() |>   add_recipe(aes_rec) |>   add_model(aes_spec) # Fit the model to the training data aes_train_fit <-   aes_wf |>   fit_resamples(     resamples = cv_folds,     control = control_resamples(save_pred = TRUE)   ) # Evaluate the model's performance on the training data aes_train_fit |>   collect_metrics() ## # A tibble: 2 × 6 ##   .metric  .estimator  mean     n std_err .config              ##   <chr>    <chr>      <dbl> <int>   <dbl> <chr>                ## 1 accuracy multiclass 0.804    10  0.0210 Preprocessor1_Model1 ## 2 roc_auc  hand_till  0.923    10  0.0109 Preprocessor1_Model1 aes_train_fit |>   conf_mat_resampled(tidy = FALSE) |>   autoplot(type = \"heatmap\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"approach-2","dir":"Articles","previous_headings":"Concepts and strategies > Analysis","what":"Approach 2","title":"9. Building predictive models","text":"first approach applied TF-IDF transformation text used regularized multinomial logistic regression model. also restricted tokens 1,000 frequent tokens arbitrarily set regularization parameter 0.01. resulted aggregate accuracy score 80.4% training data. good start, see can better. second approach, try improve model applying principled approach feature hyperparameter selection. 'tune' max_tokens penalty hyperparameters recipe model specifications, respectively. need update recipe model specification include placeholders parameters replacing previous values tune(). also need update workflow include updated recipe model specification. can now create workflow includes recipe model specification. Now set range values max_tokens penalty hyperparameters. grid_regular() function dials package allow us specify grid values hyperparameter. range = argument specifies range values include grid. max_tokens, straightforward. penalty, specifying range values log scale. range values 0.001 0.1. levels argument specifies number values include grid. case, include 5 values max_tokens 10 values penalty. result 50 combinations hyperparameter values. pass aes_wf workflow tune_grid() function grid values specified tune hyperparameters. aes_grid object tibble contains grid combinations hyperparameter values. case, 50 combinations. means going fit 50 models training data! lot models, worth get robust estimate model's performance. can use collect_metrics() function collect metrics cross-validation folds tuning parameters, result lot output. Instead, can use autoplot() function visualize metrics. Figure 3: Metrics model tuning Approach 2 see variation across folds accuracy ROC-AUC scores. help us make informed decision hyperparameters use. metric use select best model something consider.Accuracy important measure, tell whole story. particular, accuracy tell us well model class --overall correct incorrect predictions. get better sense model across classes, can pay attention ROC-AUC score. ROC-AUC score measure area receiver operating characteristic (ROC) curve. ROC curve plots true positive rate (TPR) false positive rate (FPR) class different probability thresholds. measure useful affected class imbalance. select best model based ROC-AUC score. can now update workflow best hyperparameters. can now see updated workflow replace tune() placeholders best hyperparameters selected. perform resampled fit training data using new tuned model compare results previous, abritrarily tuned model. accuracy score improved just bit, aggregate score 80.4% 81.9%. likelihood, want continue iterate model, applying different feature selection engineering procedures, different models, different hyperparameters --consider suggestions next section. However, sake time, stop train final model training data apply model test data assess interpret results.","code":"# Update the recipe aes_rec <-   aes_base_rec |>   step_tokenize(text) |>   step_tokenfilter(text, max_tokens = tune()) |> # adds placeholder   step_tfidf(text, smooth_idf = FALSE)  # Update the model specification aes_spec <-   multinom_reg(     penalty = tune(), # adds placeholder     mixture = 1   ) |>   set_engine(\"glmnet\") |>   set_mode(\"classification\") # Create a workflow aes_wf <-   workflow() |>   add_recipe(aes_rec) |>   add_model(aes_spec) # Set the hyperparameter grid aes_grid <-   grid_regular(     max_tokens(range = c(250, 2000)),     penalty(range = c(-3, -1)),     levels = c(max_tokens = 5, penalty = 10)   )  aes_grid ## # A tibble: 50 × 2 ##    max_tokens penalty ##         <int>   <dbl> ##  1        250 0.001   ##  2        687 0.001   ##  3       1125 0.001   ##  4       1562 0.001   ##  5       2000 0.001   ##  6        250 0.00167 ##  7        687 0.00167 ##  8       1125 0.00167 ##  9       1562 0.00167 ## 10       2000 0.00167 ## # ℹ 40 more rows # Tune the hyperparameters aes_tune <-   aes_wf |>   tune_grid(     resamples = cv_folds,     grid = aes_grid,     control = control_resamples(save_pred = TRUE)   ) # Plot the collected metrics aes_tune |> autoplot() # Get the best model aes_tune_best <-   aes_tune |>   select_best(\"roc_auc\")  aes_tune_best ## # A tibble: 1 × 3 ##   penalty max_tokens .config               ##     <dbl>      <int> <chr>                 ## 1  0.0215       1562 Preprocessor4_Model07 # Update the workflow aes_wf <-   aes_wf |>   finalize_workflow(aes_tune_best)  aes_wf ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: multinom_reg() ##  ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 3 Recipe Steps ##  ## • step_tokenize() ## • step_tokenfilter() ## • step_tfidf() ##  ## ── Model ─────────────────────────────────────────────────────────────────────── ## Multinomial Regression Model Specification (classification) ##  ## Main Arguments: ##   penalty = 0.0215443469003188 ##   mixture = 1 ##  ## Computational engine: glmnet # Fit the model to the training data aes_train_fit <-   aes_wf |>   fit_resamples(     resamples = cv_folds,     control = control_resamples(save_pred = TRUE)   )  # Evaluate the model's performance on the training data aes_train_fit |>   collect_metrics() ## # A tibble: 2 × 6 ##   .metric  .estimator  mean     n std_err .config              ##   <chr>    <chr>      <dbl> <int>   <dbl> <chr>                ## 1 accuracy multiclass 0.819    10  0.0135 Preprocessor1_Model1 ## 2 roc_auc  hand_till  0.938    10  0.0104 Preprocessor1_Model1"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"interpreting-the-model","dir":"Articles","previous_headings":"Concepts and strategies > Analysis > Approach 2","what":"Interpreting the model","title":"9. Building predictive models","text":"stage ready interpret model. first fit model training data, apply model test data, evaluate model's performance test data. Finally, dig model interpret importance features help us understand model can tell us words indicative, , variety. fit final model training data evaluate testing data using last_fit() function takes updated workflow original split created earlier stored aes_split. can now collect performance metrics testing data. accuracy model test data 73.6%. lower accuracy training data. surprised? really. model trained training data, surprising perform better training data test data, despite fact used cross-validation evaluate model training data. good reminder model perfect expect . 'kap' metric mean? Kappa statistic measure agreement predicted actual classes. measure agreement corrected possibility correct prediction may occurred chance. kappa statistic ranges 0 1, 0 indicating agreement chance 1 indicating perfect agreement. case, kappa statistic 87.2%, indicates moderate amount agreement predicted actual classes. explore difference performance across classes. , use conf_mat() function yardstick package create confusion matrix autoplot() function ggfortify package visualize . Figure 4: Confusion matrix model Approach 2 can see model good job predicting Peninsular Spanish, well varieties. surprising given Peninsular Spanish frequent class data. good reminder accuracy metric consider evaluating model. can get better sense model across classes looking ROC-AUC score.  Taken together, decent model can predict variety Spanish text written . can also see although prediction accuracy appears higher Peninsular Spanish, ROC-AUC curves suggest model better job predicting varieties based features. still room improvement --recognized earlier. However, important start use testing data improve model. testing data used evaluate model. start use testing data improve model, longer unbiased estimate model's performance. now dig model's features explore words driving model's predictions. approach depend model. case, used multinomial logistic regression model, linear model. means can interpret model's coefficients understand importance features. Coefficients positive indicate feature associated reference class coefficients negative indicate feature associated non-reference class. classification tasks two classes, straightforward interpret. issue , however, two classes (.e., Argentina, Mexico, Spain). cases, coefficients estimates class need extracted standardized compared across classes. can using extract_fit_parsnip() function parsnip package. extract model object workflow object. tidy() function broom package organize coefficients (log-odds) predictor terms outcome class. can use filter() function dplyr package remove intercept term mutate() function dplyr package remove \"tfidf_text_\" prefix term names legible. Now standardize log-odds coefficients comparable across classes, use scale() function base R transform coeffients class mean 0 standard deviation 1. scale() returns matrix, use .vector() function convert matrix vector. Finally, visualize top 25 terms class. Note using reorder_within() scale_x_reordered() functions tidytext package reorder terms way facets allow distinct terms x-axis class. coord_flip() function ggplot2 package used flip axes easier reading. Figure 5: Top 25 terms class can assess distinct features class also gauge magnitude estimates. cautious, however, terms derived model performs moderately well.","code":"# Fit the final model aes_final_fit <- last_fit(aes_wf, aes_split) # Get the performance metrics aes_final_fit |>   collect_metrics() ## # A tibble: 2 × 4 ##   .metric  .estimator .estimate .config              ##   <chr>    <chr>          <dbl> <chr>                ## 1 accuracy multiclass     0.736 Preprocessor1_Model1 ## 2 roc_auc  hand_till      0.872 Preprocessor1_Model1 aes_final_fit |>   collect_predictions() |>   conf_mat(truth = variety, estimate = .pred_class) |>   autoplot(type = \"heatmap\") # Get the ROC-AUC score aes_final_fit |>   collect_predictions() |>   roc_curve(truth = variety, .pred_Argentina:.pred_Spain) |>   autoplot() # Get the coefficients aes_coefs <-   aes_final_fit |>   extract_fit_parsnip() |>   tidy() |>   filter(term != \"(Intercept)\") |>   mutate(term = str_remove(term, \"tfidf_text_\"))  slice_sample(aes_coefs, n = 10) ## # A tibble: 10 × 4 ##    class     term      estimate penalty ##    <chr>     <chr>        <dbl>   <dbl> ##  1 Mexico    vengó            0  0.0215 ##  2 Mexico    tú               0  0.0215 ##  3 Mexico    tin              0  0.0215 ##  4 Mexico    papel            0  0.0215 ##  5 Spain     están            0  0.0215 ##  6 Argentina perdido          0  0.0215 ##  7 Argentina caballero        0  0.0215 ##  8 Spain     señora           0  0.0215 ##  9 Argentina casar            0  0.0215 ## 10 Spain     serás            0  0.0215 aes_coefs_z <-   aes_coefs |>   group_by(class) |>   mutate(z_score = as.vector(scale(estimate))) |>   ungroup()  slice_sample(aes_coefs_z, n = 10) ## # A tibble: 10 × 5 ##    class     term     estimate penalty z_score ##    <chr>     <chr>       <dbl>   <dbl>   <dbl> ##  1 Argentina iba             0  0.0215 -0.0857 ##  2 Spain     pudo            0  0.0215 -0.0320 ##  3 Spain     pará            0  0.0215 -0.0320 ##  4 Argentina dicho           0  0.0215 -0.0857 ##  5 Argentina sirve           0  0.0215 -0.0857 ##  6 Mexico    cuesta          0  0.0215 -0.0787 ##  7 Spain     demonios        0  0.0215 -0.0320 ##  8 Argentina punto           0  0.0215 -0.0857 ##  9 Mexico    también         0  0.0215 -0.0787 ## 10 Spain     cabeza          0  0.0215 -0.0320 aes_coefs_z |>   mutate(term = reorder_within(term, z_score, class)) |>   slice_max(n = 25, order_by = z_score, by = class) |>   ggplot(aes(x = term, y = z_score)) +   geom_col() +   scale_x_reordered() +   facet_wrap(~class, scales = \"free_y\") +   coord_flip()"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"other-approaches","dir":"Articles","previous_headings":"Concepts and strategies > Analysis","what":"Other approaches","title":"9. Building predictive models","text":"seen, many decisions make building predictive model. scratched surface options available. section, briefly consider approaches may interest. Features: used words recipe chapter classification task. merely order keep focus process building predictive model. many features used. example, use n-grams, character n-grams, word embeddings. textrecipes package provides many options text preprocessing feature engineering. look can derive linguistic units using textrecipes. First, set simple dataset base recipe. Now, say instead words, interested deriving word \\(n\\)-grams terms. use step_tokenize() function recipe. time, however, add value token = argument. case, use \"ngrams\". textrecipes uses tokenization engine tokenizers package, types tokenization available available (see help(tokenizers) information). default tokens = \"ngrams\" produces trigrams. Another option use character n-grams. useful want capture information morphology words. character n-grams, can use \"character_shingle\". default tokens = \"character_shingle\" also produces trigrams. Now, say want change number words n-gram character n-gram. can using options = argument. pass tokenizer-specific options. example, change number words n-gram, can use n = argument. like calculate multiple \\(n\\)-gram windows, can pass n_min = argument. Names values arguments options = take depend type tokenization specified. also use metadata, year text written, author, genre, etc. cases, update base recipe include metadata predictors can use necessary preprocessing steps prepare metadata modeling using functions recipes() package (.e., `step_normalize(), step_dummy(), etc.). also use features derived text, word length, syntactic complexity, sentiment, readability, etc. number stylistic features available using step_textfeature() function, 26 (see ?count_functions). However, also possible derive features working original dataset adding features Models: big advantage using tidymodels approach modeling allows us easily try different models. used multinomial logistic regression model recipe, also try models, random forest model, support vector machine, neural network. can simply changing model specification workflow. example, use random forest model. first need update model specification use rand_forest() function parsnip package create random forest model. also need update engine use ranger package, fast implementation random forest models. Finally, need update mode \"classification\". important understand different models different hyperparameters. say logistic_reg() multinom_reg() models, can tune penalty hyperparameter. However, case models. example, rand_forest() model penalty hyperparameter. Instead, mtry hyperparameter, number variables consider split. can tune hyperparameter way tuned penalty hyperparameter using tune(), grid_regular(), tune_grid(). models consider text classification include Naive Bayes, Support Vector Machines, Neural Networks. tidymodels framework supports models. last point consider whether want able interpret features drive model's performance. , want use model allows us interpret features. example, use linear model, logistic regression model, tree-based model, random forest model. However, able interpret features neural network model. Furthermore, methods use interpret features depend model. example, can interpret features linear model looking coefficients. However, interpret features random forest model way. Instead, can use vip() function vip package visualize importance features.","code":"df <- tibble(   outcome = factor(c(\"a\", \"a\", \"b\", \"b\")),   date = as.Date(c(\"2020-01-01\", \"2021-06-14\", \"2020-11-05\", \"2023-12-25\")),   text = c(     \"This is a fantastic sentence.\",     \"This is another great sentence.\",     \"This is a third, boring sentence.\",     \"This is a fourth and final sentence.\"   ) )  base_rec <- recipe(outcome ~ text, data = df) base_rec |>   step_tokenize(     text,     token = \"ngrams\", # word n-grams   ) |>   show_tokens(text) ## [[1]] ## [1] \"this is a\"            \"is a fantastic\"       \"a fantastic sentence\" ##  ## [[2]] ## [1] \"this is another\"        \"is another great\"       \"another great sentence\" ##  ## [[3]] ## [1] \"this is a\"             \"is a third\"            \"a third boring\"        ## [4] \"third boring sentence\" ##  ## [[4]] ## [1] \"this is a\"          \"is a fourth\"        \"a fourth and\"       ## [4] \"fourth and final\"   \"and final sentence\" base_rec |>   step_tokenize(     text,     token = \"character_shingle\" # character n-grams   ) |>   show_tokens(text) ## [[1]] ##  [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"saf\" \"afa\" \"fan\" \"ant\" \"nta\" \"tas\" \"ast\" ## [13] \"sti\" \"tic\" \"ics\" \"cse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\" ##  ## [[2]] ##  [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"san\" \"ano\" \"not\" \"oth\" \"the\" \"her\" \"erg\" ## [13] \"rgr\" \"gre\" \"rea\" \"eat\" \"ats\" \"tse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\" ##  ## [[3]] ##  [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"sat\" \"ath\" \"thi\" \"hir\" \"ird\" \"rdb\" \"dbo\" ## [13] \"bor\" \"ori\" \"rin\" \"ing\" \"ngs\" \"gse\" \"sen\" \"ent\" \"nte\" \"ten\" \"enc\" \"nce\" ##  ## [[4]] ##  [1] \"thi\" \"his\" \"isi\" \"sis\" \"isa\" \"saf\" \"afo\" \"fou\" \"our\" \"urt\" \"rth\" \"tha\" ## [13] \"han\" \"and\" \"ndf\" \"dfi\" \"fin\" \"ina\" \"nal\" \"als\" \"lse\" \"sen\" \"ent\" \"nte\" ## [25] \"ten\" \"enc\" \"nce\" base_rec |>   step_tokenize(     text,     token = \"ngrams\",     options = list(n = 2) # word bigrams   ) |>   show_tokens(text) ## [[1]] ## [1] \"this is\"            \"is a\"               \"a fantastic\"        ## [4] \"fantastic sentence\" ##  ## [[2]] ## [1] \"this is\"        \"is another\"     \"another great\"  \"great sentence\" ##  ## [[3]] ## [1] \"this is\"         \"is a\"            \"a third\"         \"third boring\"    ## [5] \"boring sentence\" ##  ## [[4]] ## [1] \"this is\"        \"is a\"           \"a fourth\"       \"fourth and\"     ## [5] \"and final\"      \"final sentence\" base_rec |>   step_tokenize(     text,     token = \"ngrams\",     options = list(n = 2, n_min = 1) # word unigrams and bigrams   ) |>   show_tokens(text) ## [[1]] ## [1] \"this\"               \"this is\"            \"is\"                 ## [4] \"is a\"               \"a\"                  \"a fantastic\"        ## [7] \"fantastic\"          \"fantastic sentence\" \"sentence\"           ##  ## [[2]] ## [1] \"this\"           \"this is\"        \"is\"             \"is another\"     ## [5] \"another\"        \"another great\"  \"great\"          \"great sentence\" ## [9] \"sentence\"       ##  ## [[3]] ##  [1] \"this\"            \"this is\"         \"is\"              \"is a\"            ##  [5] \"a\"               \"a third\"         \"third\"           \"third boring\"    ##  [9] \"boring\"          \"boring sentence\" \"sentence\"        ##  ## [[4]] ##  [1] \"this\"           \"this is\"        \"is\"             \"is a\"           ##  [5] \"a\"              \"a fourth\"       \"fourth\"         \"fourth and\"     ##  [9] \"and\"            \"and final\"      \"final\"          \"final sentence\" ## [13] \"sentence\" base_rec <- recipe(outcome ~ date + text, data = df) # add date  base_rec |>   step_tokenize(text) |>   step_date(date, features = c(\"year\")) |> # extract the year   prep() |>   juice() ## # A tibble: 4 × 4 ##   date             text outcome date_year ##   <date>      <tknlist> <fct>       <int> ## 1 2020-01-01 [5 tokens] a            2020 ## 2 2021-06-14 [5 tokens] a            2021 ## 3 2020-11-05 [6 tokens] b            2020 ## 4 2023-12-25 [7 tokens] b            2023 df <-   df |>   left_join(     # Calculate word count and average word length     df |>       unnest_tokens(word, text, drop = FALSE) |>       group_by(text) |>       summarize(         word_count = n(),         avg_word_length = mean(nchar(word))       )   )  recipe(   outcome ~ ., # use all variables   data = df ) |>   step_tokenize(text) |>   step_tf(text) |>   prep() |>   bake(new_data = NULL) ## # A tibble: 4 × 16 ##   date       word_count avg_word_length outcome tf_text_a tf_text_and ##   <date>          <int>           <dbl> <fct>       <int>       <int> ## 1 2020-01-01          5            4.8  a               1           0 ## 2 2021-06-14          5            5.2  a               0           0 ## 3 2020-11-05          6            4.33 b               1           0 ## 4 2023-12-25          7            4.14 b               1           1 ## # ℹ 10 more variables: tf_text_another <int>, tf_text_boring <int>, ## #   tf_text_fantastic <int>, tf_text_final <int>, tf_text_fourth <int>, ## #   tf_text_great <int>, tf_text_is <int>, tf_text_sentence <int>, ## #   tf_text_third <int>, tf_text_this <int> # Create a model specification aes_spec <-   # Random Forest   rand_forest(     mtry = 10,     trees = 1000   ) |>   set_engine(\"ranger\") |> # use the ranger engine   set_mode(\"classification\")"},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"9. Building predictive models","text":"recipe, covered foundational skills needed construct predictive (classification) model using tidymodels framework. examined key steps predictive modeling: identifying data, dividing training test sets, preprocessing, iterative model training, result interpretation. used dataset Spanish texts three different varieties demonstrate process iterating two approaches. first approach, used multinomial logistic regression model TF-IDF features. second approach, tuned hyperparameters model preprocessing steps improve model's performance. also touched upon alternative methods, like incorporating features n-grams experimenting models random forests, may prove useful text classification tasks. matierals chapter now understanding build understand text classification model R, equipped insights develop predictive analysis projects.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"check-your-understanding","dir":"Articles","previous_headings":"","what":"Check your understanding","title":"9. Building predictive models","text":"TRUEFALSE two basic types prediction models: regression classification. purpose splitting data training testing sets? make computation fasterTo avoid overfitting modelTo decrease size datasetTo make model simpler purpose cross-validation? make computation fasterTo avoid overfitting modelTo evaluate model's performanceTo make model simpler following models appropriate classification task? Logistic regressionRandom forestSupport vector machineLinear regression Iterative improvement modeling involves: Changing modelChanging hyperparametersChanging preprocessing stepsAll TRUEFALSE Feature importance measures uniform across models.","code":""},{"path":"https://qtalr.github.io/qtalrkit/articles/recipe-9.html","id":"lab-preparation","dir":"Articles","previous_headings":"","what":"Lab preparation","title":"9. Building predictive models","text":"preparation Lab 9, review ensure familiar following concepts: Building feature engineering pipelines recipes Building model specifications parsnip Iterative model training, evaluation, improvement workflows, tune, yardstick lab, opportunity apply concepts new dataset classification task. consider dataset task performed lab think might approach task feature engineering model selection perspective. asked submit code brief reflection approach results.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jerid Francom. Author, maintainer.","code":""},{"path":"https://qtalr.github.io/qtalrkit/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Francom J (2024). qtalrkit: Quantitative Text Analysis Linguists Resource Kit. R package version 0.9.2, https://qtalr.github.io/qtalrkit/, https://github.com/qtalr/qtalrkit.","code":"@Manual{,   title = {qtalrkit: Quantitative Text Analysis for Linguists Resource Kit},   author = {Jerid Francom},   year = {2024},   note = {R package version 0.9.2, https://qtalr.github.io/qtalrkit/},   url = {https://github.com/qtalr/qtalrkit}, }"},{"path":"https://qtalr.github.io/qtalrkit/index.html","id":"quantitative-text-analysis-for-linguistics-resources-kit","dir":"","previous_headings":"","what":"Quantitative Text Analysis for Linguists Resource Kit","title":"Quantitative Text Analysis for Linguists Resource Kit","text":"goal qtalrkit provide supporting resources book “Introduction Quantitative Text Analysis Linguistics: Reproducible Research using R”.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/add_pkg_to_bib.html","id":null,"dir":"Reference","previous_headings":"","what":"Add package to BibTeX file — add_pkg_to_bib","title":"Add package to BibTeX file — add_pkg_to_bib","text":"function adds package BibTeX file. uses knitr::write_bib function write package name file.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/add_pkg_to_bib.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add package to BibTeX file — add_pkg_to_bib","text":"","code":"add_pkg_to_bib(pkg_name, bib_file = \"packages.bib\")"},{"path":"https://qtalr.github.io/qtalrkit/reference/add_pkg_to_bib.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add package to BibTeX file — add_pkg_to_bib","text":"pkg_name name package add BibTeX file. bib_file name BibTeX file write .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/add_pkg_to_bib.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add package to BibTeX file — add_pkg_to_bib","text":"","code":"if (FALSE) { add_pkg_to_bib(\"dplyr\", \"my_bib_file.bib\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"function calculates various association metrics (PMI, Dice's Coefficient, Lambda-Rank) bigrams given corpus. data frame must contain document token indices, well 'type' variable representing tokens.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"","code":"calc_assoc_metrics(   data,   doc_index,   token_index,   type,   association = \"all\",   verbose = FALSE )"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"data data frame containing corpus. doc_index string name column 'data' represents document index. token_index string name column 'data' represents token index. type string name column 'data' represents tokens terms. association character vector specifying metrics calculate. Can combination 'pmi' (Pointwise Mutual Information), 'dice_coeff' (Dice's Coefficient), 'g_score' (G-score), '' (calculate metrics). Default ''. verbose logical value indicating whether keep intermediate probability columns ('p_xy', 'p_x', 'p_y') result. Default FALSE.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"data frame one row per bigram columns calculated metric. 'verbose' TRUE, intermediate probabilities used calculations also included.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Association Metrics for Bigrams — calc_assoc_metrics","text":"","code":"if (FALSE) { library(dplyr) data <- tibble::tibble(   doc_index = c(1, 1, 1, 2),   token_index = c(1, 2, 3, 1),   type = c(\"word1\", \"word2\", \"word3\", \"word2\") ) calc_assoc_metrics(data, doc_index, token_index, type) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Document Frequency (DF) — calc_df","title":"Calculate Document Frequency (DF) — calc_df","text":"function internal use .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Document Frequency (DF) — calc_df","text":"","code":"calc_df(tdm)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Document Frequency (DF) — calc_df","text":"tdm term-document matrix (TDM) row represents type column represents document.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Document Frequency (DF) — calc_df","text":"numeric vector containing Document Frequency 'DF' type TDM.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Document Frequency (DF) — calc_df","text":"function calculates Document Frequency 'DF' type (e.g., term, lemma) term-document matrix (TDM). intended used within package calc_dispersion_metrics function.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"function internal use .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"","code":"calc_dp(tdm_normalized, corpus_parts)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"tdm_normalized normalized term-document matrix (TDM) row represents type column represents document. values proportions type's frequency total frequency across documents. corpus_parts numeric vector containing proportions document corpus, used calculate Deviation Proportions (DP).","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"numeric vector containing Deviation Proportions (DP) type TDM.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_dp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Gries' Deviation of Proportions (DP) — calc_dp","text":"function calculates Deviation Proportions (DP) based Gries' Deviation Proportions method. intended used within package, particularly calc_dispersion_metrics function.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Inverse Document Frequency (IDF) — calc_idf","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"function internal use .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"","code":"calc_idf(tdm)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"tdm term-document matrix (TDM) row represents type column represents document.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"numeric vector containing Inverse Document Frequency 'DF' type TDM.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_idf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Inverse Document Frequency (IDF) — calc_idf","text":"function calculates Inverse Document Frequency 'IDF' type (e.g., term, lemma) term-document matrix (TDM). intended used within package calc_dispersion_metrics function.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"function takes categorical variable input calculates normalized entropy variable. normalized entropy measure amount uncertainty randomness variable, normalized maximum possible entropy variable.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"","code":"calc_normalized_entropy(x)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"x categorical variable.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"normalized entropy variable.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_normalized_entropy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the normalized entropy for a categorical variable. — calc_normalized_entropy","text":"","code":"if (FALSE) { # Calculate the normalized entropy of a vector of categorical data x <- c(\"A\", \"B\", \"B\", \"C\", \"C\", \"C\", \"D\", \"D\", \"D\", \"D\") calc_normalized_entropy(x) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Type Metrics for Text Data — calc_type_metrics","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"function calculates type metrics tokenized text data.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"","code":"calc_type_metrics(data, type, documents, frequency = NULL, dispersion = NULL)"},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"data data frame containing tokenized text data type variable data contains type (e.g., term, lemma) analyze. documents variable data contains document IDs. frequency character vector indicating frequency metrics use. NULL (default), type n returned. options: '', 'rf' calculates relative frequency, 'orf' calculates observed relative frequency. Can specify multiple options: c(\"rf\", \"orf\"). dispersion character vector indicating dispersion metrics use. NULL (default), type n returned. options: '', 'df' calculates Document Frequency. 'idf' calculates Inverse Document Frequency. 'dp' calculates Gries' Deviation Proportions. Can specify multiple options: c(\"df\", \"idf\").","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"data frame columns: type: unique types input data. n: frequency type across documents. Optionally (based frequency dispersion arguments): rf: relative frequency type across documents. orf: observed relative frequency (per 100) type across documents. df: document frequency type. idf: inverse document frequency type. dp: Gries' Deviation Proportions type.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"Gries, Stefan Th. (2023). Statistical Methods Corpus Linguistics. Readings Corpus Linguistics: Teaching Research Guide Scholars Nigeria Beyond, pp. 78-114.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Type Metrics for Text Data — calc_type_metrics","text":"","code":"if (FALSE) { data <- data.frame(   term = c(\"word1\", \"word1\", \"word2\", \"word2\", \"word2\", \"word3\"),   documents = c(\"doc1\", \"doc2\", \"doc1\", \"doc1\", \"doc2\", \"doc2\") ) calc_type_metrics(   data = data,   type = term,   documents = documents,   frequency = c(\"rf\", \"orf\"),   dispersion = c(\"df\", \"idf\") ) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/confirm_permission.html","id":null,"dir":"Reference","previous_headings":"","what":"Confirm permission to use data — confirm_permission","title":"Confirm permission to use data — confirm_permission","text":"function internal use .","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/confirm_permission.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confirm permission to use data — confirm_permission","text":"","code":"confirm_permission()"},{"path":"https://qtalr.github.io/qtalrkit/reference/confirm_permission.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confirm permission to use data — confirm_permission","text":"TRUE user confirms permission, FALSE otherwise","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/confirm_permission.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Confirm permission to use data — confirm_permission","text":"function confirms user permission use data. , script returns FALSE stops.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a data dictionary for a given data frame. — create_data_dictionary","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"function takes data frame creates data dictionary. data dictionary includes variable name, human-readable name, variable type, description. model specified, function uses OpenAI's API generate information based characteristics data frame.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"","code":"create_data_dictionary(   data,   file_path,   model = NULL,   sample_n = 5,   grouping = NULL,   force = FALSE )"},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"data data frame create data dictionary . file_path file path save data dictionary . model ID OpenAI chat completion models use generating descriptions (see openai::list_models()). NULL (default), scaffolding data dictionary created. sample_n number rows sample data frame use input model. Default NULL. grouping character vector column names group sampling rows data frame model. Default NULL. force TRUE, overwrite file file_path already exists. Default FALSE.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"data frame containing variable name, human-readable name, variable type, description variable input data frame.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_dictionary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a data dictionary for a given data frame. — create_data_dictionary","text":"","code":"if (FALSE) { data(mtcars) create_data_dictionary(mtcars, \"mtcars_data_dictionary.csv\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":null,"dir":"Reference","previous_headings":"","what":"Create data origin file — create_data_origin","title":"Create data origin file — create_data_origin","text":"function creates data frame attributes origin data, writes CSV file specified file path, returns data frame.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create data origin file — create_data_origin","text":"","code":"create_data_origin(file_path)"},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create data origin file — create_data_origin","text":"file_path character string specifying file path data origin file saved.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create data origin file — create_data_origin","text":"tibble containing data origin information.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/create_data_origin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create data origin file — create_data_origin","text":"","code":"if (FALSE) { create_data_origin(\"data_origin.csv\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Curate SWDA Data — curate_swda_data","title":"Curate SWDA Data — curate_swda_data","text":"function curates SWDA (Switchboard Dialog Act) data processing .utt files specified directory.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Curate SWDA Data — curate_swda_data","text":"","code":"curate_swda_data(dir_path)"},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Curate SWDA Data — curate_swda_data","text":"dir_path path directory containing .utt files.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Curate SWDA Data — curate_swda_data","text":"data frame containing curated SWDA data.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Curate SWDA Data — curate_swda_data","text":"","code":"if (FALSE) { curate_swda_data(\"/path/to/directory\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract SWDA data from a file — curate_swda_file","title":"Extract SWDA data from a file — curate_swda_file","text":"function reads file containing SWDA data extracts relevant information document ID, speaker IDs, text.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract SWDA data from a file — curate_swda_file","text":"","code":"curate_swda_file(file)"},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract SWDA data from a file — curate_swda_file","text":"file path file containing SWDA data.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract SWDA data from a file — curate_swda_file","text":"data frame containing extracted SWDA data, including document ID, speaker IDs, damsl tags, speaker turns, utterance numbers, utterance text.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/curate_swda_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract SWDA data from a file — curate_swda_file","text":"","code":"if (FALSE) { file <- \"/path/to/swda_data.txt\" swda_data <- extract_swda_data(file) head(swda_data) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify Outliers in a Numeric Variable — find_outliers","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"function identifies outliers numeric variable data.frame using interquartile range (IQR) method.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"","code":"find_outliers(data, variable_name)"},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"data data.frame object. variable_name symbol representing numeric variable data.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"data.frame containing outliers variable_name.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/find_outliers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Identify Outliers in a Numeric Variable — find_outliers","text":"","code":"if (FALSE) { data(mtcars) find_outliers(mtcars, mpg) find_outliers(mtcars, wt) }"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a Compressed File and Decompress its Contents — get_compressed_data","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"Possible file types include .zip, .gz, .tar, .tgz","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"","code":"get_compressed_data(url, target_dir, force = FALSE, confirmed = FALSE)"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"url character vector representing full url compressed file target_dir directory compressed file downloaded force optional argument forcefully overwrites existing data confirmed TRUE, user confirmed permission use data. FALSE, function prompt user confirm permission. Setting TRUE useful reproducible workflows.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"Download extract compressed data file","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_compressed_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"","code":"if (FALSE) { get_compressed_data(url = \"http://www.test.com/file.zip\", target_dir = \"./\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"Retrieves Gutenberg works based specified criteria saves data CSV file.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"","code":"get_gutenberg_works(   target_dir,   lcc_subject,   birth_year = NULL,   death_year = NULL,   force = FALSE,   confirmed = FALSE )"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"target_dir directory CSV file saved. lcc_subject character vector specifying Library Congress Classification (LCC) subjects filter works. birth_year optional integer specifying minimum birth year authors include. death_year optional integer specifying maximum death year authors include. force logical value indicating whether overwrite existing data already exists. confirmed logical value indicating whether skip confirmation prompt number works greater 1000.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"None","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"function retrieves Gutenberg works based specified LCC subjects optional author birth death years. checks data already exists target directory provides option overwrite . function also creates target directory exist. number works greater 1000 'confirmed' parameter set TRUE, prompts user confirmation. retrieved works filtered based public domain rights USA availability text. resulting works downloaded saved CSV file target directory. information Library Congress Classification (LCC) subjects, refer Library Congress Classification Guide.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_gutenberg_works.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieves Gutenberg works based on specified criteria and saves the data to a CSV file. — get_gutenberg_works","text":"","code":"if (FALSE) { # Retrieve works with LCC subject \"Political Theory\" and save to \"/path/to/works_fiction.csv\" get_gutenberg_works(\"/path/to\", \"JC\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Downloads TalkBank data and saves it to disk — get_talkbank_data","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"Downloads utterances, transcripts, participants, tokens, token types data TalkBank database saves disk specified target directory.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"","code":"get_talkbank_data(   corpus_name,   corpus_path,   target_dir,   force = FALSE,   confirmed = FALSE )"},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"corpus_name name TalkBank corpus download data . corpus_path path TalkBank corpus download data . target_dir directory save downloaded data . force TRUE, data downloaded even already exists disk. confirmed TRUE, user confirmed permission use data. FALSE, function prompt user confirm permission. Setting TRUE useful reproducible workflows.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"message indicating whether data acquired already existed disk.","code":""},{"path":"https://qtalr.github.io/qtalrkit/reference/get_talkbank_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Downloads TalkBank data and saves it to disk — get_talkbank_data","text":"","code":"if (FALSE) { # Download CABNC data from the Conversation Bank to a directory called \"data\" get_talkbank_data(corpus_name = \"ca\", corpus_path = c(\"ca\", \"CABNC\"), target_dir = \"data\") }"},{"path":"https://qtalr.github.io/qtalrkit/reference/qtalrkit-package.html","id":null,"dir":"Reference","previous_headings":"","what":"qtalrkit: Quantitative Text Analysis for Linguists Resource Kit — qtalrkit-package","title":"qtalrkit: Quantitative Text Analysis for Linguists Resource Kit — qtalrkit-package","text":"Support package textbook \"Introduction Quantitative Text Analysis Linguists: Reproducible Research using R\". Includes access interactive code exercises demos, data, misc functions.","code":""},{"path":[]},{"path":"https://qtalr.github.io/qtalrkit/reference/qtalrkit-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"qtalrkit: Quantitative Text Analysis for Linguists Resource Kit — qtalrkit-package","text":"Maintainer: Jerid Francom francojc@wfu.edu (ORCID)","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-092","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.9.2","title":"qtalrkit 0.9.2","text":"Updates recipes","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-091","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.9.1","title":"qtalrkit 0.9.1","text":"Changes mirror “https://gutenberg.pglaf.org/” get_gutenberg_works() function Organizes output include LCC classification part data written disk get_gutenberg_works() function","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-090","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.9.0","title":"qtalrkit 0.9.0","text":"Updated version number reflect package now beta Added curate_swda_data() function curate data Switchboard Dialog Act Corpus","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0040","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.4.0","title":"qtalrkit 0.0.4.0","text":"Adds get_gutenberg_works() function import data Project Gutenberg","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003400","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.400","title":"qtalrkit 0.0.3.400","text":"Fixes warnings calc_assoc_metrics() Updates Date DESCRIPTION file Adds test-add_pkg_to_bib.R","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.000","title":"qtalrkit 0.0.3.000","text":"Adds calc_assoc_metrics calculate (pmi, dice, G) given type bigram","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003210","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.210","title":"qtalrkit 0.0.3.210","text":"Removes calc_dispersion_metrics() function replaces calc_type_metric() includes frequency dispersion metrics.","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003200","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.200","title":"qtalrkit 0.0.3.200","text":"Fixes bug get_compressed_data() caused function create dot file copies original files","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003100","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.100","title":"qtalrkit 0.0.3.100","text":"Adds idf measure calc_dispersion_metrics()","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-003000-1","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.3.000","title":"qtalrkit 0.0.3.000","text":"Adds calc_dispersion_metrics() function calculate dispersion metrics","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-002000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.2.000","title":"qtalrkit 0.0.2.000","text":"Added get_talkbank_data() function import data TalkBank Added internal confirm_permissions() function confirm users aware permissions required use data Updated get_*() functions use confirm_permissions() internally Changed get_outliers() find_outliers() consistent functions","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019400","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9400","title":"qtalrkit 0.0.1.9400","text":"Updated create_data_dictionary() provide default scaffold structure data dictionary, lieu OpenAI model. scaffold updated manually user.","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019300","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9300","title":"qtalrkit 0.0.1.9300","text":"Added create_data_origin() function. creates .csv file scaffold data origin file","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019200","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9200","title":"qtalrkit 0.0.1.9200","text":"Adds project template RStudio: “Minimal Reproducible Project”","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019100","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9100","title":"qtalrkit 0.0.1.9100","text":"Adjusted create_data_dictionary() produce results line QTALR textbook","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0019000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.9000","title":"qtalrkit 0.0.1.9000","text":"Added get_outliers() function Added Instructor Guide","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0010000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.1.0000","title":"qtalrkit 0.0.1.0000","text":"Added R tutorial 0","code":""},{"path":"https://qtalr.github.io/qtalrkit/news/index.html","id":"qtalrkit-0009000","dir":"Changelog","previous_headings":"","what":"qtalrkit 0.0.0.9000","title":"qtalrkit 0.0.0.9000","text":"Added NEWS.md file track changes package.","code":""}]
