---
title: "7. Transforming and documenting data"
pkgdown:
  as_is: true
bibliography: [bibliography.bib, packages.bib]
biblio-style: apalike
link-citations: true
---

```{r, child="_common.Rmd"}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

<!--

Purpose: To introduce the concept of transforming data to prepare it for analysis. This includes text normalization and tokenization as well as the creation of new variables by splitting, merging, and recoding existing variables.

Description: Building on our understanding of tidy data, we now look at ways to transform data to bring it more in line with our research goals. This includes modifying the unit of observation and/ or adding additional attributes to the data. We will cover some of the basic operations and make reference to the fact that these operations can be performed in a variety of ways and each project will have its own unique requirements.

Approach:

Prerequisites: This recipe assumes a basic understanding of the concepts of and methods for data acquisition and tidying data. This includes experience with the readr, stringr, dplyr, and tidyr packages. It also assumes basic familiarity with regular expressions.

Context: This is the third of three tutorials on data preparation. The first tutorial introduced the concept of data acquisition and the second tutorial introduced the concept of tidying data. This tutorial will introduce the concept of transforming data. The accompanying lab for this recipe will continue with the options for forking/ cloning a project from GitHub, working with an existing project, and creating a new project. The lab is designed to be completed in a 50 minute class period.
-->

The curated dataset reflects a tidy version of the original data. This data is relatively project-neutral. A such, project-specific changes are often made to bring the data more in line with the research goals. This may include modifying the unit of observation and/ or adding additional attributes to the data. This process may generate one or more new datasets that are used for analysis.

In this recipe, we will explore a practical example of transforming data. This will include operations such as:

- Text normalization and tokenization
- Creating new variables by splitting, merging, and recoding existing variables
- Augmenting data with additional variables from other sources or resources

Along the way, we will employ a variety of tools and techniques to accomplish these tasks. Let's load the packages we will need for this recipe.

```{r load-packages, message=FALSE, warning=FALSE}
# Load packages
library(readr)
library(dplyr)
library(stringr)
library(tidyr)
library(tidytext)
library(qtalrkit)
```

In Lab 7, we will apply what we have learned in this recipe to a new dataset.

## Concepts and strategies

### Orientation

- ANC corpus the MASC corpus.
-



### Transforming data

### Documenting data

## Summary

## Check your understanding

(... examples ...)

1. `r torf(TRUE)` Literate Programming, first introduced by Donald Knuth in 1984, allows the combination of computer code and text prose in one document.
2. The programming paradigm Literate Programming is implemented through `r mcq(c(answer = "Quarto", "R", "RStudio", "GitHub"))`, a platform that facilitates the creation of a variety of output documents based on the same source code.
3. Which of the following components does a basic Quarto document *not* contain? `r mcq(c("Front-matter section", "Prose section", answer = "Back-matter section", "Code block"))`
4. To generate a PDF document in Quarto, you can edit the format attribute value in the front-matter section to `r fitb(c(answer = "pdf", "html", "word"), ignore_case = TRUE)`.
5. `r torf(TRUE)` The code block options `echo` and `include` can be used to hide the code and output, respectively.
6. `r torf(FALSE)` In Quarto, a code block, where the programming language code is entered, is bounded by three underscores (`_`).

## Lab prepation

## References
