---
title: "5. Collecting and documenting data"
pkgdown:
  as_is: true
bibliography: [bibliography.bib, packages.bib]
biblio-style: apalike
link-citations: true
---

<!--  
The goal will be to do one, more advanced example

- `gutenbergr` example
  - Literature: American and English Literature
  - Metadata: `gutenberg_metadata`, `gutenberg_authors`, `gutenberg_subjects`
  - Directory structure
  - Data origin file
-->


```{r, child="_common.Rmd"}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Overview

In this recipe, we will take a closer look at the data collection and documentation process. Just as in Acquire data chapter, we will group the data collection process into three types of collection strategies: **downloading**, **APIs**, and **web scraping**. We will also discuss the importance of documenting the data collection process including the creation and completion of a data origin file.

In the process we will employ the following R skills: 

- control statements
  - `if` statements
  - `message` function
  - `stop` function (`.call = FALSE`)
- custom functions
  - required and optional arguments (setting default values)
  - argument checks (with `if` statements)
  - `return` function
- Vectorized Operations
  - Iteration functions in R
    - `purr::map()`

We will also use the following R packages:

- `here` (?)
- `readr`
- `fs`
- `dplyr`
- `qtalrkit`
- `gutenbergr`
- `rvest`
- `xml2`

## Cases

<!-- Programmatic downloads -->

- [ ] Integrate the **Santa Barbara Corpus of Spoken American English** (SBCSAE) into the discussion on downloading compressed files from the internet.
  - [ ] This will provide the opportunity to discuss the `unzip()` function.
  - [ ] Also an opportunity to discuss a custom function to download various types of compressed data, e.g. `.zip`, `.tar.gz`, `.tar.bz2`, etc.
    - [ ] `qtalrkit::get_compressed_data()`
      - [ ] Includes a `junkpaths` argument to discard the containing directory of the compressed file.
      - [ ] Include `force = TRUE` argument to overwrite existing data.
      - [ ] Includes `message()` and `stop()`/ `invisible()` functions to provide status messages and stop the function if the arguments are not specified correctly.


<!-- APIs  -->

- Literary genres data collection (API: `gutenbergr` package)
  - [ ] **Project Gutenberg** (work on fixing or changing the data that is accessed)
    - [ ] Write custom function to download data and metadata from Project Gutenberg


To get started let's install and/ or load the `gutenbergr` package. If a package is not part of the R base library, we cannot assume that the user will have the package in their library. The standard approach for installing and then loading a package is by using the `install.packages()` function and then calling `library()`.

```{r}
#| label: ad-standard-install-load-gutenbergr
#| eval: false

install.packages("gutenbergr") # install `gutenbergr` package
library(gutenbergr) # load the `gutenbergr` package
```

<!-- Web scraping -->
- [SOTU data collection (1790-2006)](https://www.gutenberg.org/ebooks/5050)
  - Note that the Project Gutenberg site explicitly requests that [only human users access the site](https://www.gutenberg.org/policy/terms_of_use.html). There is a metadata catalog file here: https://www.gutenberg.org/cache/epub/feeds/
  - Scraping one page may not be a problem, but scraping multiple pages (~100) may be a problem.
    - > Blocks to Internet addresses (IP addresses) are applied automatically based on the volume of traffic and related factors. Such blocks automatically expire after a few days.

---


### Downloads

Boilerplate information for the SBCSAE (Santa Barbara Corpus of Spoken American English): 

```{r}
#| label: ad-get-zip-data-function
#| eval: false

get_zip_data <- function(url, target_dir) {
  # Function: to download and decompress a .zip file to a target directory

  # Check to see if the data already exists
  if (!dir.exists(target_dir)) { # if data does not exist, download/ decompress
    cat("Creating target data directory \n") # print status message
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat("Downloading data... \n") # print status message
    temp <- tempfile() # create a temporary space for the file to be written to
    download.file(url = url, destfile = temp) # download the data to the temp file
    unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE) # decompress the temp file in the target directory
    cat("Data downloaded! \n") # print status message
  } else { # if data exists, don't download it again
    cat("Data already exists \n") # print status message
  }
}
```

OK. You should have recognized the general steps in this function: the argument `url` and `target_dir` specify where to get the data and where to write the decompressed files, the `if()` statement evaluates whether the data already exists, if not (`!dir.exists(target_dir)`) then the data is downloaded and decompressed, if it does exist (`else`) then it is not downloaded.

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

The prefixed `!` in the logical expression `dir.exists(target_dir)` returns the opposite logical value. This is needed in this case so when the target directory exists, the expression will return `FALSE`, not `TRUE`, and therefore not proceed in downloading the resource.
:::

There are a couple key tweaks I've added that provide some additional functionality. For one I've included the function `dir.create()` to create the target directory where the data will be written. I've also added an additional argument to the `unzip()` function, `junkpaths = TRUE`. Together these additions allow the user to create an arbitrary directory path where the files, and only the files, will be extracted to on our disk. This will discard the containing directory of the `.zip` file which can be helpful when we want to add multiple `.zip` files to the same target directory.

A practical scenario where this applies is when we want to download data from a corpus that is contained in multiple `.zip` files but still maintain these files in a single primary data directory. Take for example the [Santa Barbara Corpus](http://www.linguistics.ucsb.edu/research/santa-barbara-corpus). This corpus resource includes a series of interviews in which there is one `.zip` file, `SBCorpus.zip` which contains the [transcribed interviews](http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip) and another `.zip` file, `metadata.zip` which organizes the [meta-data](http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip) associated with each speaker. Applying our initial strategy to download and decompress the data will lead to the following directory structure:

```
data
├── derived
└── original
    ├── SBCorpus
    │   ├── TRN
    │   └── __MACOSX
    │       └── TRN
    └── metadata
        └── __MACOSX
```

By applying our new custom function `get_zip_data()` to the transcriptions and then the meta-data we can better organize the data.

```{r}
#| label: ad-get-zip-data-sbc
#| eval: false

# Download corpus transcriptions
get_zip_data(
  url = "http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip", 
  target_dir = "../data/original/sbc/transcriptions/"
)

# Download corpus meta-data
get_zip_data(
  url = "http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip", 
  target_dir = "../data/original/sbc/meta-data/"
)
```

```
data
├── derived
└── original
    └── sbc
        ├── meta-data
        └── transcriptions
```

If we add data from other sources we can keep them logical separate and allow our data collection to scale without creating unnecessary complexity. Let's add the Switchboard Corpus sample using our `get_zip_data()` function to see this in action.

```{r}
#| label: ad-get-zip-data-scs
#| eval: false

# Download corpus
get_zip_data(
  url = "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip", 
  target_dir = "../data/original/scs/"
)
```

```
data
├── derived
└── original
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript
```

### APIs


Now that we have `pacman` installed and loaded into our R session, let's use the `p_load()` function to make sure to install/ load the two packages we will need for the upcoming tasks. If you are following along with the `project_template`, add this code within the SETUP section of the `1_acquire_data.Rmd` file.

```{r}
#| label: ad-p-load-packages
#| eval: false

# Script-specific options or packages
pacman::p_load(dplyr, gutenbergr)
```

::: {.callout}
**`r fontawesome::fa('hand-point-up')` Tip**

Note that the arguments `dplyr` and `gutenbergr` are comma-separated but not quoted when using `p_load()`. When using `install.packages()` to install, package names need to be quoted (character strings). `library()` can take quotes or no quotes, but only one package at a time.
:::

Project Gutenberg provides access to thousands of texts in the public domain. The `gutenbergr` package contains a set of tables, or **data frames** in R speak, that index the meta-data for these texts broken down by text (`gutenberg_metadata`), author (`gutenberg_authors`), and subject (`gutenberg_subjects`). I'll use the `glimpse()` function loaded in the [tidyverse](https://CRAN.R-project.org/package=tidyverse) package [^acquire-data-3] to summarize the structure of these data frames.

[^acquire-data-3]: `tidyverse` is not a typical package. It is a set of packages: `ggplot2`, `dplyr`, `tidyr`, `readr`, `purrr`, and `tibble`. These packages are all installed/ loaded with `tidyverse` and form the backbone for the type of work you will typically do in most analyses.

```{r}
#| label: ad-glimpse-meta-data-gutenbergr
#| eval: false
glimpse(gutenberg_metadata) # summarize text meta-data
glimpse(gutenberg_authors) # summarize authors meta-data
glimpse(gutenberg_subjects) # summarize subjects meta-data
```

::: callout-warning
**Tip**

The `gutenberg_metadata`, `gutenberg_authors`, and `gutenberg_subjects` are periodically updated. To check to see when each data frame was last updated run:

`attr(gutenberg_metadata, "date_updated")`
:::

To download the text itself we use the `gutenberg_download()` function which takes one required argument, `gutenberg_id`. The `gutenberg_download()` function is what is known as 'vectorized', that is, it can take a single value or multiple values for the argument `gutenberg_id`. Vectorization refers to the process of applying a function to each of the elements stored in a **vector** --a primary object type in R. A vector is a grouping of values of one of various types including character (`chr`), integer (`int`), double (`dbl`), and logical (`lgl`) and a data frame is a grouping of vectors. The `gutenberg_download()` function takes an integer vector which can be manually added or selected from the `gutenberg_metadata` or `gutenberg_subjects` data frames using the `$` operator (e.g. `gutenberg_metadata$gutenberg_id`).

Let's first add them manually here as a toy example by generating a vector of integers from 1 to 5 assigned to the variable name `ids`.

```{r}
#| label: ad-int-vector-ex
#| eval: false
ids <- 1:5 # integer vector of values 1 to 5
ids
```

To download the works from Project Gutenberg corresponding to the `gutenberg_id`s 1 to 5, we pass the `ids` object to the `gutenberg_download()` function.

```{r}
#| label: ad-download-ids-gutenbergr
#| eval: false

works_sample <- gutenberg_download(gutenberg_id = ids) # download works with `gutenberg_id` 1-5
glimpse(works_sample) # summarize `works` dataset
```

Two attributes are returned: `gutenberg_id` and `text`. The `text` column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded. There are many more attributes available from the Project Gutenberg API that can be accessed by passing a character vector of the attribute names to the argument `meta_fields`. The column names of the `gutenberg_metadata` data frame contains the available attributes.

```{r}
#| label: ad-metadata-attributes-gutenbergr
#| eval: false
names(gutenberg_metadata) # print the column names of the `gutenberg_metadata` data frame
```

Let's augment our previous download with the title and author of each of the works. To create a character vector we use the `c()` function, then, quote and delimit the individual elements of the vector with a comma.

```{r}
#| label: ad-download-ids-meta-gutenbergr
#| eval: false
# download works with `gutenberg_id` 1-5 including `title` and `author` as attributes
works_sample <-
  gutenberg_download(
    gutenberg_id = ids,
    meta_fields = c("title", "author")
  ) #

glimpse(works_sample) # summarize dataset
```

Now, in a more practical scenario we would like to select the values of `gutenberg_id` by some principled query such as works from a specific author, language, or subject. To do this we first query either the `gutenberg_metadata` data frame or the `gutenberg_subjects` data frame. Let's say we want to download a random sample of 10 works from English Literature (Library of Congress Classification, "PR"). Using the `dplyr::filter()` function (`dplyr` is part of the `tidyverse` package set) we first extract all the Gutenberg ids from `gutenberg_subjects` where `subject_type == "lcc"` and `subject == "PR"` assigning the result to `ids`.[^acquire-data-4]

[^acquire-data-4]: See [Library of Congress Classification](https://www.loc.gov/catdir/cpso/lcco/) documentation for a complete list of subject codes.

```{r}
#| label: ad-eng-lit-subjects-gutenbergr
#| eval: false
# filter for only English literature
ids <-
  filter(gutenberg_subjects, subject_type == "lcc", subject == "PR")
glimpse(ids)
```

::: callout-warning
**Tip**

The operators `=` and `==` are not equivalents. `==` is used for logical evaluation and `=` is an alternate notation for variable assignment (`<-`).
:::

The `gutenberg_subjects` data frame does not contain information as to whether a `gutenberg_id` is associated with a plain-text version. To limit our query to only those English Literature works with text, we filter the `gutenberg_metadata` data frame by the ids we have selected in `ids` and the attribute `has_text` in the `gutenberg_metadata` data frame.

```{r}
#| label: ad-eng-lit-subjects-text-gutenbergr
#| eval: false

# Filter for only those works that have text
ids_has_text <-
  filter(
    gutenberg_metadata,
    gutenberg_id %in% ids$gutenberg_id,
    has_text == TRUE
  )
glimpse(ids_has_text)
```

::: callout-warning
**Tip**

A couple R programming notes on the code phrase `gutenberg_id %in% ids$gutenberg_id`. First, the `$` symbol in `ids$gutenberg_id` is the programmatic way to target a particular column in an R data frame. In this example we select the `ids` data frame and the column `gutenberg_id`, which is a integer vector. The `gutenberg_id` variable that precedes the `%in%` operator does not need an explicit reference to a data frame because the primary argument of the `filter()` function is this data frame (`gutenberg_metadata`). Second, the `%in%` operator logically evaluates whether the vector elements in `gutenberg_metadata$gutenberg_ids` are also found in the vector `ids$gutenberg_id` returning `TRUE` and `FALSE` accordingly. This effectively filters those ids which are not in both vectors.
:::

As we can see the number of works with text is fewer than the number of works listed, `nrow(ids)` versus `nrow(ids_has_text)`. Now we can safely do our random selection of 10 works, with the function `slice_sample()` and be confident that the ids we select will contain text when we take the next step by downloading the data.

```{r}
#| label: ad-select-eng-lit-gutenbergr
#| eval: false
set.seed(123) # make the sampling reproducible
ids_sample <- slice_sample(ids_has_text, n = 10) # sample 10 works
glimpse(ids_sample) # summarize the dataset
```

```{r}
#| label: ad-eng-lit-download-gutenbergr
#| eval: false

works_pr <- gutenberg_download(
  gutenberg_id = ids_sample$gutenberg_id,
  meta_fields = c("author", "title")
)
glimpse(works_pr) # summarize the dataset
```

```{r}
#| label: ad-eng-lit-download-gutenbergr-save
#| eval: false

works_pr <- gutenberg_download(
  gutenberg_id = ids_sample$gutenberg_id,
  meta_fields = c("author", "title")
)
write_rds(works_pr, file = "data/acquire-data/gutenberg_works_pr.rds")
```

```{r}
#| label: ad-eng-lit-download-gutenbergr-show
#| echo: false
#| eval: false
works_pr <- read_rds(file = "data/acquire-data/gutenberg_works_pr.rds") # read rds file from disk
glimpse(works_pr) # summarize the dataset
```


To avoid downloading dataset that already resides on disk, let's implement a similar strategy to the one used for direct downloads (`get_zip_data()`). I've incorporated the code for sampling and downloading data for a particular subject from Project Gutenberg with a control statement to check if the dataset file already exists into a function I named `get_gutenberg_subject()`. Take a look at this function below.

```{r}
#| label: ad-get-gutenberg-subject-fun
#| eval: false
get_gutenberg_subject <- function(subject, target_file, sample_size = 10) {
  # Function: to download texts from Project Gutenberg with 
  # a specific LCC subject and write the data to disk.
  
  pacman::p_load(dplyr, gutenbergr) # install/load necessary packages
  
  # Check to see if the data already exists
  if(!file.exists(target_file)) { # if data does not exist, download and write
    target_dir <- dirname(target_file) # generate target directory for the .csv file
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat("Downloading data... \n") # print status message
    # Select all records with a particular LCC subject
    ids <- 
      filter(gutenberg_subjects, 
             subject_type == "lcc", subject == subject) # select subject
    # Select only those records with plain text available
    set.seed(123) # make the sampling reproducible
    ids_sample <- 
      filter(gutenberg_metadata, 
             gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames 
             has_text == TRUE) |> # select those ids that have text
      slice_sample(n = sample_size) # sample N works 
    # Download sample with associated `author` and `title` metadata
    works_sample <- 
      gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, 
                         meta_fields = c("author", "title"))
    # Write the dataset to disk in .csv format
    write_csv(works_sample, file = target_file)
    cat("Data downloaded! \n") # print status message
  } else { # if data exists, don't download it again
    cat("Data already exists \n") # print status message
  }
}
```

Adding this function to our function script `functions/acquire_functions.R`, we can now source this function in our `analysis/1_acquire_data.Rmd` script to download multiple subjects and store them in on disk in their own file.

Let's download American Literature now (LCC code "PQ").

```{r}
#| label: ad-get-am-lit
#| eval: false

# Download Project Gutenberg text for subject 'PQ' (American Literature)
# and then write this dataset to disk in .rds format

# Select all records with a particular LCC subject
ids <-
  filter(
    gutenberg_subjects,
    subject_type == "lcc", subject == "PQ"
  ) # select subject
# Select only those records with plain text available
set.seed(123) # make the sampling reproducible
ids_sample <-
  filter(
    gutenberg_metadata,
    gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames
    has_text == TRUE
  ) |> # select those ids that have text
  slice_sample(n = sample_size) # sample N works
# Download sample with associated `author` and `title` metadata
works_pq <-
  gutenberg_download(
    gutenberg_id = ids_sample$gutenberg_id,
    meta_fields = c("author", "title")
  )

write_rds(x = works_pq, file = "data/acquire-data/gutenberg_works_pq.rds")
```

```{r}
#| label: ad-get-am-lit-show
#| eval: false

# Download Project Gutenberg text for subject 'PQ' (American Literature)
# and then write this dataset to disk in .csv format
get_gutenberg_subject(subject = "PQ", target_file = "../data/original/gutenberg/works_pq.csv")
```

Applying this function to both the English and American Literature datasets, our data directory structure now looks like this:

``` bash
data
├── derived
└── original
    ├── gutenberg
    │   ├── works_pq.csv
    │   └── works_pr.csv
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs
        ├── README
        ├── discourse
        ├── disfluency
        ├── documentation
        ├── tagged
        ├── timed-transcript
        └── transcript
```

### Web Scraping



## Summary

## Check your understanding

(... examples ...)

1. `r torf(TRUE)` Literate Programming, first introduced by Donald Knuth in 1984, allows the combination of computer code and text prose in one document. 
2. The programming paradigm Literate Programming is implemented through `r mcq(c(answer = "Quarto", "R", "RStudio", "GitHub"))`, a platform that facilitates the creation of a variety of output documents based on the same source code.
3. Which of the following components does a basic Quarto document *not* contain? `r mcq(c("Front-matter section", "Prose section", answer = "Back-matter section", "Code block"))`
4. To generate a PDF document in Quarto, you can edit the format attribute value in the front-matter section to `r fitb(c(answer = "pdf", "html", "word"), ignore_case = TRUE)`.
5. `r torf(TRUE)` The code block options `echo` and `include` can be used to hide the code and output, respectively.
6. `r torf(FALSE)` In Quarto, a code block, where the programming language code is entered, is bounded by three underscores (`_`).

## References