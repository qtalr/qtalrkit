---
title: "6. Organizing and documenting data"
pkgdown:
  as_is: true
bibliography: [bibliography.bib, packages.bib]
biblio-style: apalike
link-citations: true
---

<!--  
The goal will be to do one, more advanced example

- PELIC
  - CSV file `compiled` with nested `tokens` column
  - Data checks
  - Data dictionary

-->

```{r, child="_common.Rmd"}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Overview

Skills:

- Pattern Matching
  - Regular Expressions (basics)
    - Literal characters: `a`, `b`, `c`
    - Special sequences: `\d`, `\w`, `\s`
    - Metacharacters: `.`, `^`, `$`, `*`, `+`, `?`, `\`
    - Sets: `[abc]`, `[^abc]`, `[a-z]`, `[a-zA-Z]`
- Manipulate datasets
  - `dplyr` verbs: `select()`, `filter()`, `arrange()`, `mutate()`, `summarise()`
  - tidyselect: `starts_with()`, `ends_with()`, `contains()`, `matches()`, `one_of()`, `everything()`
  - ...

Packages: 

- `stringr`
- `dplyr`
- `tidyselect`
- `purrr`

## Cases: 

### Unstructured: ANC (.txt) files



### Structured: Wordbank (.csv) files



### Semi-structured: SWDA (.utt) files



#### Reading data

- [ ] cover the basics of reading semi-structured data

#### Orientation

- [ ] this SWDA data has been mentioned, reference is all that is needed.

As an example we will work with the The Switchboard Dialog Act Corpus (SDAC) which extends the [Switchboard Corpus](https://catalog.ldc.upenn.edu/LDC97S62) with speech act annotation. **(ADD CITATION)**

The main directory structure of the SDAC data looks like this:

```bash
data/
├── derived/
└── original/
    └── sdac/
        ├── README
        ├── doc/
        ├── sw00utt/
        ├── sw01utt/
        ├── sw02utt/
        ├── sw03utt/
        ├── sw04utt/
        ├── sw05utt/
        ├── sw06utt/
        ├── sw07utt/
        ├── sw08utt/
        ├── sw09utt/
        ├── sw10utt/
        ├── sw11utt/
        ├── sw12utt/
        └── sw13utt/
```

The `README` file contains basic information about the resource, the `doc/` directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with `sw...` contain individual conversation files. Here's a peek at internal structure of the first couple directories.

```bash
├── README
├── doc
│   └── manual.august1.html
├── sw00utt
│   ├── sw_0001_4325.utt
│   ├── sw_0002_4330.utt
│   ├── sw_0003_4103.utt
│   ├── sw_0004_4327.utt
│   ├── sw_0005_4646.utt
```

Let's take a look at the first conversation file (`sw_0001_4325.utt`) to see how it is structured.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-preview
#| echo: false
#| comment: ">"
#| linewidth: 0.9

readtext::readtext("data/understanding-data/formats_sdac_sample.txt", verbosity = 0) |> # sw_0001_4325.utt
  pull(text) |>
  cat(fill = TRUE)
```

There are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of `=` characters. Second the header contains meta-information of various types. Third, the text is interleaved with an annotation scheme. 

Some of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let's take a look at the `README` file. In this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 'DAMSL' dialog act labels. The `README` file refers us to the `doc/manual.august1.html` file for more information on this scheme.

At this point we open the the `doc/manual.august1.html` file in a browser and do some investigation. We find out that 'DAMSL' stands for 'Discourse Annotation and Markup System of Labeling' and that the first characters of each line of the conversation text  correspond to one or a combination of labels for each utterance. So for our first utterances we have:

```plain
o = "Other"
qw = "Wh-Question"
qy^d = "Declarative Yes-No-Question"
+ = "Segment (multi-utterance)"
```

Each utterance is also labeled for speaker ('A' or 'B'), speaker turn ('1', '2', '3', etc.), and each utterance within that turn ('utt1', 'utt2', etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.

Now let's turn to the meta-data in the header. We see here that there is information about the creation of the file: 'FILENAME', 'TOPIC', 'DATE', etc. The `doc/manual.august1.html` file doesn't have much to say about this information so I returned to the [LDC Documentation](https://catalog.ldc.upenn.edu/docs/LDC97S62/) and found more information in the [Online Documentation](https://catalog.ldc.upenn.edu/docs/LDC97S62/) section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the `caller_tab.csv` file. This tabular file does not contain column names, but the `caller_doc.txt` does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the 'FILENAME' information contained three pieces of useful information delimited by underscores `_`. 

```plain
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*


FILENAME:	4325_1632_1519
TOPIC#:		323
DATE:		920323
TRANSCRIBER:	glp
```

The first information is the document id (`4325`), the second and third correspond to the speaker number: the first being speaker A (`1632`) and the second speaker B (`1519`).

In sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of `=` characters. The header section contains a 'FILENAME' line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let's set out to create a tidy dataset with the following column structure:

```{r}
#| label: tbl-cd-semi-sdac-idealized-dataset
#| tbl-cap: "Idealized structure for the SDAC dataset."
#| echo: false

# Sample data (created with `dput()`)

structure(list(doc_id = c("4325", "4325", "4325"), damsl_tag = c(
  "o ",
  "qw ", "qy^d "
), speaker = c("A", "A", "B"), turn_num = c(
  "1",
  "1", "2"
), utterance_num = c("1", "2", "1"), utterance_text = c(
  "Okay.  /",
  "{D So, }", "[ [ I guess, +"
), speaker_id = c(
  "1632", "1632",
  "1519"
)), .Names = c(
  "doc_id", "damsl_tag", "speaker", "turn_num",
  "utterance_num", "utterance_text", "speaker_id"
), row.names = c(
  NA,
  3L
), class = "data.frame") |>
  knitr::kable(booktabs = TRUE)
```

#### Tidy the data

- [ ] Add idealized structure for the SWDA dataset

```{r}
#| eval: false
#| label: cd-semi-copy-data
#| include: false

# [ ] careful with this, it depends on the location of the project. It will be best to copy the data to the project ignored data/ directory for this chapter

# copy the original data from the `project_implementation` project
fs::dir_copy(path = "../project_implementation/data/original/sdac/", new_path = "data/curate-datasets/sdac/")
```

Let's begin by reading one of the conversation files into R as a character vector using the `read_lines()` function from the readr package.

```{r}
#| eval: false
#| label: cd-semi-sdac-read-example-file-show

# [ ] is it possible to use readtext, if not, then we need to explain why before jumping in to readr::read_lines()

doc <-
  read_lines(file = "../data/original/sdac/sw00utt/sw_0001_4325.utt") # read a single file as character vector
```

```{r}
#| eval: false
#| label: cd-semi-sdac-read-example-file-run
#| echo: false

doc <-
  read_lines(file = "data/curate-datasets/sdac/sw00utt/sw_0001_4325.utt") # read a single file as character vector
```

To isolate the vector element that contains the document and speaker ids, we use `str_detect()` from the stringr package. This function takes two arguments, a string and a pattern, and returns a logical value, `TRUE` if the pattern is matched or `FALSE` if not. We can use the output of this function, then, to subset the `doc` character vector and only return the vector element (line) that contains `digits_digits_digits` with a regular expression. The expression combines the digit matching operator `\\d` with the `+` operator to match 1 or more contiguous digits. We then separate three groups of `\\d+` with underscores `_`. The result is `\\d+_\\d+_\\d+`. 

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-1

doc[str_detect(doc, pattern = "\\d+_\\d+_\\d+")] # isolate pattern
```

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

Regular Expressions are a powerful pattern matching syntax. They are used extensively in text manipulation and we will see them again and again.

To develop regular expressions, it is helpful to have a tool that allows you to interactively test your pattern matching. The stringr package has a handy function `str_view()` and `str_view_all()` which allow for interactive pattern matching. A good website to practice Regular Expressions is [RegEx101](https://regex101.com/). You can also install the regexplain package in R to get access to a useful [RStudio Addin](https://rstudio.github.io/rstudioaddins/). 
:::

The next step is to extract the three digit sequences that correspond to the `doc_id`, `speaker_a_id`, and `speaker_b_id`. First we extract the pattern that we have identified with `str_extract()` and then we can break up the single character vector into multiple parts based on the underscore `_`. The `str_split()` function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors. 

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-2

doc[str_detect(doc, "\\d+_\\d+_\\d+")] |> # isolate pattern
  str_extract(pattern = "\\d+_\\d+_\\d+") |> # extract the pattern
  str_split(pattern = "_") # split the character vector
```

- [] I will have introduced lists in earlier material (swirl Objects), at least.

A **list** is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same --hence the tabular format). In this case we have a list of length 1, whose sole element is a character vector of length 3 --one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our `str_split()` function we don't want the results to be conflated as a single character vector blurring the distinction between the individual character vectors. If we *would* like to conflate, or *flatten* a list, we can use the `unlist()` function.

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-3

doc[str_detect(doc, "\\d+_\\d+_\\d+")] |> # isolate pattern
  str_extract(pattern = "\\d+_\\d+_\\d+") |> # extract the pattern
  str_split(pattern = "_") |> # split the character vector
  unlist() # flatten the list to a character vector
```

Let's flatten the list in this case, as we have a single character vector, and assign this result to `doc_speaker_info`.

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-4

doc_speaker_info <-
  doc[str_detect(doc, "\\d+_\\d+_\\d+")] |> # isolate pattern
  str_extract(pattern = "\\d+_\\d+_\\d+") |> # extract the pattern
  str_split(pattern = "_") |> # split the character vector
  unlist() # flatten the list to a character vector
```

`doc_speaker_info` is now a character vector of length three. Let's subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-5

doc_id <- doc_speaker_info[1] # extract by index
speaker_a_id <- doc_speaker_info[2] # extract by index
speaker_b_id <- doc_speaker_info[3] # extract by index
```

The next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of `=` separates the header section from the text section. What we need to do is to index the point in our character vector `doc` where that line occurs and then subset the `doc` from that point until the end of the character vector. Let's first find the point where the `=` sequence occurs. We will again use the `str_detect()` function to find the pattern we are looking for (a contiguous sequence of `=`), but then we will pass the logical result to the `which()` function which will return the element index number of this match. 

```{r}
#| eval: false
#| label: cd-semi-sdac-text

doc |>
  str_detect(pattern = "=+") |> # match 1 or more `=`
  which() # find vector index
```

So for this file `31` is the index in `doc` where the `=` sequence occurs. Now it is important to keep in mind that we are working with a single file from the `sdac/` data. We need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the `=+` pattern will match `=`, or `==`, or `===`, etc. it is not implausible to believe that there might be a `=` character on some other line in one of the other files. Let's update our regular expression to avoid this potential scenario by only matching sequences of three or more `=`. In this case we will make use of the curly bracket operators `{}`. 

```{r}
#| eval: false
#| label: cd-semi-sdac-text-2

doc |>
  str_detect(pattern = "={3,}") |> # match 3 or more `=`
  which() # find vector index
```

We will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for `===`, `====`, etc.

`31` is the index for the `=` sequence, but we want the next line to be where we start reading the text section. To do this we increment the index by 1.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-3

text_start_index <-
  doc |>
  str_detect(pattern = "={3,}") |> # match 3 or more `=`
  which() # find vector index
text_start_index <- text_start_index + 1 # increment index by 1
```

The index for the end of the text is simply the length of the `doc` vector. We can use the `length()` function to get this index.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-4

text_end_index <- length(doc)
```

We now have the bookends, so to speak, for our text section. To extract the text we subset the `doc` vector by these indices.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-5

text <- doc[text_start_index:text_end_index] # extract text
head(text) # preview first lines of `text`
```

The text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the `str_trim()` function which by default will remove leading and trailing whitespace from each line.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-6

text <- str_trim(text) # remove leading and trailing whitespace
head(text) # preview first lines of `text`
```

To remove blank lines we will use the a logical expression to subset the `text` vector. `text != ""` means return TRUE where lines are not blank, and FALSE where they are.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-7

text <- text[text != ""] # remove blank lines
head(text) # preview first lines of `text`
```

Our first step towards a tidy dataset is to now combine the `doc_id` and each element of `text` in a data frame. 

```{r}
#| eval: false
#| label: tbl-cd-semi-sdac-text-8
#| tbl-cap: "First 5 observations of prelim data curation of the SDAC data."

data <- data.frame(doc_id, text) # tidy format `doc_id` and `text`
slice_head(data, n = 5) |> # preview first lines of `text`
  knitr::kable(booktabs = TRUE)
```

With our data now in a data frame, its time to parse the `text` column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns. To do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row of `data$text` and extract it. 

The best way to learn regular expressions is to use them. To this end I've included a link to the interactive regular expression practice website [regex101](https://regex101.com).

Open this site and copy the text below into the 'TEST STRING' field.  

```plain
o          A.1 utt1: Okay.  /
qw          A.1 utt2: {D So, }
qy^d          B.2 utt1: [ [ I guess, +
+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /
+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /
qy          A.5 utt1: Does it say something? /
sd          B.6 utt1: I think it usually does.  /
ad          B.6 utt2: You might try, {F uh, }  /
h          B.6 utt3: I don't know,  /
ad          B.6 utt4: hold it down a little longer,  /
```

```{r}
#| eval: false
#| label: fig-cd-regex-101-image
#| echo: false
#| fig-cap: "RegEx101"

knitr::include_graphics("figures/curate-datasets/cd-regex-101.png")
```

Now manually type the following regular expressions into the 'REGULAR EXPRESSION' field one-by-one (each is on a separate line). Notice what is matched as you type and when you've finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.

```plain
^.+?\s
[AB]\.\d+
utt\d+
:.+$
```

As you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text. To apply these expressions to our data and extract this information into separate columns we will make use of the `mutate()` and `str_extract()` functions. `mutate()` will take our data frame and create new columns with values we match and extract from each row in the data frame with `str_extract()`. Notice that `str_extract()` is different than `str_extract_all()`. When we work with `mutate()` each row will be evaluated in turn, therefore we only need to make one match per row in `data$text`. 

I've chained each of these steps in the code below, dropping the original `text` column with `select(-text)`, and overwriting `data` with the results.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-parse

# Extract column information from `text`
data <-
  data |> # current dataset
  mutate(damsl_tag = str_extract(string = text, pattern = "^.+?\\s")) |> # extract damsl tags
  mutate(speaker_turn = str_extract(string = text, pattern = "[AB]\\.\\d+")) |> # extract speaker_turn pairs
  mutate(utterance_num = str_extract(string = text, pattern = "utt\\d+")) |> # extract utterance number
  mutate(utterance_text = str_extract(string = text, pattern = ":.+$")) |> # extract utterance text
  select(-text) # drop the `text` column

glimpse(data) # preview the data set
```

::: {.callout}
**`r fontawesome::fa('hand-point-up')` Tip**

One twist you will notice is that regular expressions in R require double backslashes (`\\\\`) where other programming environments use a single backslash (`\\`).
:::

There are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the `speaker_turn` column into `speaker` and `turn_num` columns and second we need to remove unwanted characters from the `damsl_tag`, `utterance_num`, and `utterance_text` columns. 

To separate the values of a column into two columns we use the `separate()` function. It takes a column to separate and character vector of the names of the new columns to create. By default the values of the input column will be separated by non-alphanumeric characters. In our case this means the `.` will be our separator.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-organize

data <-
  data |> # current dataset
  separate(
    col = speaker_turn, # source column
    into = c("speaker", "turn_num")
  ) # separate speaker_turn into distinct columns: speaker and turn_num

glimpse(data) # preview the data set
```

To remove unwanted leading or trailing whitespace we apply the `str_trim()` function. For removing other characters we matching the character(s) and replace them with an empty string (`""`) with the `str_replace()` function. Again, I've chained these functions together and overwritten `data` with the results.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-clean

# Clean up column information
data <-
  data |> # current dataset
  mutate(damsl_tag = str_trim(damsl_tag)) |> # remove leading/ trailing whitespace
  mutate(utterance_num = str_replace(string = utterance_num, pattern = "utt", replacement = "")) |> # remove 'utt'
  mutate(utterance_text = str_replace(string = utterance_text, pattern = ":\\s", replacement = "")) |> # remove ': '
  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace

glimpse(data) # preview the data set
```

To round out our tidy dataset for this single conversation file we will connect the `speaker_a_id` and `speaker_b_id` with speaker A and B in our current dataset adding a new column `speaker_id`. The `case_when()` function does exactly this: allows us to map rows of `speaker` with the value "A" to `speaker_a_id` and rows with value "B" to `speaker_b_id`.

```{r}
#| eval: false
#| label: cd-semi-sdac-speaker-ids

# Link speaker with speaker_id
data <-
  data |> # current dataset
  mutate(speaker_id = case_when( # create speaker_id
    speaker == "A" ~ speaker_a_id, # speaker_a_id value when A
    speaker == "B" ~ speaker_b_id # speaker_b_id value when B
  ))

glimpse(data) # preview the data set
```

We now have the tidy dataset we set out to create. But this dataset only includes one conversation file! We want to apply this code to all 1155 conversation files in the `sdac/` corpus. The approach will be to create a custom function which groups the code we've done for this single file and then iterative send each file from the corpus through this function and combine the results into one data frame. 

Here's the custom function with some extra code to print a progress message for each file when it runs. 

```{r}
#| eval: false
#| label: cd-semi-extract-sdac-metadata-function

# [ ] rename to `extract_swda_data()`.
# [ ] add to `qtalrkit` package, note the convention of `extract_` prefix for curation functions. In combination with `get_compressed_data()` this corpus can be curated with few steps.

extract_sdac_metadata <- function(file) {
  # Function: to read a Switchboard Corpus Dialogue file and extract meta-data
  cat("Reading", basename(file), "...")
  
  # Read `file` by lines
  doc <- read_lines(file) 
  
  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`
  doc_speaker_info <- 
    doc[str_detect(doc, "\\d+_\\d+_\\d+")] |> # isolate pattern
    str_extract("\\d+_\\d+_\\d+") |> # extract the pattern
    str_split(pattern = "_") |> # split the character vector
    unlist() # flatten the list to a character vector
  doc_id <- doc_speaker_info[1] # extract `doc_id`
  speaker_a_id <- doc_speaker_info[2] # extract `speaker_a_id`
  speaker_b_id <- doc_speaker_info[3] # extract `speaker_b_id`
  
  # Extract `text`
  text_start_index <- # find where header info stops
    doc |> 
    str_detect(pattern = "={3,}") |> # match 3 or more `=`
    which() # find vector index
  
  text_start_index <- text_start_index + 1 # increment index by 1
  text_end_index <- length(doc) # get the end of the text section
  
  text <- doc[text_start_index:text_end_index] # extract text
  text <- str_trim(text) # remove leading and trailing whitespace
  text <- text[text != ""] # remove blank lines
  
  data <- data.frame(doc_id, text) # tidy format `doc_id` and `text`
  
  # Extract column information from `text`
  data <- 
    data |> 
    mutate(damsl_tag = str_extract(string = text, pattern = "^.+?\\s")) |>  # extract damsl tags
    mutate(speaker_turn = str_extract(string = text, pattern = "[AB]\\.\\d+")) |> # extract speaker_turn pairs
    mutate(utterance_num = str_extract(string = text, pattern = "utt\\d+")) |> # extract utterance number
    mutate(utterance_text = str_extract(string = text, pattern = ":.+$")) |>  # extract utterance text
    select(-text)
  
  # Separate speaker_turn into distinct columns
  data <-
    data |> # current dataset
    separate(col = speaker_turn, # source column
             into = c("speaker", "turn_num")) # separate speaker_turn into distinct columns: speaker and turn_num
  
  # Clean up column information
  data <- 
    data |> 
    mutate(damsl_tag = str_trim(damsl_tag)) |> # remove leading/ trailing whitespace
    mutate(utterance_num = str_replace(string = utterance_num, pattern = "utt", replacement = "")) |> # remove 'utt'
    mutate(utterance_text = str_replace(string = utterance_text, pattern = ":\\s", replacement = "")) |> # remove ': '
    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace
  
  # Link speaker with speaker_id
  data <- 
    data |> # current dataset
    mutate(speaker_id = case_when( # create speaker_id
      speaker == "A" ~ speaker_a_id, # speaker_a_id value when A
      speaker == "B" ~ speaker_b_id # speaker_b_id value when B
    ))
  cat(" done.\n")
  return(data) # return the data frame object
}
```

As a sanity check we will run the `extract_sdac_metadata()` function on a the conversation file we were just working on to make sure it works as expected.

```{r}
#| eval: false
#| label: cd-extract-sdac-metadata-test-show

extract_sdac_metadata(file = "../data/original/sdac/sw00utt/sw_0001_4325.utt") |>
  glimpse()
```

```{r}
#| eval: false
#| label: cd-extract-sdac-metadata-test-run
#| echo: false

extract_sdac_metadata(file = "data/curate-datasets/sdac/sw00utt/sw_0001_4325.utt") |>
  glimpse()
```

Looks good!

So now it's time to create a vector with the paths to all of the conversation files. `fs::dir_ls()` interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (`regexp = \\.utt$`) so we don't accidentally include other files in the corpus. `recurse` set to `TRUE` means we will get the full path to each file.

```{r}
#| eval: false
#| label: cd-semi-sdac-list-files-show

sdac_files <-
  fs::dir_ls(
    path = "../data/original/sdac/", # source directory
    recurse = TRUE, # traverse all sub-directories
    type = "file", # only return files
    regexp = "\\.utt$"
  ) # only return files ending in .utt
head(sdac_files) # preview file paths
```

```{r}
#| eval: false
#| label: cd-semi-sdac-list-files-run
#| echo: false
#| results: hide

sdac_files <-
  fs::dir_ls(
    path = "data/curate-datasets/sdac/", # source directory
    recurse = TRUE, # traverse all sub-directories
    type = "file", # only return files
    regexp = "\\.utt$"
  ) # only return files ending in .utt
head(sdac_files)
```

```plain
../data/original/sdac/sw00utt/sw_0001_4325.utt
../data/original/sdac/sw00utt/sw_0002_4330.utt
../data/original/sdac/sw00utt/sw_0003_4103.utt
../data/original/sdac/sw00utt/sw_0004_4327.utt
../data/original/sdac/sw00utt/sw_0005_4646.utt
../data/original/sdac/sw00utt/sw_0006_4108.utt
```

o pass each conversation file in the vector of paths to our conversation files iteratively to the `extract_sdac_metadata()` function we use `map()`. This will apply the function to each conversation file and return a data frame for each. `bind_rows()` will then join the resulting data frames by rows to give us a single tidy dataset for all 1155 conversations. Note there is a lot of processing going on here we have to be patient.

```{r}
#| eval: false
#| label: cd-semi-sdac-tidy-all
#| results: hide

# Read files and return a tidy dataset
sdac <-
  sdac_files |> # pass file names
  map(extract_sdac_metadata) |> # read and tidy iteratively
  bind_rows() # bind the results into a single data frame
```

We now see that we have `nrow(sdac)` observations (individual utterances in this dataset). 

```{r}
#| eval: false
#| label: cd-semi-sdac-preview-complete

glimpse(sdac) # preview complete curated dataset
```

#### Write datasets

Again as in the previous cases, we will write this dataset to disk to prepare for the next step in our text analysis project. 

```{r}
#| eval: false
#| label: cd-semi-sdac-write

fs::dir_create(path = "../data/derived/sdac/") # create sdac subdirectory
write_csv(sdac,
  file = "../data/derived/sdac/sdac_curated.csv"
) # write sdac to disk and label as the curated dataset
```

The directory structure now looks like this: 

```bash
data/
├── derived/
│   └── sdac/
│       └── sdac_curated.csv
└── original/
    └── sdac/
        ├── README
        ├── doc/
        ├── sw00utt/
        ├── sw01utt/
        ├── sw02utt/
        ├── sw03utt/
        ├── sw04utt/
        ├── sw05utt/
        ├── sw06utt/
        ├── sw07utt/
        ├── sw08utt/
        ├── sw09utt/
        ├── sw10utt/
        ├── sw11utt/
        ├── sw12utt/
        └── sw13utt/
```














## Summary

## Check your understanding

(... examples ...)

1. `r torf(TRUE)` Literate Programming, first introduced by Donald Knuth in 1984, allows the combination of computer code and text prose in one document. 
2. The programming paradigm Literate Programming is implemented through `r mcq(c(answer = "Quarto", "R", "RStudio", "GitHub"))`, a platform that facilitates the creation of a variety of output documents based on the same source code.
3. Which of the following components does a basic Quarto document *not* contain? `r mcq(c("Front-matter section", "Prose section", answer = "Back-matter section", "Code block"))`
4. To generate a PDF document in Quarto, you can edit the format attribute value in the front-matter section to `r fitb(c(answer = "pdf", "html", "word"), ignore_case = TRUE)`.
5. `r torf(TRUE)` The code block options `echo` and `include` can be used to hide the code and output, respectively.
6. `r torf(FALSE)` In Quarto, a code block, where the programming language code is entered, is bounded by three underscores (`_`).

## References